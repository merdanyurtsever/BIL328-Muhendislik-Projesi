{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba68792d",
   "metadata": {},
   "source": [
    "# MÃ¼zik TÃ¼rÃ¼ SÄ±nÄ±flandÄ±rma Projesi:\n",
    "\n",
    "Bu notebook, FMA (Free Music Archive) veri setini kullanarak mÃ¼zik tÃ¼rÃ¼ sÄ±nÄ±flandÄ±rma modeli geliÅŸtirmek iÃ§in veri hazÄ±rlama ve dengeleme iÅŸlemlerini iÃ§ermektedir.\n",
    "\n",
    "## Gerekli KÃ¼tÃ¼phanelerin Ä°Ã§e AktarÄ±lmasÄ±:\n",
    "\n",
    "AÅŸaÄŸÄ±daki hÃ¼crede, projede kullanÄ±lacak temel Python kÃ¼tÃ¼phaneleri import edilmektedir:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40af102",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from imblearn.over_sampling import RandomOverSampler, BorderlineSMOTE\n",
    "from collections import Counter\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set(style='whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226d4b29",
   "metadata": {},
   "source": [
    "# YardÄ±mcÄ± Fonksiyonlar\n",
    "## SÄ±nÄ±f DaÄŸÄ±lÄ±mÄ± GÃ¶rselleÅŸtirme Fonksiyonu\n",
    "AÅŸaÄŸÄ±daki fonksiyon, veri setindeki sÄ±nÄ±f daÄŸÄ±lÄ±mlarÄ±nÄ± gÃ¶rselleÅŸtirmek iÃ§in kullanÄ±lacaktÄ±r. Bu gÃ¶rselleÅŸtirme, veri dengesizliÄŸini anlamamÄ±za yardÄ±mcÄ± olur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c88248",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_class_distribution(y, labels, title):\n",
    "    counts = pd.Series(y).value_counts().sort_index()\n",
    "    valid_indices = counts.index[counts.index < len(labels)]\n",
    "    counts = counts.loc[valid_indices]\n",
    "    names = labels[counts.index]\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    ax = sns.barplot(x=names, y=counts.values, hue=names, palette='viridis', legend=False)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('SÄ±nÄ±f')\n",
    "    ax.set_ylabel('SayÄ±')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37fb5f4b",
   "metadata": {},
   "source": [
    "# Veri YÃ¼kleme ve Ã–n Ä°ÅŸleme\n",
    "Bu bÃ¶lÃ¼mdeki fonksiyon:\n",
    "\n",
    "FMA metadata dosyalarÄ±nÄ± yÃ¼kler\n",
    "Gerekli sÃ¼tunlarÄ± seÃ§er\n",
    "Eksik verileri temizler\n",
    "Etiketleri kodlar\n",
    "Veriyi sayÄ±sal formata dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa01832",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    tracks_path = 'fma_metadata/tracks.csv'\n",
    "    features_path = 'fma_metadata/features.csv'\n",
    "\n",
    "    if not os.path.exists(tracks_path) or not os.path.exists(features_path):\n",
    "        raise FileNotFoundError(f\"Gerekli veri dosyalarÄ± bulunamadÄ±. '{tracks_path}' ve '{features_path}' dosyalarÄ±nÄ±n mevcut olduÄŸundan emin olun.\")\n",
    "\n",
    "    tracks = pd.read_csv(tracks_path, index_col=0, header=[0,1])\n",
    "    \n",
    "    features = pd.read_csv(features_path, index_col=0, header=[0,1])  # Ã‡ok seviyeli baÅŸlÄ±kla oku\n",
    "    features = features.loc[:, features.columns.get_level_values(0) != 'statistics']  # 'statistics' sÃ¼tunlarÄ±nÄ± kaldÄ±r\n",
    "    features = features.astype(np.float32)  # SayÄ±sal olmayan sÃ¼tunlarÄ± kaldÄ±rdÄ±ktan sonra float'a dÃ¶nÃ¼ÅŸtÃ¼r\n",
    "\n",
    "    features.index = features.index.astype(str)\n",
    "    tracks.index = tracks.index.astype(str)\n",
    "\n",
    "    genre_series = tracks[('track', 'genre_top')].dropna()\n",
    "    common_index = features.index.intersection(genre_series.index)\n",
    "\n",
    "    X = features.loc[common_index]\n",
    "    y_labels = genre_series.loc[common_index]\n",
    "\n",
    "    X = X.fillna(0).replace([np.inf, -np.inf], 0).astype(np.float32)\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "    y = label_encoder.fit_transform(y_labels)\n",
    "\n",
    "    print('Veriler yÃ¼klendi ve Ã¶niÅŸlendi.')\n",
    "    return X, y, label_encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24fa339b",
   "metadata": {},
   "source": [
    "# BaÅŸlangÄ±Ã§ Veri Analizi\n",
    "Verinin ilk yÃ¼klemesini yapÄ±p, baÅŸlangÄ±Ã§taki sÄ±nÄ±f daÄŸÄ±lÄ±mÄ±nÄ± inceleyelim. Bu analiz, veri dengesizliÄŸi problemini gÃ¶rselleÅŸtirmemize yardÄ±mcÄ± olacak."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05193f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veriyi yÃ¼kle ve Ã¶niÅŸle\n",
    "X, y, le = load_data()\n",
    "\n",
    "# BaÅŸlangÄ±Ã§ daÄŸÄ±lÄ±mÄ±nÄ± gÃ¶ster\n",
    "plot_class_distribution(y, le.classes_, 'BaÅŸlangÄ±Ã§ SÄ±nÄ±f DaÄŸÄ±lÄ±mÄ±')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69df28eb",
   "metadata": {},
   "source": [
    "# Veri BÃ¶lme ve EÄŸitim Seti Analizi\n",
    "Veriyi eÄŸitim ve test setlerine ayÄ±rÄ±p, eÄŸitim setindeki sÄ±nÄ±f daÄŸÄ±lÄ±mÄ±nÄ± inceliyoruz. Stratified split kullanarak orijinal daÄŸÄ±lÄ±mÄ± koruyoruz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9677606b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veriyi bÃ¶l ve eÄŸitim daÄŸÄ±lÄ±mÄ±nÄ± gÃ¶ster\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "plot_class_distribution(y_train, le.classes_, 'EÄŸitim Seti DaÄŸÄ±lÄ±mÄ±')\n",
    "print(f'EÄŸitim/test bÃ¶lÃ¼nmesi tamamlandÄ±: X_train {X_train.shape}, X_test {X_test.shape}')\n",
    "\n",
    "# DetaylÄ± daÄŸÄ±lÄ±mÄ± yazdÄ±r\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "print(\"\\nEÄŸitim Seti DaÄŸÄ±lÄ±mÄ± (ham sayÄ±lar):\")\n",
    "for i, (u, c) in enumerate(zip(unique, counts)):\n",
    "    print(f\"SÄ±nÄ±f {u} ({le.classes_[i]}): {c} Ã¶rnek\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b6834d",
   "metadata": {},
   "source": [
    "# Veri Dengeleme - AÅŸama 1\n",
    "Ä°lk aÅŸamada, Ã§ok az Ã¶rneÄŸe sahip sÄ±nÄ±flar iÃ§in RandomOverSampler kullanÄ±lÄ±yor. Bu aÅŸama, BorderlineSMOTE iÃ§in yeterli Ã¶rnek sayÄ±sÄ±na ulaÅŸmamÄ±zÄ± saÄŸlar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8a22ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AdÄ±m 1: En az temsil edilen sÄ±nÄ±flar iÃ§in RandomOverSampler\n",
    "print('\\nAdÄ±m 1: AÅŸÄ±rÄ± az temsil edilen sÄ±nÄ±flar iÃ§in RandomOverSampler uygulanÄ±yor...')\n",
    "min_samples_threshold = 20  # BorderlineSMOTE iÃ§in gereken minimum Ã¶rnek sayÄ±sÄ±\n",
    "ros = RandomOverSampler(sampling_strategy={3: min_samples_threshold}, random_state=42)\n",
    "X_partial, y_partial = ros.fit_resample(X_train, y_train)\n",
    "\n",
    "# Ara sonuÃ§larÄ± gÃ¶ster\n",
    "unique_partial, counts_partial = np.unique(y_partial, return_counts=True)\n",
    "print(\"\\nRandomOverSampler sonrasÄ± daÄŸÄ±lÄ±m (ham sayÄ±lar):\")\n",
    "for i, (u, c) in enumerate(zip(unique_partial, counts_partial)):\n",
    "    print(f\"SÄ±nÄ±f {u} ({le.classes_[i]}): {c} Ã¶rnek\")\n",
    "\n",
    "plot_class_distribution(y_partial, le.classes_, 'RandomOverSampler SonrasÄ±')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e09752",
   "metadata": {},
   "source": [
    "# Veri Dengeleme - AÅŸama 2\n",
    "Ä°kinci aÅŸamada, daha sofistike bir yaklaÅŸÄ±m olan BorderlineSMOTE kullanÄ±larak kalan sÄ±nÄ±flar dengeleniyor. Bu yÃ¶ntem, sadece rastgele kopyalama yerine sentetik Ã¶rnekler oluÅŸturur.\n",
    "\n",
    "Not: Bu aÅŸama, veri setinin yapÄ±sÄ±na baÄŸlÄ± olarak baÅŸarÄ±sÄ±z olabilir. Bu durumda, ilk aÅŸamadaki sonuÃ§lar kullanÄ±lacaktÄ±r."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a24646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AdÄ±m 2: Kalan sÄ±nÄ±flar iÃ§in BorderlineSMOTE\n",
    "print('\\nAdÄ±m 2: Kalan sÄ±nÄ±flar iÃ§in BorderlineSMOTE uygulanÄ±yor...')\n",
    "borderline_smote = BorderlineSMOTE(random_state=42)\n",
    "\n",
    "try:\n",
    "    X_res, y_res = borderline_smote.fit_resample(X_partial, y_partial)\n",
    "    print(f'Kombine Ã¶rnekleme tamamlandÄ±: X_res {X_res.shape}, y_res {y_res.shape}')\n",
    "    \n",
    "    # Son daÄŸÄ±lÄ±mÄ± yazdÄ±r ve gÃ¶ster\n",
    "    unique_res, counts_res = np.unique(y_res, return_counts=True)\n",
    "    print(\"\\nSon DaÄŸÄ±lÄ±m (ham sayÄ±lar):\")\n",
    "    for i, (u, c) in enumerate(zip(unique_res, counts_res)):\n",
    "        print(f\"SÄ±nÄ±f {u} ({le.classes_[i]}): {c} Ã¶rnek\")\n",
    "    \n",
    "    plot_class_distribution(y_res, le.classes_, 'Son DengelenmiÅŸ DaÄŸÄ±lÄ±m')\n",
    "\n",
    "except Exception as e:\n",
    "    print(f'BorderlineSMOTE Ã¶rnekleme baÅŸarÄ±sÄ±z oldu: {e} - kÄ±smi Ã¶rneklenmiÅŸ veri kullanÄ±lÄ±yor')\n",
    "    X_res, y_res = X_partial, y_partial\n",
    "    plot_class_distribution(y_res, le.classes_, 'KÄ±smi Ã–rnekleme (BorderlineSMOTE baÅŸarÄ±sÄ±z)')\n",
    "\n",
    "print(\"\\nÄ°ÅŸlem hattÄ± tamamlandÄ±. Yeniden Ã¶rneklenmiÅŸ eÄŸitim verisi (X_res, y_res) ve test verisi (X_test, y_test) hazÄ±r.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5628ff20",
   "metadata": {},
   "source": [
    "# Ã–zellik SeÃ§imi (K-Best Feature Selection)\n",
    "Model performansÄ±nÄ± artÄ±rmak ve aÅŸÄ±rÄ± Ã¶ÄŸrenmeyi (overfitting) azaltmak iÃ§in K-Best Ã¶zellik seÃ§imi algoritmasÄ±nÄ± uygulayacaÄŸÄ±z. Bu algoritma, her Ã¶zelliÄŸin hedef deÄŸiÅŸkenle olan istatistiksel iliÅŸkisini Ã¶lÃ§er ve en anlamlÄ± K Ã¶zelliÄŸi seÃ§er."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db7e8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Best Ã¶zellik seÃ§imi uygulamasÄ±\n",
    "print('\\nK-Best Ã¶zellik seÃ§imi uygulanÄ±yor...')\n",
    "\n",
    "k = 250  # SeÃ§ilecek Ã¶zellik sayÄ±sÄ±\n",
    "print(f\"Toplam Ã¶zellik sayÄ±sÄ±: {X_res.shape[1]}, SeÃ§ilecek Ã¶zellik sayÄ±sÄ±: {k}\")\n",
    "\n",
    "# SelectKBest ile Ã¶zellik seÃ§imi\n",
    "selector = SelectKBest(score_func=f_classif, k=k)\n",
    "X_res_selected = selector.fit_transform(X_res, y_res)\n",
    "X_test_selected = selector.transform(X_test)\n",
    "\n",
    "# Hangi Ã¶zelliklerin seÃ§ildiÄŸini gÃ¶steren gÃ¶rselleÅŸtirme\n",
    "selected_mask = selector.get_support()\n",
    "scores = selector.scores_\n",
    "feature_indices = np.arange(len(selected_mask))\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(feature_indices, scores, alpha=0.3, color='g')\n",
    "plt.bar(feature_indices[selected_mask], scores[selected_mask], color='g')\n",
    "plt.title('Ã–zellik SkorlarÄ± ve SeÃ§ilen Ã–zellikler')\n",
    "plt.xlabel('Ã–zellik Ä°ndeksi')\n",
    "plt.ylabel('F-deÄŸeri (F-value)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Ã–zellik seÃ§imi tamamlandÄ±. SeÃ§ilen Ã¶zelliklerin boyutu: {X_res_selected.shape}\")\n",
    "\n",
    "# Orijinal veriyi gÃ¼ncellenmiÅŸ veri ile deÄŸiÅŸtirelim\n",
    "X_res = X_res_selected\n",
    "X_test = X_test_selected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91aecbc1",
   "metadata": {},
   "source": [
    "# PyTorch LSTM MODEL EÄÄ°TÄ°MÄ°\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed38cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DengelenmiÅŸ veri setinden doÄŸrulama seti ayÄ±r\n",
    "X_train_bal, X_val, y_train_bal, y_val = train_test_split(\n",
    "    X_res, y_res, test_size=0.1, stratify=y_res, random_state=42\n",
    ")\n",
    "\n",
    "# Veri Ã–lÃ§eklendirme (StandardScaler)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_bal)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"Veri Ã¶lÃ§eklendirme tamamlandÄ±.\")\n",
    "print(f\"Ã–lÃ§eklenmiÅŸ eÄŸitim verisi boyutu: {X_train_scaled.shape}\")\n",
    "print(f\"Ã–lÃ§eklenmiÅŸ doÄŸrulama verisi boyutu: {X_val_scaled.shape}\")\n",
    "print(f\"Ã–lÃ§eklenmiÅŸ test verisi boyutu: {X_test_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7d2a55",
   "metadata": {},
   "source": [
    "# LSTM Modeli iÃ§in Veri HazÄ±rlÄ±ÄŸÄ±\n",
    "PyTorch LSTM modeli iÃ§in, veriyi uygun formata dÃ¶nÃ¼ÅŸtÃ¼rmemiz gerekir. LSTM modeller sÄ±ralÄ± veri bekler, bu nedenle Ã¶znitelik vektÃ¶rÃ¼nÃ¼ zamansal bir diziye dÃ¶nÃ¼ÅŸtÃ¼receÄŸiz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7637d1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch tensÃ¶rlerine dÃ¶nÃ¼ÅŸtÃ¼rme ve veri setlerini hazÄ±rlama\n",
    "def create_sequence_data(X, y, sequence_length=10):\n",
    "    \"\"\"\n",
    "    Ã–znitelik vektÃ¶rÃ¼nÃ¼ sÄ±ralÄ± verilere dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r.\n",
    "    FMA veri seti sÄ±ralÄ± yapÄ±da deÄŸil, bu nedenle yapay bir sÄ±ra oluÅŸturuyoruz.\n",
    "    \"\"\"\n",
    "    # Veri boyutlarÄ±nÄ± kontrol et\n",
    "    n_samples, n_features = X.shape\n",
    "    \n",
    "    # Veriyi yeniden ÅŸekillendirme\n",
    "    features_per_timestep = n_features // sequence_length\n",
    "    \n",
    "    if features_per_timestep == 0:\n",
    "        features_per_timestep = 1\n",
    "        sequence_length = min(sequence_length, n_features)\n",
    "    \n",
    "    # Son timestep'e sÄ±ÄŸmayan Ã¶zellikleri ele alma\n",
    "    remainder = n_features - (sequence_length * features_per_timestep)\n",
    "    \n",
    "    # Yeniden ÅŸekillendirilmiÅŸ veri iÃ§in array oluÅŸturma\n",
    "    X_seq = np.zeros((n_samples, sequence_length, features_per_timestep))\n",
    "    \n",
    "    # Veriyi yeniden ÅŸekillendirme\n",
    "    for i in range(n_samples):\n",
    "        for t in range(sequence_length):\n",
    "            start_idx = t * features_per_timestep\n",
    "            end_idx = min(start_idx + features_per_timestep, n_features)\n",
    "            \n",
    "            if start_idx < n_features:\n",
    "                X_seq[i, t, :end_idx-start_idx] = X[i, start_idx:end_idx]\n",
    "    \n",
    "    # PyTorch tensÃ¶rlerine dÃ¶nÃ¼ÅŸtÃ¼rme\n",
    "    X_tensor = torch.FloatTensor(X_seq)\n",
    "    y_tensor = torch.LongTensor(y)\n",
    "    \n",
    "    return X_tensor, y_tensor\n",
    "\n",
    "# SÄ±ralÄ± veri iÃ§in hiperparametre\n",
    "sequence_length = 5\n",
    "\n",
    "# Ã–lÃ§eklenmiÅŸ verileri sÄ±ralÄ± forma dÃ¶nÃ¼ÅŸtÃ¼rme\n",
    "X_train_seq, y_train_tensor = create_sequence_data(X_train_scaled, y_train_bal, sequence_length)\n",
    "X_val_seq, y_val_tensor = create_sequence_data(X_val_scaled, y_val, sequence_length)\n",
    "X_test_seq, y_test_tensor = create_sequence_data(X_test_scaled, y_test, sequence_length)\n",
    "\n",
    "print(f\"EÄŸitim veri boyutu: {X_train_seq.shape}\")\n",
    "print(f\"DoÄŸrulama veri boyutu: {X_val_seq.shape}\")\n",
    "print(f\"Test veri boyutu: {X_test_seq.shape}\")\n",
    "\n",
    "# PyTorch DataLoader oluÅŸturma\n",
    "batch_size = 512\n",
    "train_dataset = TensorDataset(X_train_seq, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_seq, y_val_tensor)\n",
    "test_dataset = TensorDataset(X_test_seq, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062aadbb",
   "metadata": {},
   "source": [
    "# LSTM Model TanÄ±mÄ± ve EÄŸitimi\n",
    "AÅŸaÄŸÄ±da mÃ¼zik tÃ¼rÃ¼ sÄ±nÄ±flandÄ±rmasÄ± iÃ§in bir LSTM (Long Short-Term Memory) aÄŸÄ± tanÄ±mlÄ±yoruz. LSTM'ler, mÃ¼zik gibi sÄ±ralÄ± verilerde baÅŸarÄ±lÄ± olan bir derin Ã¶ÄŸrenme mimarisidir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a708d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM model sÄ±nÄ±fÄ±nÄ± tanÄ±mlama\n",
    "class MusicGenreLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout=0.3):\n",
    "        super(MusicGenreLSTM, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # LSTM katmanlarÄ±\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size, \n",
    "            hidden_size=hidden_size, \n",
    "            num_layers=num_layers, \n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        # Batch normalization\n",
    "        self.batch_norm = nn.BatchNorm1d(hidden_size)\n",
    "        \n",
    "        # Dropout katmanÄ±\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Tam baÄŸlantÄ±lÄ± katmanlar\n",
    "        self.fc1 = nn.Linear(hidden_size, 128)  # Ä°lk tam baÄŸlantÄ±lÄ± katman\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "        \n",
    "        # Aktivasyon fonksiyonlarÄ±\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # LSTM katmanÄ±ndan geÃ§irme\n",
    "        # x ÅŸekli: (batch_size, sequence_length, input_size)\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        \n",
    "        # Son zaman adÄ±mÄ±nÄ±n Ã§Ä±ktÄ±sÄ±nÄ± al\n",
    "        # lstm_out ÅŸekli: (batch_size, sequence_length, hidden_size)\n",
    "        lstm_out = lstm_out[:, -1, :]\n",
    "        \n",
    "        # Batch normalization\n",
    "        batch_norm_out = self.batch_norm(lstm_out)\n",
    "        \n",
    "        # Ä°lk tam baÄŸlantÄ±lÄ± katman\n",
    "        fc1_out = self.fc1(batch_norm_out)\n",
    "        fc1_out = self.relu(fc1_out)\n",
    "        fc1_out = self.dropout(fc1_out)\n",
    "        \n",
    "        # Ä°kinci tam baÄŸlantÄ±lÄ± katman (Ã§Ä±kÄ±ÅŸ katmanÄ±)\n",
    "        out = self.fc2(fc1_out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "# Model parametreleri\n",
    "input_size = X_train_seq.shape[2]  # Bir zaman adÄ±mÄ±ndaki Ã¶zellik sayÄ±sÄ±\n",
    "hidden_size = 128  # LSTM gizli katman boyutu\n",
    "num_layers = 2  # LSTM katman sayÄ±sÄ±\n",
    "num_classes = len(le.classes_)  # SÄ±nÄ±f sayÄ±sÄ±\n",
    "dropout = 0.3\n",
    "\n",
    "# GPU kullanÄ±labilir mi kontrol et\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"KullanÄ±lan cihaz: {device}\")\n",
    "\n",
    "# Model oluÅŸturma\n",
    "model = MusicGenreLSTM(input_size, hidden_size, num_layers, num_classes, dropout).to(device)\n",
    "print(model)\n",
    "\n",
    "# KayÄ±p fonksiyonu ve optimize edici tanÄ±mlama\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
    "\n",
    "# EÄŸitim fonksiyonu\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=50, early_stopping_patience=5, min_improvement_threshold=0.001):\n",
    "    # Ã–lÃ§Ã¼m deÄŸerlerini saklayacak listeler\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accs = []\n",
    "    val_accs = []\n",
    "    \n",
    "    # En iyi doÄŸrulama kaybÄ±nÄ± ve modeli saklama\n",
    "    # min_improvement_threshold: DoÄŸrulama kaybÄ±ndaki minimum iyileÅŸme eÅŸiÄŸi, \n",
    "    # bunun altÄ±ndaki iyileÅŸmeler anlamlÄ± kabul edilmez ve erken durdurma sayacÄ± sÄ±fÄ±rlanmaz.\n",
    "    best_val_loss = float('inf')\n",
    "    best_model = None\n",
    "    \n",
    "    # Erken durdurma iÃ§in sayaÃ§ ve sabÄ±r parametresi\n",
    "    early_stopping_counter = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # EÄŸitim modu\n",
    "        model.train()\n",
    "        \n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            # GradyanlarÄ± sÄ±fÄ±rla\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Ä°leri geÃ§iÅŸ\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Geri yayÄ±lÄ±m ve optimize etme\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Ä°statistikleri gÃ¼ncelle\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            train_total += labels.size(0)\n",
    "            train_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        # DoÄŸrulama modu\n",
    "        model.eval()\n",
    "        \n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                \n",
    "                # Ä°leri geÃ§iÅŸ\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                # Ä°statistikleri gÃ¼ncelle\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        # Epoch sonuÃ§larÄ±nÄ± hesapla\n",
    "        epoch_train_loss = train_loss / len(train_loader.dataset)\n",
    "        epoch_val_loss = val_loss / len(val_loader.dataset)\n",
    "        epoch_train_acc = train_correct / train_total\n",
    "        epoch_val_acc = val_correct / val_total\n",
    "        \n",
    "        # Ã–ÄŸrenme oranÄ±nÄ± ayarla\n",
    "        scheduler.step(epoch_val_loss)\n",
    "        \n",
    "        # SonuÃ§larÄ± sakla\n",
    "        train_losses.append(epoch_train_loss)\n",
    "        val_losses.append(epoch_val_loss)\n",
    "        train_accs.append(epoch_train_acc)\n",
    "        val_accs.append(epoch_val_acc)\n",
    "        \n",
    "        # EÄŸitim durumunu yazdÄ±r\n",
    "        print(f'Epoch {epoch+1}/{num_epochs} - '\n",
    "              f'Train Loss: {epoch_train_loss:.4f}, Train Acc: {epoch_train_acc:.4f}, '\n",
    "              f'Val Loss: {epoch_val_loss:.4f}, Val Acc: {epoch_val_acc:.4f}')\n",
    "        \n",
    "        # En iyi modeli sakla ve erken durdurma durumunu kontrol et\n",
    "        # DoÄŸrulama kaybÄ±ndaki iyileÅŸme miktarÄ±nÄ± hesapla\n",
    "        improvement = best_val_loss - epoch_val_loss\n",
    "        \n",
    "        if epoch_val_loss < best_val_loss:\n",
    "            # EÄŸer iyileÅŸme miktarÄ± eÅŸik deÄŸerinden fazlaysa sayacÄ± sÄ±fÄ±rla\n",
    "            if improvement > min_improvement_threshold:\n",
    "                early_stopping_counter = 0  # Counter sÄ±fÄ±rla\n",
    "                print(f'Validation loss improved by {improvement:.6f}, which is above threshold ({min_improvement_threshold:.6f})')\n",
    "            else:\n",
    "                # Ä°yileÅŸme var ama eÅŸik deÄŸerinin altÄ±nda, bu durumda counter'Ä± artÄ±rÄ±yoruz\n",
    "                early_stopping_counter += 1\n",
    "                print(f'Validation loss improved by only {improvement:.6f}, which is below threshold ({min_improvement_threshold:.6f})')\n",
    "            \n",
    "            # En iyi modeli ve validation loss deÄŸerini her durumda gÃ¼ncelle\n",
    "            best_val_loss = epoch_val_loss\n",
    "            best_model = model.state_dict()\n",
    "        else:\n",
    "            early_stopping_counter += 1  # Counter artÄ±r\n",
    "            \n",
    "        # Erken durdurma kontrolÃ¼\n",
    "        if early_stopping_counter >= early_stopping_patience:\n",
    "            print(f'Erken durdurma: Validation loss {early_stopping_patience} epoch boyunca yeterince iyileÅŸmedi (minimum eÅŸik: {min_improvement_threshold:.6f}).')\n",
    "            break\n",
    "    \n",
    "    # En iyi model aÄŸÄ±rlÄ±klarÄ±nÄ± yÃ¼kle\n",
    "    model.load_state_dict(best_model)\n",
    "    \n",
    "    return model, train_losses, val_losses, train_accs, val_accs\n",
    "\n",
    "# Modeli eÄŸit\n",
    "print(\"Model eÄŸitimi baÅŸlÄ±yor...\")\n",
    "num_epochs = 50\n",
    "early_stopping_patience = 3  # Model belirli bir eÅŸik deÄŸerinden fazla iyileÅŸmezse, bu sayÄ±da epoch sonra eÄŸitimi durdur\n",
    "\n",
    "try:\n",
    "    # DoÄŸrulama kaybÄ±nda 0.02 altÄ±ndaki iyileÅŸmeleri Ã¶nemsiz olarak kabul et\n",
    "    min_improvement_threshold = 0.02  \n",
    "    \n",
    "    model, train_losses, val_losses, train_accs, val_accs = train_model(\n",
    "        model, train_loader, val_loader, criterion, optimizer, scheduler, \n",
    "        num_epochs=num_epochs, early_stopping_patience=early_stopping_patience,\n",
    "        min_improvement_threshold=min_improvement_threshold\n",
    "    )\n",
    "    print(\"Model eÄŸitimi tamamlandÄ±!\")\n",
    "except KeyboardInterrupt:\n",
    "    print(\"EÄŸitim kullanÄ±cÄ± tarafÄ±ndan durduruldu.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207afc45",
   "metadata": {},
   "source": [
    "# Model DeÄŸerlendirmesi ve GÃ¶rselleÅŸtirme\n",
    "Bu bÃ¶lÃ¼mde eÄŸitilmiÅŸ modeli test veri seti Ã¼zerinde deÄŸerlendirip, sonuÃ§larÄ± gÃ¶rselleÅŸtireceÄŸiz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5108decf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EÄŸitim sonuÃ§larÄ±nÄ± gÃ¶rselleÅŸtirme\n",
    "def plot_training_history(train_losses, val_losses, train_accs, val_accs):\n",
    "    plt.figure(figsize=(14, 5))\n",
    "    \n",
    "    # KayÄ±p grafiÄŸi\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses, label='EÄŸitim', marker='o')\n",
    "    plt.plot(val_losses, label='DoÄŸrulama', marker='*')\n",
    "    plt.title('Model KaybÄ±')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('KayÄ±p (Cross-Entropy)')\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "    \n",
    "    # DoÄŸruluk grafiÄŸi\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(train_accs, label='EÄŸitim', marker='o')\n",
    "    plt.plot(val_accs, label='DoÄŸrulama', marker='*')\n",
    "    plt.title('Model DoÄŸruluÄŸu')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('DoÄŸruluk')\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# EÄŸitim sonuÃ§larÄ±nÄ± gÃ¶rselleÅŸtir\n",
    "try:\n",
    "    plot_training_history(train_losses, val_losses, train_accs, val_accs)\n",
    "except NameError:\n",
    "    print(\"EÄŸitim geÃ§miÅŸi bulunamadÄ±. Ã–nce modeli eÄŸitin.\")\n",
    "\n",
    "# Test veri seti Ã¼zerinde deÄŸerlendirme\n",
    "def evaluate_model(model, test_loader, device):\n",
    "    model.eval()\n",
    "    \n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            \n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(predicted.cpu().numpy())\n",
    "    \n",
    "    # DoÄŸruluk hesapla\n",
    "    accuracy = np.mean(np.array(y_true) == np.array(y_pred))\n",
    "    \n",
    "    # SonuÃ§larÄ± yazdÄ±r\n",
    "    print(f\"Test DoÄŸruluÄŸu: {accuracy:.4f}\")\n",
    "    \n",
    "    # SÄ±nÄ±flandÄ±rma raporu\n",
    "    print(\"\\nSÄ±nÄ±flandÄ±rma Raporu:\")\n",
    "    print(classification_report(y_true, y_pred, target_names=le.classes_))\n",
    "    \n",
    "    # KarmaÅŸÄ±klÄ±k matrisi\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=le.classes_, yticklabels=le.classes_)\n",
    "    plt.title('KarmaÅŸÄ±klÄ±k Matrisi')\n",
    "    plt.xlabel('Tahmin Edilen Etiketler')\n",
    "    plt.ylabel('GerÃ§ek Etiketler')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return y_true, y_pred\n",
    "\n",
    "# Test veri seti Ã¼zerinde deÄŸerlendir\n",
    "try:\n",
    "    y_true, y_pred = evaluate_model(model, test_loader, device)\n",
    "except NameError:\n",
    "    print(\"Model bulunamadÄ±. Ã–nce modeli eÄŸitin.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5894637c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve Analysis for Multiclass Classification\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from itertools import cycle\n",
    "\n",
    "def plot_multiclass_roc_curve(model, test_loader, device, label_encoder, title=\"ROC Curves\"):\n",
    "    \"\"\"Plot ROC curves for multiclass classification\"\"\"\n",
    "    model.eval()\n",
    "    all_predictions_proba = []\n",
    "    all_true_labels = []\n",
    "    \n",
    "    # Get prediction probabilities\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            # Apply softmax to get probabilities\n",
    "            proba = torch.softmax(outputs, dim=1)\n",
    "            all_predictions_proba.extend(proba.cpu().numpy())\n",
    "            all_true_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    y_true = np.array(all_true_labels)\n",
    "    y_proba = np.array(all_predictions_proba)\n",
    "    n_classes = len(label_encoder.classes_)\n",
    "    \n",
    "    # Binarize the labels for multiclass ROC\n",
    "    y_true_bin = label_binarize(y_true, classes=range(n_classes))\n",
    "    \n",
    "    # Compute ROC curve and ROC area for each class\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    \n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_true_bin[:, i], y_proba[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "    \n",
    "    # Compute micro-average ROC curve and ROC area\n",
    "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_true_bin.ravel(), y_proba.ravel())\n",
    "    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "    \n",
    "    # Plot ROC curves\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Plot micro-average ROC curve\n",
    "    plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "             label=f'Micro-average ROC (AUC = {roc_auc[\"micro\"]:.3f})',\n",
    "             color='deeppink', linestyle=':', linewidth=4)\n",
    "    \n",
    "    # Plot ROC curve for each class\n",
    "    colors = cycle(['aqua', 'darkorange', 'cornflowerblue', 'red', 'green', 'purple', 'brown', 'pink'])\n",
    "    for i, color in zip(range(n_classes), colors):\n",
    "        plt.plot(fpr[i], tpr[i], color=color, linewidth=2,\n",
    "                 label=f'{label_encoder.classes_[i]} (AUC = {roc_auc[i]:.3f})')\n",
    "    \n",
    "    # Plot random classifier line\n",
    "    plt.plot([0, 1], [0, 1], 'k--', linewidth=2, label='Random Classifier')\n",
    "    \n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate', fontsize=12)\n",
    "    plt.ylabel('True Positive Rate', fontsize=12)\n",
    "    plt.title(f'{title} - Multiclass ROC Curves', fontsize=14, fontweight='bold')\n",
    "    plt.legend(loc=\"lower right\", fontsize=10)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print AUC summary\n",
    "    print(f\"\\nğŸ“Š ROC AUC Scores:\")\n",
    "    print(f\"{'Genre':<15} {'AUC Score':<10}\")\n",
    "    print(f\"{'='*25}\")\n",
    "    for i, genre in enumerate(label_encoder.classes_):\n",
    "        print(f\"{genre:<15} {roc_auc[i]:<10.3f}\")\n",
    "    print(f\"{'='*25}\")\n",
    "    print(f\"{'Micro-Average':<15} {roc_auc['micro']:<10.3f}\")\n",
    "    \n",
    "    return roc_auc\n",
    "\n",
    "# Plot ROC curves for the LSTM model\n",
    "print(\"\\nğŸ­ Generating ROC Curves for Multiclass Classification...\")\n",
    "roc_scores = plot_multiclass_roc_curve(model, test_loader, device, le, \n",
    "                                      \"LSTM Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec05adf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support, f1_score\n",
    "\n",
    "def plot_f1_scores_by_genre(y_true, y_pred, class_names, title=\"F1 Scores by Genre\"):\n",
    "    \"\"\"Plot F1 scores for each genre with detailed visualization\"\"\"\n",
    "    \n",
    "    # Calculate F1 scores for each class\n",
    "    precision, recall, f1_scores, support = precision_recall_fscore_support(y_true, y_pred, average=None)\n",
    "    \n",
    "    # Calculate macro and weighted averages\n",
    "    f1_macro = f1_score(y_true, y_pred, average='macro')\n",
    "    f1_weighted = f1_score(y_true, y_pred, average='weighted')\n",
    "    \n",
    "    # Create the plot\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    # Create bar plot\n",
    "    x_pos = np.arange(len(class_names))\n",
    "    bars = plt.bar(x_pos, f1_scores, alpha=0.8, \n",
    "                   color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', \n",
    "                         '#8c564b', '#e377c2', '#7f7f7f'][:len(class_names)])\n",
    "    \n",
    "    # Customize the plot\n",
    "    plt.xlabel('Music Genres', fontsize=12, fontweight='bold')\n",
    "    plt.ylabel('F1 Score', fontsize=12, fontweight='bold')\n",
    "    plt.title(f'{title}\\nMacro Avg: {f1_macro:.3f} | Weighted Avg: {f1_weighted:.3f}', \n",
    "              fontsize=14, fontweight='bold')\n",
    "    plt.xticks(x_pos, class_names, rotation=45, ha='right')\n",
    "    plt.ylim(0, 1.0)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, (bar, score, support_count) in enumerate(zip(bars, f1_scores, support)):\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                f'{score:.3f}\\n(n={support_count})',\n",
    "                ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    # Add horizontal lines for averages\n",
    "    plt.axhline(y=f1_macro, color='red', linestyle='--', alpha=0.7, \n",
    "                label=f'Macro Average: {f1_macro:.3f}')\n",
    "    plt.axhline(y=f1_weighted, color='orange', linestyle='--', alpha=0.7, \n",
    "                label=f'Weighted Average: {f1_weighted:.3f}')\n",
    "    \n",
    "    # Add legend and grid\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.grid(True, alpha=0.3, axis='y')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print detailed F1 score analysis\n",
    "    print(f\"\\nğŸ“Š F1 Score Analysis by Genre:\")\n",
    "    print(f\"{'Genre':<15} {'F1 Score':<10} {'Precision':<10} {'Recall':<10} {'Support':<10}\")\n",
    "    print(f\"{'='*65}\")\n",
    "    \n",
    "    for i, genre in enumerate(class_names):\n",
    "        print(f\"{genre:<15} {f1_scores[i]:<10.3f} {precision[i]:<10.3f} {recall[i]:<10.3f} {support[i]:<10}\")\n",
    "    \n",
    "    print(f\"{'='*65}\")\n",
    "    print(f\"{'Macro Avg':<15} {f1_macro:<10.3f} {np.mean(precision):<10.3f} {np.mean(recall):<10.3f} {np.sum(support):<10}\")\n",
    "    print(f\"{'Weighted Avg':<15} {f1_weighted:<10.3f} {np.average(precision, weights=support):<10.3f} {np.average(recall, weights=support):<10.3f} {np.sum(support):<10}\")\n",
    "    \n",
    "    # Identify best and worst performing genres\n",
    "    best_genre_idx = np.argmax(f1_scores)\n",
    "    worst_genre_idx = np.argmin(f1_scores)\n",
    "    \n",
    "    print(f\"\\nğŸ† Best Performing Genre: {class_names[best_genre_idx]} (F1: {f1_scores[best_genre_idx]:.3f})\")\n",
    "    print(f\"ğŸ” Needs Improvement: {class_names[worst_genre_idx]} (F1: {f1_scores[worst_genre_idx]:.3f})\")\n",
    "    \n",
    "    # Performance categories\n",
    "    excellent_genres = [class_names[i] for i, score in enumerate(f1_scores) if score >= 0.8]\n",
    "    good_genres = [class_names[i] for i, score in enumerate(f1_scores) if 0.6 <= score < 0.8]\n",
    "    poor_genres = [class_names[i] for i, score in enumerate(f1_scores) if score < 0.6]\n",
    "    \n",
    "    print(f\"\\nğŸ“ˆ Performance Categories:\")\n",
    "    if excellent_genres:\n",
    "        print(f\"   ğŸŸ¢ Excellent (â‰¥0.8): {', '.join(excellent_genres)}\")\n",
    "    if good_genres:\n",
    "        print(f\"   ğŸŸ¡ Good (0.6-0.8): {', '.join(good_genres)}\")\n",
    "    if poor_genres:\n",
    "        print(f\"   ğŸ”´ Needs Work (<0.6): {', '.join(poor_genres)}\")\n",
    "    \n",
    "    return f1_scores, f1_macro, f1_weighted\n",
    "\n",
    "# Plot F1 scores for the LSTM model\n",
    "print(\"\\nğŸ“Š Generating F1 Score Analysis for Each Genre...\")\n",
    "f1_individual, f1_macro_score, f1_weighted_score = plot_f1_scores_by_genre(\n",
    "    y_true, y_pred, le.classes_, \n",
    "    \"LSTM Model - F1 Scores by Genre\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pydebian (3.11.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
