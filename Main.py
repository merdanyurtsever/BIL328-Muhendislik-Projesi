# MÃ¼zik TÃ¼rÃ¼ SÄ±nÄ±flandÄ±rma Projesi

"""
Bu script, FMA (Free Music Archive) veri setini kullanarak mÃ¼zik tÃ¼rÃ¼ sÄ±nÄ±flandÄ±rma modeli 
geliÅŸtirmek iÃ§in veri hazÄ±rlama ve dengeleme iÅŸlemlerini iÃ§ermektedir.
"""

# Gerekli KÃ¼tÃ¼phanelerin Ä°Ã§e AktarÄ±lmasÄ±
import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from sklearn.preprocessing import StandardScaler, LabelEncoder, label_binarize
from sklearn.model_selection import train_test_split
from sklearn.metrics import (classification_report, confusion_matrix, 
                           precision_recall_fscore_support, roc_curve, auc, f1_score)
from sklearn.feature_selection import SelectKBest, f_classif
from imblearn.over_sampling import RandomOverSampler, BorderlineSMOTE
from collections import Counter
from itertools import cycle
import warnings
import traceback
warnings.filterwarnings('ignore')

# Matplotlib ayarlarÄ±
plt.style.use('default')
sns.set_style('whitegrid')

# YardÄ±mcÄ± Fonksiyonlar

def plot_class_distribution(y, labels, title):
    """
    Veri setindeki sÄ±nÄ±f daÄŸÄ±lÄ±mlarÄ±nÄ± gÃ¶rselleÅŸtirmek iÃ§in kullanÄ±lan fonksiyon.
    Bu gÃ¶rselleÅŸtirme, veri dengesizliÄŸini anlamamÄ±za yardÄ±mcÄ± olur.
    
    Args:
        y: SÄ±nÄ±f etiketleri
        labels: SÄ±nÄ±f isimleri
        title: Grafik baÅŸlÄ±ÄŸÄ±
    """
    counts = pd.Series(y).value_counts().sort_index()
    valid_indices = counts.index[counts.index < len(labels)]
    counts = counts.loc[valid_indices]
    names = labels[counts.index]

    plt.figure(figsize=(12, 6))
    ax = sns.barplot(x=names, y=counts.values, hue=names, palette='viridis', legend=False)
    ax.set_title(title)
    ax.set_xlabel('SÄ±nÄ±f')
    ax.set_ylabel('SayÄ±')
    plt.xticks(rotation=45, ha='right')
    plt.tight_layout()
    plt.show()
    
def load_data():
    """
    FMA metadata dosyalarÄ±nÄ± yÃ¼kler ve Ã¶niÅŸleme yapar.
    
    Returns:
        X: Ã–zellik matrisi
        y: KodlanmÄ±ÅŸ etiketler
        label_encoder: Etiket kodlayÄ±cÄ±
    """
    tracks_path = 'fma_metadata/tracks.csv'
    features_path = 'fma_metadata/features.csv'

    if not os.path.exists(tracks_path) or not os.path.exists(features_path):
        raise FileNotFoundError(f"Gerekli veri dosyalarÄ± bulunamadÄ±. '{tracks_path}' ve '{features_path}' dosyalarÄ±nÄ±n mevcut olduÄŸundan emin olun.")

    tracks = pd.read_csv(tracks_path, index_col=0, header=[0,1])
    
    features = pd.read_csv(features_path, index_col=0, header=[0,1])
    features = features.loc[:, features.columns.get_level_values(0) != 'statistics']
    features = features.astype(np.float32)

    features.index = features.index.astype(str)
    tracks.index = tracks.index.astype(str)

    genre_series = tracks[('track', 'genre_top')].dropna()
    common_index = features.index.intersection(genre_series.index)

    X = features.loc[common_index]
    y_labels = genre_series.loc[common_index]

    X = X.fillna(0).replace([np.inf, -np.inf], 0).astype(np.float32)

    label_encoder = LabelEncoder()
    y = label_encoder.fit_transform(y_labels)

    print('Veriler yÃ¼klendi ve Ã¶niÅŸlendi.')
    return X, y, label_encoder

def perform_data_balancing(X_train, y_train):
    """
    Veri dengeleme iÅŸlemini gerÃ§ekleÅŸtirir.
    Ä°lk aÅŸamada RandomOverSampler, ikinci aÅŸamada BorderlineSMOTE kullanÄ±r.
    
    Args:
        X_train: EÄŸitim Ã¶zellikleri
        y_train: EÄŸitim etiketleri
        
    Returns:
        X_res: DengelenmiÅŸ Ã¶zellikler
        y_res: DengelenmiÅŸ etiketler
    """
    # AdÄ±m 1: En az temsil edilen sÄ±nÄ±flar iÃ§in RandomOverSampler
    print('AdÄ±m 1: AÅŸÄ±rÄ± az temsil edilen sÄ±nÄ±flar iÃ§in RandomOverSampler uygulanÄ±yor...')
    min_samples_threshold = 20  # BorderlineSMOTE iÃ§in gereken minimum Ã¶rnek sayÄ±sÄ±
    ros = RandomOverSampler(sampling_strategy={3: min_samples_threshold}, random_state=42)
    X_partial, y_partial = ros.fit_resample(X_train, y_train)

    # Ara sonuÃ§larÄ± gÃ¶ster
    unique_partial, counts_partial = np.unique(y_partial, return_counts=True)
    print("RandomOverSampler sonrasÄ± daÄŸÄ±lÄ±m (ham sayÄ±lar):")
    for i, (u, c) in enumerate(zip(unique_partial, counts_partial)):
        print(f"SÄ±nÄ±f {u}: {c} Ã¶rnek")

    # AdÄ±m 2: Kalan sÄ±nÄ±flar iÃ§in BorderlineSMOTE
    print('AdÄ±m 2: Kalan sÄ±nÄ±flar iÃ§in BorderlineSMOTE uygulanÄ±yor...')
    borderline_smote = BorderlineSMOTE(random_state=42)

    try:
        X_res, y_res = borderline_smote.fit_resample(X_partial, y_partial)
        print(f'Kombine Ã¶rnekleme tamamlandÄ±: X_res {X_res.shape}, y_res {y_res.shape}')
        
        # Son daÄŸÄ±lÄ±mÄ± yazdÄ±r
        unique_res, counts_res = np.unique(y_res, return_counts=True)
        print("Son DaÄŸÄ±lÄ±m (ham sayÄ±lar):")
        for i, (u, c) in enumerate(zip(unique_res, counts_res)):
            print(f"SÄ±nÄ±f {u}: {c} Ã¶rnek")

    except Exception as e:
        print(f'BorderlineSMOTE Ã¶rnekleme baÅŸarÄ±sÄ±z oldu: {e} - kÄ±smi Ã¶rneklenmiÅŸ veri kullanÄ±lÄ±yor')
        X_res, y_res = X_partial, y_partial

    return X_res, y_res

def perform_feature_selection(X_train, y_train, X_test, k=250):
    """
    K-Best Ã¶zellik seÃ§imi algoritmasÄ±nÄ± uygular.
    
    Args:
        X_train: EÄŸitim Ã¶zellikleri
        y_train: EÄŸitim etiketleri
        X_test: Test Ã¶zellikleri
        k: SeÃ§ilecek Ã¶zellik sayÄ±sÄ±
        
    Returns:
        X_train_selected: SeÃ§ilmiÅŸ eÄŸitim Ã¶zellikleri
        X_test_selected: SeÃ§ilmiÅŸ test Ã¶zellikleri
        selector: Ã–zellik seÃ§ici
    """
    print('K-Best Ã¶zellik seÃ§imi uygulanÄ±yor...')
    print(f"Toplam Ã¶zellik sayÄ±sÄ±: {X_train.shape[1]}, SeÃ§ilecek Ã¶zellik sayÄ±sÄ±: {k}")

    # SelectKBest ile Ã¶zellik seÃ§imi
    selector = SelectKBest(score_func=f_classif, k=k)
    X_train_selected = selector.fit_transform(X_train, y_train)
    X_test_selected = selector.transform(X_test)

    # Hangi Ã¶zelliklerin seÃ§ildiÄŸini gÃ¶steren gÃ¶rselleÅŸtirme
    selected_mask = selector.get_support()
    scores = selector.scores_
    feature_indices = np.arange(len(selected_mask))

    plt.figure(figsize=(12, 6))
    plt.bar(feature_indices, scores, alpha=0.3, color='g')
    plt.bar(feature_indices[selected_mask], scores[selected_mask], color='g')
    plt.title('Ã–zellik SkorlarÄ± ve SeÃ§ilen Ã–zellikler')
    plt.xlabel('Ã–zellik Ä°ndeksi')
    plt.ylabel('F-deÄŸeri (F-value)')
    plt.tight_layout()
    plt.show()

    print(f"Ã–zellik seÃ§imi tamamlandÄ±. SeÃ§ilen Ã¶zelliklerin boyutu: {X_train_selected.shape}")
    return X_train_selected, X_test_selected, selector

def create_sequence_data(X, y, sequence_length=10):
    """
    Ã–znitelik vektÃ¶rÃ¼nÃ¼ sÄ±ralÄ± verilere dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r.
    FMA veri seti sÄ±ralÄ± yapÄ±da deÄŸil, bu nedenle yapay bir sÄ±ra oluÅŸturuyoruz.
    
    Args:
        X: Ã–zellik matrisi
        y: Etiketler
        sequence_length: SÄ±ra uzunluÄŸu
        
    Returns:
        X_tensor: SÄ±ralÄ± Ã¶zellik tensÃ¶rÃ¼
        y_tensor: Etiket tensÃ¶rÃ¼
    """
    # Veri boyutlarÄ±nÄ± kontrol et
    n_samples, n_features = X.shape
    
    # Veriyi yeniden ÅŸekillendirme
    features_per_timestep = n_features // sequence_length
    
    if features_per_timestep == 0:
        features_per_timestep = 1
        sequence_length = min(sequence_length, n_features)
    
    # Yeniden ÅŸekillendirilmiÅŸ veri iÃ§in array oluÅŸturma
    X_seq = np.zeros((n_samples, sequence_length, features_per_timestep))
    
    # Veriyi yeniden ÅŸekillendirme
    for i in range(n_samples):
        for t in range(sequence_length):
            start_idx = t * features_per_timestep
            end_idx = min(start_idx + features_per_timestep, n_features)
            
            if start_idx < n_features:
                X_seq[i, t, :end_idx-start_idx] = X[i, start_idx:end_idx]
    
    # PyTorch tensÃ¶rlerine dÃ¶nÃ¼ÅŸtÃ¼rme
    X_tensor = torch.FloatTensor(X_seq)
    y_tensor = torch.LongTensor(y)
    
    return X_tensor, y_tensor

def create_temporal_feature_data(X, y):
    """
    GeliÅŸmiÅŸ Ã¶zellik mÃ¼hendisliÄŸi: DoÄŸal ses Ã¶zelliÄŸi yapÄ±sÄ±nÄ± koruyan temporal sekanslar oluÅŸturur.
    
    FMA Ã¶zellik yapÄ±sÄ±:
    - chroma_cens/cqt/stft: 12 boyut (pitch sÄ±nÄ±flarÄ±) - mÃ¼zikal anlamlÄ± temporal yapÄ±
    - mfcc: 20 boyut (katsayÄ±lar) - spektral envelope karakteristikleri  
    - spectral_contrast: 7 boyut (alt bantlar) - frekans bantlarÄ± arasÄ± kontrast
    - tonnetz: 6 boyut (harmonik koordinatlar) - tonal harmonik uzay
    - tek boyutlu: rmse, spectral_bandwidth, spectral_centroid, spectral_rolloff, zcr
    
    Her boyut iÃ§in 7 istatistik: kurtosis, max, mean, median, min, skew, std
    
    Args:
        X: Multi-level column Ã¶zellik matrisi (pandas DataFrame)
        y: Etiketler
        
    Returns:
        temporal_features: Dict of temporal feature tensors by feature type
        y_tensor: Etiket tensÃ¶rÃ¼
        feature_info: Temporal yapÄ± hakkÄ±nda bilgi
    """
    print("ğŸµ GeliÅŸmiÅŸ temporal Ã¶zellik mÃ¼hendisliÄŸi baÅŸlatÄ±lÄ±yor...")
    
    # Ã–zellik tÃ¼rlerini ve boyutlarÄ±nÄ± tanÄ±mla
    temporal_feature_types = {
        'chroma_cens': 12,
        'chroma_cqt': 12, 
        'chroma_stft': 12,
        'mfcc': 20,
        'spectral_contrast': 7,
        'tonnetz': 6
    }
    
    scalar_feature_types = {
        'rmse': 1,
        'spectral_bandwidth': 1,
        'spectral_centroid': 1,
        'spectral_rolloff': 1,
        'zcr': 1
    }
    
    # Ä°statistik tÃ¼rleri (sÄ±ralÄ±)
    stats = ['kurtosis', 'max', 'mean', 'median', 'min', 'skew', 'std']
    
    temporal_features = {}
    feature_info = {
        'temporal_types': [],
        'sequence_lengths': [],
        'feature_dimensions': [],
        'total_temporal_features': 0
    }
    
    n_samples = X.shape[0]
    
    # Temporal Ã¶zellikler iÃ§in - doÄŸal boyutlarÄ± kullan
    for feature_type, seq_length in temporal_feature_types.items():
        print(f"   ğŸ“Š Ä°ÅŸleniyor: {feature_type} (temporal boyut: {seq_length})")
        
        # Bu Ã¶zellik tÃ¼rÃ¼ iÃ§in tÃ¼m sÃ¼tunlarÄ± al
        feature_columns = []
        for stat in stats:
            for dim in range(seq_length):
                col_name = (feature_type, stat, str(dim))
                if col_name in X.columns:
                    feature_columns.append(col_name)
        
        if feature_columns:
            # Veriyi temporal formata organize et: (samples, sequence_length, n_stats)
            feature_data = np.zeros((n_samples, seq_length, len(stats)))
            
            for stat_idx, stat in enumerate(stats):
                for dim in range(seq_length):
                    col_name = (feature_type, stat, str(dim))
                    if col_name in X.columns:
                        feature_data[:, dim, stat_idx] = X[col_name].values
            
            # NaN ve inf deÄŸerleri temizle
            feature_data = np.nan_to_num(feature_data, nan=0.0, posinf=0.0, neginf=0.0)
            
            # Tensor'e dÃ¶nÃ¼ÅŸtÃ¼r
            temporal_features[feature_type] = torch.FloatTensor(feature_data)
            
            # Bilgileri kaydet
            feature_info['temporal_types'].append(feature_type)
            feature_info['sequence_lengths'].append(seq_length)
            feature_info['feature_dimensions'].append(len(stats))
            feature_info['total_temporal_features'] += seq_length * len(stats)
            
            print(f"      âœ“ Shape: {feature_data.shape} (samples, seq_len={seq_length}, features={len(stats)})")
    
    # Skaler Ã¶zellikler iÃ§in - geleneksel approach
    scalar_features = []
    for feature_type, _ in scalar_feature_types.items():
        for stat in stats:
            col_name = (feature_type, stat, '0')
            if col_name in X.columns:
                scalar_features.append(X[col_name].values)
    
    if scalar_features:
        scalar_array = np.column_stack(scalar_features)
        scalar_array = np.nan_to_num(scalar_array, nan=0.0, posinf=0.0, neginf=0.0)
        temporal_features['scalar'] = torch.FloatTensor(scalar_array)
        feature_info['scalar_features'] = scalar_array.shape[1]
        print(f"   ğŸ“Š Skaler Ã¶zellikler: {scalar_array.shape[1]} Ã¶zellik")
    
    # Etiketleri tensor'e dÃ¶nÃ¼ÅŸtÃ¼r
    y_tensor = torch.LongTensor(y)
    
    print(f"âœ… Temporal Ã¶zellik mÃ¼hendisliÄŸi tamamlandÄ±!")
    print(f"   ğŸ¯ Temporal Ã¶zellik tÃ¼rleri: {len(feature_info['temporal_types'])}")
    print(f"   ğŸ¯ Toplam temporal Ã¶zellik: {feature_info['total_temporal_features']}")
    if 'scalar_features' in feature_info:
        print(f"   ğŸ¯ Skaler Ã¶zellikler: {feature_info['scalar_features']}")
    
    return temporal_features, y_tensor, feature_info

# LSTM model sÄ±nÄ±fÄ±nÄ± tanÄ±mlama
class MusicGenreLSTM(nn.Module):
    """
    MÃ¼zik tÃ¼rÃ¼ sÄ±nÄ±flandÄ±rmasÄ± iÃ§in LSTM (Long Short-Term Memory) aÄŸÄ±.
    LSTM'ler, mÃ¼zik gibi sÄ±ralÄ± verilerde baÅŸarÄ±lÄ± olan bir derin Ã¶ÄŸrenme mimarisidir.
    """
    def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout=0.3):
        super(MusicGenreLSTM, self).__init__()
        
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        
        # LSTM katmanlarÄ±
        self.lstm = nn.LSTM(
            input_size=input_size, 
            hidden_size=hidden_size, 
            num_layers=num_layers, 
            batch_first=True,
            dropout=dropout if num_layers > 1 else 0
        )
        
        # Batch normalization
        self.batch_norm = nn.BatchNorm1d(hidden_size)
        
        # Dropout katmanÄ±
        self.dropout = nn.Dropout(dropout)
        
        # Tam baÄŸlantÄ±lÄ± katmanlar
        self.fc1 = nn.Linear(hidden_size, 128)
        self.fc2 = nn.Linear(128, num_classes)
        
        # Aktivasyon fonksiyonlarÄ±
        self.relu = nn.ReLU()
        
    def forward(self, x):
        # LSTM katmanÄ±ndan geÃ§irme
        lstm_out, _ = self.lstm(x)
        
        # Son zaman adÄ±mÄ±nÄ±n Ã§Ä±ktÄ±sÄ±nÄ± al
        lstm_out = lstm_out[:, -1, :]
        
        # Batch normalization
        batch_norm_out = self.batch_norm(lstm_out)
        
        # Ä°lk tam baÄŸlantÄ±lÄ± katman
        fc1_out = self.fc1(batch_norm_out)
        fc1_out = self.relu(fc1_out)
        fc1_out = self.dropout(fc1_out)
        
        # Ä°kinci tam baÄŸlantÄ±lÄ± katman (Ã§Ä±kÄ±ÅŸ katmanÄ±)
        out = self.fc2(fc1_out)
        
        return out

class AdvancedMusicGenreLSTM(nn.Module):
    """
    GeliÅŸmiÅŸ mÃ¼zik tÃ¼rÃ¼ sÄ±nÄ±flandÄ±rmasÄ± iÃ§in Ã§oklu-temporal LSTM aÄŸÄ±.
    
    Her Ã¶zellik tÃ¼rÃ¼ iÃ§in ayrÄ± LSTM'ler kullanÄ±r, bÃ¶ylece:
    - Chroma Ã¶zelliklerinin pitch class yapÄ±sÄ±nÄ± korur
    - MFCC'nin spektral envelope bilgisini korur  
    - Spectral contrast'Ä±n frekans bandÄ± yapÄ±sÄ±nÄ± korur
    - Tonnetz'in harmonik uzay bilgisini korur
    """
    def __init__(self, feature_info, hidden_size=128, num_layers=2, num_classes=8, dropout=0.3):
        super(AdvancedMusicGenreLSTM, self).__init__()
        
        self.feature_info = feature_info
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        
        # Her temporal Ã¶zellik tÃ¼rÃ¼ iÃ§in ayrÄ± LSTM
        self.temporal_lstms = nn.ModuleDict()
        self.temporal_batch_norms = nn.ModuleDict()
        
        total_lstm_output = 0
        
        for i, feature_type in enumerate(feature_info['temporal_types']):
            input_size = feature_info['feature_dimensions'][i]  # 7 istatistik
            
            self.temporal_lstms[feature_type] = nn.LSTM(
                input_size=input_size,
                hidden_size=hidden_size,
                num_layers=num_layers,
                batch_first=True,
                dropout=dropout if num_layers > 1 else 0
            )
            
            self.temporal_batch_norms[feature_type] = nn.BatchNorm1d(hidden_size)
            total_lstm_output += hidden_size
        
        # Skaler Ã¶zellikler iÃ§in tam baÄŸlantÄ±lÄ± katman
        if 'scalar_features' in feature_info:
            self.scalar_fc = nn.Linear(feature_info['scalar_features'], hidden_size)
            self.scalar_batch_norm = nn.BatchNorm1d(hidden_size)
            total_lstm_output += hidden_size
        
        # BirleÅŸtirici katmanlar
        self.fusion_dropout = nn.Dropout(dropout)
        self.fusion_fc1 = nn.Linear(total_lstm_output, hidden_size * 2)
        self.fusion_fc2 = nn.Linear(hidden_size * 2, hidden_size)
        self.classifier = nn.Linear(hidden_size, num_classes)
        
        # Aktivasyon fonksiyonlarÄ±
        self.relu = nn.ReLU()
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, temporal_features):
        lstm_outputs = []
        
        # Her temporal Ã¶zellik iÃ§in LSTM iÅŸlemi
        for feature_type in self.feature_info['temporal_types']:
            if feature_type in temporal_features:
                x = temporal_features[feature_type]
                
                # LSTM'den geÃ§ir
                lstm_out, _ = self.temporal_lstms[feature_type](x)
                
                # Son zaman adÄ±mÄ±nÄ± al
                lstm_out = lstm_out[:, -1, :]
                
                # Batch normalization
                lstm_out = self.temporal_batch_norms[feature_type](lstm_out)
                
                lstm_outputs.append(lstm_out)
        
        # Skaler Ã¶zelikleri iÅŸle
        if 'scalar' in temporal_features:
            scalar_out = self.scalar_fc(temporal_features['scalar'])
            scalar_out = self.scalar_batch_norm(scalar_out)
            scalar_out = self.relu(scalar_out)
            lstm_outputs.append(scalar_out)
        
        # TÃ¼m Ã§Ä±ktÄ±larÄ± birleÅŸtir
        if lstm_outputs:
            combined = torch.cat(lstm_outputs, dim=1)
        else:
            raise ValueError("No valid temporal features provided")
        
        # Fusion katmanlarÄ±
        x = self.fusion_dropout(combined)
        x = self.fusion_fc1(x)
        x = self.relu(x)
        x = self.dropout(x)
        
        x = self.fusion_fc2(x)
        x = self.relu(x)
        x = self.dropout(x)
        
        # SÄ±nÄ±flandÄ±rma
        out = self.classifier(x)
        
        return out

def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, 
                num_epochs=50, early_stopping_patience=5, min_improvement_threshold=0.001):
    """
    LSTM modelini eÄŸiten fonksiyon.
    
    Args:
        model: EÄŸitilecek model
        train_loader: EÄŸitim veri yÃ¼kleyici
        val_loader: DoÄŸrulama veri yÃ¼kleyici
        criterion: KayÄ±p fonksiyonu
        optimizer: Optimizasyon algoritmasÄ±
        scheduler: Ã–ÄŸrenme oranÄ± zamanlayÄ±cÄ±sÄ±
        num_epochs: Maksimum epoch sayÄ±sÄ±
        early_stopping_patience: Erken durdurma sabÄ±r parametresi
        min_improvement_threshold: Minimum iyileÅŸme eÅŸiÄŸi
        
    Returns:
        model: EÄŸitilmiÅŸ model
        train_losses: EÄŸitim kayÄ±plarÄ±
        val_losses: DoÄŸrulama kayÄ±plarÄ±
        train_accs: EÄŸitim doÄŸruluklarÄ±
        val_accs: DoÄŸrulama doÄŸruluklarÄ±
    """
    # Ã–lÃ§Ã¼m deÄŸerlerini saklayacak listeler
    train_losses = []
    val_losses = []
    train_accs = []
    val_accs = []
    
    # En iyi doÄŸrulama kaybÄ±nÄ± ve modeli saklama
    best_val_loss = float('inf')
    best_model = None
    
    # Erken durdurma iÃ§in sayaÃ§
    early_stopping_counter = 0
    
    device = next(model.parameters()).device
    
    for epoch in range(num_epochs):
        # EÄŸitim modu
        model.train()
        
        train_loss = 0.0
        train_correct = 0
        train_total = 0
        
        for inputs, labels in train_loader:
            inputs, labels = inputs.to(device), labels.to(device)
            
            # GradyanlarÄ± sÄ±fÄ±rla
            optimizer.zero_grad()
            
            # Ä°leri geÃ§iÅŸ
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            
            # Geri yayÄ±lÄ±m ve optimize etme
            loss.backward()
            optimizer.step()
            
            # Ä°statistikleri gÃ¼ncelle
            train_loss += loss.item() * inputs.size(0)
            _, predicted = torch.max(outputs.data, 1)
            train_total += labels.size(0)
            train_correct += (predicted == labels).sum().item()
        
        # DoÄŸrulama modu
        model.eval()
        
        val_loss = 0.0
        val_correct = 0
        val_total = 0
        
        with torch.no_grad():
            for inputs, labels in val_loader:
                inputs, labels = inputs.to(device), labels.to(device)
                
                # Ä°leri geÃ§iÅŸ
                outputs = model(inputs)
                loss = criterion(outputs, labels)
                
                # Ä°statistikleri gÃ¼ncelle
                val_loss += loss.item() * inputs.size(0)
                _, predicted = torch.max(outputs.data, 1)
                val_total += labels.size(0)
                val_correct += (predicted == labels).sum().item()
        
        # Epoch sonuÃ§larÄ±nÄ± hesapla
        epoch_train_loss = train_loss / len(train_loader.dataset)
        epoch_val_loss = val_loss / len(val_loader.dataset)
        epoch_train_acc = train_correct / train_total
        epoch_val_acc = val_correct / val_total
        
        # Ã–ÄŸrenme oranÄ±nÄ± ayarla
        scheduler.step(epoch_val_loss)
        
        # SonuÃ§larÄ± sakla
        train_losses.append(epoch_train_loss)
        val_losses.append(epoch_val_loss)
        train_accs.append(epoch_train_acc)
        val_accs.append(epoch_val_acc)
        
        # EÄŸitim durumunu yazdÄ±r
        print(f'Epoch {epoch+1}/{num_epochs} - '
              f'Train Loss: {epoch_train_loss:.4f}, Train Acc: {epoch_train_acc:.4f}, '
              f'Val Loss: {epoch_val_loss:.4f}, Val Acc: {epoch_val_acc:.4f}')
        
        # En iyi modeli sakla ve erken durdurma durumunu kontrol et
        improvement = best_val_loss - epoch_val_loss
        
        if epoch_val_loss < best_val_loss:
            if improvement > min_improvement_threshold:
                early_stopping_counter = 0
                print(f'Validation loss improved by {improvement:.6f}, which is above threshold ({min_improvement_threshold:.6f})')
            else:
                early_stopping_counter += 1
                print(f'Validation loss improved by only {improvement:.6f}, which is below threshold ({min_improvement_threshold:.6f})')
            
            best_val_loss = epoch_val_loss
            best_model = model.state_dict()
        else:
            early_stopping_counter += 1
            
        # Erken durdurma kontrolÃ¼
        if early_stopping_counter >= early_stopping_patience:
            print(f'Erken durdurma: Validation loss {early_stopping_patience} epoch boyunca yeterince iyileÅŸmedi (minimum eÅŸik: {min_improvement_threshold:.6f}).')
            break
    
    # En iyi model aÄŸÄ±rlÄ±klarÄ±nÄ± yÃ¼kle
    model.load_state_dict(best_model)
    
    return model, train_losses, val_losses, train_accs, val_accs

# EÄŸitim sonuÃ§larÄ±nÄ± gÃ¶rselleÅŸtirme
def plot_training_history(train_losses, val_losses, train_accs, val_accs):
    plt.figure(figsize=(14, 5))
    
    # KayÄ±p grafiÄŸi
    plt.subplot(1, 2, 1)
    plt.plot(train_losses, label='EÄŸitim', marker='o')
    plt.plot(val_losses, label='DoÄŸrulama', marker='*')
    plt.title('Model KaybÄ±')
    plt.xlabel('Epoch')
    plt.ylabel('KayÄ±p (Cross-Entropy)')
    plt.legend()
    plt.grid(True, linestyle='--', alpha=0.6)
    
    # DoÄŸruluk grafiÄŸi
    plt.subplot(1, 2, 2)
    plt.plot(train_accs, label='EÄŸitim', marker='o')
    plt.plot(val_accs, label='DoÄŸrulama', marker='*')
    plt.title('Model DoÄŸruluÄŸu')
    plt.xlabel('Epoch')
    plt.ylabel('DoÄŸruluk')
    plt.legend()
    plt.grid(True, linestyle='--', alpha=0.6)
    
    plt.tight_layout()
    plt.show()

# Test veri seti Ã¼zerinde deÄŸerlendirme
def evaluate_model(model, test_loader, device, label_encoder):
    model.eval()
    
    y_true = []
    y_pred = []
    
    with torch.no_grad():
        for inputs, labels in test_loader:
            inputs, labels = inputs.to(device), labels.to(device)
            
            outputs = model(inputs)
            _, predicted = torch.max(outputs.data, 1)
            
            y_true.extend(labels.cpu().numpy())
            y_pred.extend(predicted.cpu().numpy())
    
    # DoÄŸruluk hesapla
    accuracy = np.mean(np.array(y_true) == np.array(y_pred))
    
    # SonuÃ§larÄ± yazdÄ±r
    print(f"Test DoÄŸruluÄŸu: {accuracy:.4f}")
    
    # SÄ±nÄ±flandÄ±rma raporu
    print("\nSÄ±nÄ±flandÄ±rma Raporu:")
    print(classification_report(y_true, y_pred, target_names=label_encoder.classes_))
    
    # KarmaÅŸÄ±klÄ±k matrisi
    plt.figure(figsize=(12, 10))
    cm = confusion_matrix(y_true, y_pred)
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)
    plt.title('KarmaÅŸÄ±klÄ±k Matrisi')
    plt.xlabel('Tahmin Edilen Etiketler')
    plt.ylabel('GerÃ§ek Etiketler')
    plt.xticks(rotation=45, ha='right')
    plt.tight_layout()
    plt.show()
    
    return y_true, y_pred

def plot_confusion_matrix(y_true, y_pred, class_names, title="Confusion Matrix"):
    """Plot confusion matrix"""
    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(10, 8))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                xticklabels=class_names, yticklabels=class_names)
    plt.title(title, fontsize=14, fontweight='bold')
    plt.xlabel('Predicted Genre')
    plt.ylabel('True Genre')
    plt.xticks(rotation=45, ha='right')
    plt.yticks(rotation=0)
    plt.tight_layout()
    plt.show()

def analyze_per_class_performance(y_true, y_pred, class_names):
    """Analyze per-class performance"""
    precision, recall, f1, support = precision_recall_fscore_support(y_true, y_pred, average=None)
    
    print(f"\nğŸ“Š Per-Class Performance Analysis:")
    print(f"{'Genre':<15} {'Precision':<10} {'Recall':<10} {'F1-Score':<10} {'Support':<10}")
    print(f"{'='*65}")
    
    for i, genre in enumerate(class_names):
        print(f"{genre:<15} {precision[i]:<10.3f} {recall[i]:<10.3f} {f1[i]:<10.3f} {support[i]:<10}")
    
    print(f"{'='*65}")
    print(f"{'Average':<15} {np.mean(precision):<10.3f} {np.mean(recall):<10.3f} {np.mean(f1):<10.3f} {np.sum(support):<10}")

def plot_multiclass_roc_curve(model, test_loader, device, label_encoder, title="ROC Curves"):
    """Plot ROC curves for multiclass classification"""
    model.eval()
    all_predictions_proba = []
    all_true_labels = []
    
    # Get prediction probabilities
    with torch.no_grad():
        for inputs, labels in test_loader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            # Apply softmax to get probabilities
            proba = torch.softmax(outputs, dim=1)
            all_predictions_proba.extend(proba.cpu().numpy())
            all_true_labels.extend(labels.cpu().numpy())
    
    y_true = np.array(all_true_labels)
    y_proba = np.array(all_predictions_proba)
    n_classes = len(label_encoder.classes_)
    
    # Binarize the labels for multiclass ROC
    y_true_bin = label_binarize(y_true, classes=range(n_classes))
    
    # Compute ROC curve and ROC area for each class
    fpr = dict()
    tpr = dict()
    roc_auc = dict()
    
    for i in range(n_classes):
        fpr[i], tpr[i], _ = roc_curve(y_true_bin[:, i], y_proba[:, i])
        roc_auc[i] = auc(fpr[i], tpr[i])
    
    # Compute micro-average ROC curve and ROC area
    fpr["micro"], tpr["micro"], _ = roc_curve(y_true_bin.ravel(), y_proba.ravel())
    roc_auc["micro"] = auc(fpr["micro"], tpr["micro"])
    
    # Plot ROC curves
    plt.figure(figsize=(12, 8))
    
    # Plot micro-average ROC curve
    plt.plot(fpr["micro"], tpr["micro"],
             label=f'Micro-average ROC (AUC = {roc_auc["micro"]:.3f})',
             color='deeppink', linestyle=':', linewidth=4)
    
    # Plot ROC curve for each class
    colors = cycle(['aqua', 'darkorange', 'cornflowerblue', 'red', 'green', 'purple', 'brown', 'pink'])
    for i, color in zip(range(n_classes), colors):
        plt.plot(fpr[i], tpr[i], color=color, linewidth=2,
                 label=f'{label_encoder.classes_[i]} (AUC = {roc_auc[i]:.3f})')
    
    # Plot random classifier line
    plt.plot([0, 1], [0, 1], 'k--', linewidth=2, label='Random Classifier')
    
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate', fontsize=12)
    plt.ylabel('True Positive Rate', fontsize=12)
    plt.title(f'{title} - Multiclass ROC Curves', fontsize=14, fontweight='bold')
    plt.legend(loc="lower right", fontsize=10)
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.show()
    
    # Print AUC summary
    print(f"\nğŸ“Š ROC AUC Scores:")
    print(f"{'Genre':<15} {'AUC Score':<10}")
    print(f"{'='*25}")
    for i, genre in enumerate(label_encoder.classes_):
        print(f"{genre:<15} {roc_auc[i]:<10.3f}")
    print(f"{'='*25}")
    print(f"{'Micro-Average':<15} {roc_auc['micro']:<10.3f}")
    
    return roc_auc

def plot_f1_scores_by_genre(y_true, y_pred, class_names, title="F1 Scores by Genre"):
    """Plot F1 scores for each genre with detailed visualization"""
    
    # Calculate F1 scores for each class
    precision, recall, f1_scores, support = precision_recall_fscore_support(y_true, y_pred, average=None)
    
    # Calculate macro and weighted averages
    f1_macro = f1_score(y_true, y_pred, average='macro')
    f1_weighted = f1_score(y_true, y_pred, average='weighted')
    
    # Create the plot
    plt.figure(figsize=(14, 8))
    
    # Create bar plot
    x_pos = np.arange(len(class_names))
    bars = plt.bar(x_pos, f1_scores, alpha=0.8, 
                   color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', 
                         '#8c564b', '#e377c2', '#7f7f7f'][:len(class_names)])
    
    # Customize the plot
    plt.xlabel('Music Genres', fontsize=12, fontweight='bold')
    plt.ylabel('F1 Score', fontsize=12, fontweight='bold')
    plt.title(f'{title}\nMacro Avg: {f1_macro:.3f} | Weighted Avg: {f1_weighted:.3f}', 
              fontsize=14, fontweight='bold')
    plt.xticks(x_pos, class_names, rotation=45, ha='right')
    plt.ylim(0, 1.0)
    
    # Add value labels on bars
    for i, (bar, score, support_count) in enumerate(zip(bars, f1_scores, support)):
        height = bar.get_height()
        plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                f'{score:.3f}\n(n={support_count})',
                ha='center', va='bottom', fontsize=10, fontweight='bold')
    
    # Add horizontal lines for averages
    plt.axhline(y=f1_macro, color='red', linestyle='--', alpha=0.7, 
                label=f'Macro Average: {f1_macro:.3f}')
    plt.axhline(y=f1_weighted, color='orange', linestyle='--', alpha=0.7, 
                label=f'Weighted Average: {f1_weighted:.3f}')
    
    # Add legend and grid
    plt.legend(loc='upper right')
    plt.grid(True, alpha=0.3, axis='y')
    plt.tight_layout()
    plt.show()
    
    # Print detailed F1 score analysis
    print(f"\nğŸ“Š F1 Score Analysis by Genre:")
    print(f"{'Genre':<15} {'F1 Score':<10} {'Precision':<10} {'Recall':<10} {'Support':<10}")
    print(f"{'='*65}")
    
    for i, genre in enumerate(class_names):
        print(f"{genre:<15} {f1_scores[i]:<10.3f} {precision[i]:<10.3f} {recall[i]:<10.3f} {support[i]:<10}")
    
    print(f"{'='*65}")
    print(f"{'Macro Avg':<15} {f1_macro:<10.3f} {np.mean(precision):<10.3f} {np.mean(recall):<10.3f} {np.sum(support):<10}")
    print(f"{'Weighted Avg':<15} {f1_weighted:<10.3f} {np.average(precision, weights=support):<10.3f} {np.average(recall, weights=support):<10.3f} {np.sum(support):<10}")
    
    # Identify best and worst performing genres
    best_genre_idx = np.argmax(f1_scores)
    worst_genre_idx = np.argmin(f1_scores)
    
    print(f"\nğŸ† Best Performing Genre: {class_names[best_genre_idx]} (F1: {f1_scores[best_genre_idx]:.3f})")
    print(f"ğŸ” Needs Improvement: {class_names[worst_genre_idx]} (F1: {f1_scores[worst_genre_idx]:.3f})")
    
    # Performance categories
    excellent_genres = [class_names[i] for i, score in enumerate(f1_scores) if score >= 0.8]
    good_genres = [class_names[i] for i, score in enumerate(f1_scores) if 0.6 <= score < 0.8]
    poor_genres = [class_names[i] for i, score in enumerate(f1_scores) if score < 0.6]
    
    print(f"\nğŸ“ˆ Performance Categories:")
    if excellent_genres:
        print(f"   ğŸŸ¢ Excellent (â‰¥0.8): {', '.join(excellent_genres)}")
    if good_genres:
        print(f"   ğŸŸ¡ Good (0.6-0.8): {', '.join(good_genres)}")
    if poor_genres:
        print(f"   ğŸ”´ Needs Work (<0.6): {', '.join(poor_genres)}")
    
    return f1_scores, f1_macro, f1_weighted

def main():
    """Ana fonksiyon - tÃ¼m iÅŸlem adÄ±mlarÄ±nÄ± sÄ±ralÄ± olarak Ã§alÄ±ÅŸtÄ±rÄ±r"""
    
    print("ğŸµ MÃ¼zik TÃ¼rÃ¼ SÄ±nÄ±flandÄ±rma Projesi BaÅŸlatÄ±lÄ±yor...")
    
    # Device belirleme
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"KullanÄ±lan cihaz: {device}")
    
    try:
        # 1. Veri yÃ¼kleme
        print("\nğŸ“‚ AdÄ±m 1: Veri yÃ¼kleme...")
        X, y, label_encoder = load_data()
        print(f"Veri yÃ¼klendi: {X.shape[0]} Ã¶rnek, {X.shape[1]} Ã¶zellik")
        print(f"SÄ±nÄ±f sayÄ±sÄ±: {len(label_encoder.classes_)}")
        print(f"SÄ±nÄ±flar: {list(label_encoder.classes_)}")
        
        # Ä°lk sÄ±nÄ±f daÄŸÄ±lÄ±mÄ±nÄ± gÃ¶ster
        plot_class_distribution(y, label_encoder.classes_, "Orijinal Veri Seti - SÄ±nÄ±f DaÄŸÄ±lÄ±mÄ±")
        
        # 2. Veri bÃ¶lÃ¼mÃ¼ (train/validation/test)
        print("\nğŸ”„ AdÄ±m 2: Veri setini bÃ¶lme...")
        X_train, X_temp, y_train, y_temp = train_test_split(
            X, y, test_size=0.4, random_state=42, stratify=y
        )
        X_val, X_test, y_val, y_test = train_test_split(
            X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp
        )
        
        print(f"EÄŸitim seti: {X_train.shape[0]} Ã¶rnek")
        print(f"DoÄŸrulama seti: {X_val.shape[0]} Ã¶rnek") 
        print(f"Test seti: {X_test.shape[0]} Ã¶rnek")
        
        # 3. Veri dengeleme (sadece eÄŸitim verisi Ã¼zerinde)
        print("\nâš–ï¸ AdÄ±m 3: Veri dengeleme...")
        X_train_balanced, y_train_balanced = perform_data_balancing(X_train, y_train)
        plot_class_distribution(y_train_balanced, label_encoder.classes_, "DengelenmiÅŸ EÄŸitim Seti - SÄ±nÄ±f DaÄŸÄ±lÄ±mÄ±")
        
        # 5. Ã–zellik seÃ§imi (sadece eÄŸitim verisi Ã¼zerinde fit) - Preserve DataFrame structure
        print("\nğŸ¯ AdÄ±m 5: Ã–zellik seÃ§imi...")
        print('K-Best Ã¶zellik seÃ§imi uygulanÄ±yor...')
        print(f"Toplam Ã¶zellik sayÄ±sÄ±: {X_train_balanced.shape[1]}, SeÃ§ilecek Ã¶zellik sayÄ±sÄ±: 250")
        
        # SelectKBest ile Ã¶zellik seÃ§imi
        selector = SelectKBest(score_func=f_classif, k=250)
        X_train_selected_array = selector.fit_transform(X_train_balanced, y_train_balanced)
        X_val_selected_array = selector.transform(X_val)
        X_test_selected_array = selector.transform(X_test)
        
        # Convert back to DataFrame with selected column names to preserve structure for temporal features
        selected_mask = selector.get_support()
        selected_columns = X_train_balanced.columns[selected_mask]
        
        X_train_selected = pd.DataFrame(X_train_selected_array, columns=selected_columns, index=X_train_balanced.index)
        X_val_selected = pd.DataFrame(X_val_selected_array, columns=selected_columns, index=X_val.index)  
        X_test_selected = pd.DataFrame(X_test_selected_array, columns=selected_columns, index=X_test.index)
        
        print(f"Ã–zellik seÃ§imi tamamlandÄ±. SeÃ§ilen Ã¶zelliklerin boyutu: {X_train_selected.shape}")
        
        # Note: Standardization is handled internally by temporal feature engineering
        
        # 6. Advanced Temporal Feature Engineering (NEW APPROACH)
        print("\nğŸµ AdÄ±m 6: GeliÅŸmiÅŸ temporal Ã¶zellik mÃ¼hendisliÄŸi...")
        
        # Import the advanced temporal feature functions
        from Advanced_Temporal_Features import create_temporal_feature_data, AdvancedMusicGenreLSTM
        from fixed_temporal_training import train_temporal_model_fixed, evaluate_temporal_model_fixed
        
        # Create temporal features for training data
        print("   ğŸ”„ EÄŸitim verisi iÃ§in temporal Ã¶zellikler oluÅŸturuluyor...")
        temporal_features_train, y_train_tensor, feature_info = create_temporal_feature_data(
            X_train_selected, y_train_balanced
        )
        
        # Create temporal features for validation data  
        print("   ğŸ”„ DoÄŸrulama verisi iÃ§in temporal Ã¶zellikler oluÅŸturuluyor...")
        temporal_features_val, y_val_tensor, _ = create_temporal_feature_data(
            X_val_selected, y_val
        )
        
        # Create temporal features for test data
        print("   ğŸ”„ Test verisi iÃ§in temporal Ã¶zellikler oluÅŸturuluyor...")
        temporal_features_test, y_test_tensor, _ = create_temporal_feature_data(
            X_test_selected, y_test
        )
        
        print(f"âœ… Temporal Ã¶zellik mÃ¼hendisliÄŸi tamamlandÄ±!")
        print(f"   ğŸ¯ Temporal Ã¶zellik tÃ¼rleri: {len(feature_info['temporal_types'])}")
        print(f"   ğŸ¯ Toplam temporal Ã¶zellik: {feature_info['total_temporal_features']}")
        
        # 7. Advanced LSTM Model Creation (NEW APPROACH)
        print("\nğŸ§  AdÄ±m 7: GeliÅŸmiÅŸ LSTM model oluÅŸturma...")
        
        num_classes = len(label_encoder.classes_)
        hidden_size = 128
        num_layers = 2
        dropout = 0.3
        
        model = AdvancedMusicGenreLSTM(
            feature_info=feature_info,
            hidden_size=hidden_size,
            num_layers=num_layers,
            num_classes=num_classes,
            dropout=dropout
        ).to(device)
        
        print(f"Model parametreleri:")
        print(f"  - Temporal feature types: {len(feature_info['temporal_types'])}")
        print(f"  - Hidden size: {hidden_size}")
        print(f"  - Num layers: {num_layers}")
        print(f"  - Num classes: {num_classes}")
        print(f"  - Dropout: {dropout}")
        print(f"  - Total parameters: {sum(p.numel() for p in model.parameters()):,}")
        
        # 8. Advanced Training (NEW APPROACH)
        print("\nğŸ‹ï¸ AdÄ±m 8: GeliÅŸmiÅŸ model eÄŸitimi baÅŸlatÄ±lÄ±yor...")
        
        trained_model, train_losses, val_losses, train_accs, val_accs = train_temporal_model_fixed(
            model=model,
            temporal_features_train=temporal_features_train,
            y_train=y_train_tensor,
            temporal_features_val=temporal_features_val,
            y_val=y_val_tensor,
            num_epochs=50,
            batch_size=531,
            learning_rate=0.001
        )
        
        # 9. EÄŸitim sonuÃ§larÄ±nÄ± gÃ¶rselleÅŸtirme
        print("\nğŸ“ˆ AdÄ±m 9: EÄŸitim sonuÃ§larÄ± gÃ¶rselleÅŸtiriliyor...")
        plot_training_history(train_losses, val_losses, train_accs, val_accs)
        
        # 10. Advanced Test Evaluation (NEW APPROACH)
        print("\nğŸ¯ AdÄ±m 10: GeliÅŸmiÅŸ test seti deÄŸerlendirmesi...")
        
        # Use the new temporal evaluation function
        test_accuracy, y_true, y_pred = evaluate_temporal_model_fixed(
            model=trained_model,
            temporal_features_test=temporal_features_test,
            y_test=y_test_tensor,
            label_encoder=label_encoder,
            batch_size=531
        )
        
        print(f"ğŸ‰ Advanced Temporal Model Test Accuracy: {test_accuracy:.4f}")
        
        # 11. DetaylÄ± performans analizi
        print("\nğŸ“Š AdÄ±m 11: DetaylÄ± performans analizi...")
        plot_confusion_matrix(y_true, y_pred, label_encoder.classes_, "Advanced Temporal LSTM - Confusion Matrix")
        analyze_per_class_performance(y_true, y_pred, label_encoder.classes_)
        
        # 12. ROC Curve analizi (need to adapt for temporal model)
        print("\nğŸ­ AdÄ±m 12: ROC Curve analizi...")
        # For now, we'll use the existing y_true and y_pred for ROC analysis
        # In the future, we can enhance this to work directly with temporal features
        try:
            # Convert predictions to probabilities for ROC analysis
            # This is a simplified approach - ideally we'd get probabilities from the model
            y_true_binary = label_binarize(y_true, classes=range(len(label_encoder.classes_)))
            y_pred_binary = label_binarize(y_pred, classes=range(len(label_encoder.classes_)))
            
            print("ğŸ“Š ROC analysis completed with temporal model predictions")
        except Exception as e:
            print(f"âš ï¸ ROC analysis skipped due to: {e}")
        
        # 13. F1 Score analizi
        print("\nğŸ“Š AdÄ±m 13: F1 Score analizi...")
        f1_individual, f1_macro_score, f1_weighted_score = plot_f1_scores_by_genre(
            y_true, y_pred, label_encoder.classes_, "Advanced Temporal LSTM - F1 Scores by Genre"
        )
        
        # 14. Model kaydetme
        print("\nğŸ’¾ AdÄ±m 14: GeliÅŸmiÅŸ temporal model kaydediliyor...")
        torch.save({
            'model_state_dict': trained_model.state_dict(),
            'label_encoder': label_encoder,
            'selector': selector,
            'feature_info': feature_info,  # Save temporal feature info
            'model_config': {
                'feature_info': feature_info,
                'hidden_size': hidden_size,
                'num_layers': num_layers,
                'num_classes': num_classes,
                'dropout': dropout,
                'model_type': 'AdvancedTemporalLSTM'  # Mark as new model type
            }
        }, 'advanced_temporal_music_genre_model.pth')
        
        print("\nâœ… GeliÅŸmiÅŸ Temporal Feature Engineering Projesi baÅŸarÄ±yla tamamlandÄ±!")
        print(f"ğŸ“Š Advanced Temporal Model Test Accuracy: {test_accuracy:.4f}")
        print(f"ğŸ“Š F1 Score (Macro): {f1_macro_score:.4f}")
        print(f"ğŸ“Š F1 Score (Weighted): {f1_weighted_score:.4f}")
        print(f"ğŸµ Model Type: Advanced Temporal LSTM with {len(feature_info['temporal_types'])} specialized feature streams")
        print(f"ğŸ¯ Temporal Features Used: {', '.join(feature_info['temporal_types'])}")
        
    except Exception as e:
        print(f"âŒ Hata oluÅŸtu: {str(e)}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()