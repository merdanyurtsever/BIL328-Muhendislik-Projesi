{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bfa6a60",
   "metadata": {},
   "source": [
    "# MÃ¼zik TÃ¼rÃ¼ SÄ±nÄ±flandÄ±rma Projesi\n",
    "\n",
    "Bu notebook, FMA (Free Music Archive) veri setini kullanarak mÃ¼zik tÃ¼rÃ¼ sÄ±nÄ±flandÄ±rma modeli geliÅŸtirmek iÃ§in veri hazÄ±rlama ve dengeleme iÅŸlemlerini iÃ§ermektedir.\n",
    "\n",
    "## Gerekli KÃ¼tÃ¼phanelerin Ä°Ã§e AktarÄ±lmasÄ±\n",
    "AÅŸaÄŸÄ±daki hÃ¼crede, projede kullanÄ±lacak temel Python kÃ¼tÃ¼phaneleri import edilmektedir:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2a41ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from imblearn.over_sampling import RandomOverSampler, BorderlineSMOTE\n",
    "from collections import Counter\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set(style='whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4206449",
   "metadata": {},
   "source": [
    "## YardÄ±mcÄ± Fonksiyonlar\n",
    "\n",
    "### SÄ±nÄ±f DaÄŸÄ±lÄ±mÄ± GÃ¶rselleÅŸtirme Fonksiyonu\n",
    "AÅŸaÄŸÄ±daki fonksiyon, veri setindeki sÄ±nÄ±f daÄŸÄ±lÄ±mlarÄ±nÄ± gÃ¶rselleÅŸtirmek iÃ§in kullanÄ±lacaktÄ±r. Bu gÃ¶rselleÅŸtirme, veri dengesizliÄŸini anlamamÄ±za yardÄ±mcÄ± olur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ae2a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_class_distribution(y, labels, title):\n",
    "    counts = pd.Series(y).value_counts().sort_index()\n",
    "    valid_indices = counts.index[counts.index < len(labels)]\n",
    "    counts = counts.loc[valid_indices]\n",
    "    names = labels[counts.index]\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    ax = sns.barplot(x=names, y=counts.values, hue=names, palette='viridis', legend=False)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('SÄ±nÄ±f')\n",
    "    ax.set_ylabel('SayÄ±')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471baca3",
   "metadata": {},
   "source": [
    "## Veri YÃ¼kleme ve Ã–n Ä°ÅŸleme\n",
    "\n",
    "Bu bÃ¶lÃ¼mdeki fonksiyon:\n",
    "- FMA metadata dosyalarÄ±nÄ± yÃ¼kler\n",
    "- Gerekli sÃ¼tunlarÄ± seÃ§er\n",
    "- Eksik verileri temizler\n",
    "- Etiketleri kodlar\n",
    "- Veriyi sayÄ±sal formata dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e25b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    tracks_path = 'fma_metadata/tracks.csv'\n",
    "    features_path = 'fma_metadata/features.csv'\n",
    "\n",
    "    if not os.path.exists(tracks_path) or not os.path.exists(features_path):\n",
    "        raise FileNotFoundError(f\"Gerekli veri dosyalarÄ± bulunamadÄ±. '{tracks_path}' ve '{features_path}' dosyalarÄ±nÄ±n mevcut olduÄŸundan emin olun.\")\n",
    "\n",
    "    tracks = pd.read_csv(tracks_path, index_col=0, header=[0,1])\n",
    "    \n",
    "    features = pd.read_csv(features_path, index_col=0, header=[0,1])  # Ã‡ok seviyeli baÅŸlÄ±kla oku\n",
    "    features = features.loc[:, features.columns.get_level_values(0) != 'statistics']  # 'statistics' sÃ¼tunlarÄ±nÄ± kaldÄ±r\n",
    "    features = features.astype(np.float32)  # SayÄ±sal olmayan sÃ¼tunlarÄ± kaldÄ±rdÄ±ktan sonra float'a dÃ¶nÃ¼ÅŸtÃ¼r\n",
    "\n",
    "    features.index = features.index.astype(str)\n",
    "    tracks.index = tracks.index.astype(str)\n",
    "\n",
    "    genre_series = tracks[('track', 'genre_top')].dropna()\n",
    "    common_index = features.index.intersection(genre_series.index)\n",
    "\n",
    "    X = features.loc[common_index]\n",
    "    y_labels = genre_series.loc[common_index]\n",
    "\n",
    "    X = X.fillna(0).replace([np.inf, -np.inf], 0).astype(np.float32)\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "    y = label_encoder.fit_transform(y_labels)\n",
    "\n",
    "    print('Veriler yÃ¼klendi ve Ã¶niÅŸlendi.')\n",
    "    return X, y, label_encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c705ac60",
   "metadata": {},
   "source": [
    "## BaÅŸlangÄ±Ã§ Veri Analizi\n",
    "\n",
    "Verinin ilk yÃ¼klemesini yapÄ±p, baÅŸlangÄ±Ã§taki sÄ±nÄ±f daÄŸÄ±lÄ±mÄ±nÄ± inceleyelim. Bu analiz, veri dengesizliÄŸi problemini gÃ¶rselleÅŸtirmemize yardÄ±mcÄ± olacak."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97f61d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veriyi yÃ¼kle ve Ã¶niÅŸle\n",
    "X, y, le = load_data()\n",
    "\n",
    "# BaÅŸlangÄ±Ã§ daÄŸÄ±lÄ±mÄ±nÄ± gÃ¶ster\n",
    "plot_class_distribution(y, le.classes_, 'BaÅŸlangÄ±Ã§ SÄ±nÄ±f DaÄŸÄ±lÄ±mÄ±')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b6b7b9",
   "metadata": {},
   "source": [
    "## Veri BÃ¶lme ve EÄŸitim Seti Analizi\n",
    "\n",
    "Veriyi eÄŸitim ve test setlerine ayÄ±rÄ±p, eÄŸitim setindeki sÄ±nÄ±f daÄŸÄ±lÄ±mÄ±nÄ± inceliyoruz. Stratified split kullanarak orijinal daÄŸÄ±lÄ±mÄ± koruyoruz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ef4617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veriyi bÃ¶l ve eÄŸitim daÄŸÄ±lÄ±mÄ±nÄ± gÃ¶ster\n",
    "# Ä°lk bÃ¶lme: Ana eÄŸitim ve test setleri\n",
    "X_train_orig, X_test, y_train_orig, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# Ä°kinci bÃ¶lme: Ana eÄŸitim setini resampling ve temiz doÄŸrulama setlerine ayÄ±r\n",
    "X_train_for_resample, X_val_clean, y_train_for_resample, y_val_clean = train_test_split(\n",
    "    X_train_orig, y_train_orig, test_size=0.15, stratify=y_train_orig, random_state=42\n",
    ")\n",
    "\n",
    "print(f'Ä°lk bÃ¶lÃ¼nme tamamlandÄ±: X_train_orig {X_train_orig.shape}, X_test {X_test.shape}')\n",
    "print(f'Ä°kinci bÃ¶lÃ¼nme tamamlandÄ±: X_train_for_resample {X_train_for_resample.shape}, X_val_clean {X_val_clean.shape}')\n",
    "\n",
    "plot_class_distribution(y_train_for_resample, le.classes_, 'Resampling Ä°Ã§in EÄŸitim Seti DaÄŸÄ±lÄ±mÄ±')\n",
    "\n",
    "# DetaylÄ± daÄŸÄ±lÄ±mÄ± yazdÄ±r\n",
    "unique, counts = np.unique(y_train_for_resample, return_counts=True)\n",
    "print(\"\\nResampling Ä°Ã§in EÄŸitim Seti DaÄŸÄ±lÄ±mÄ± (ham sayÄ±lar):\")\n",
    "for i, (u, c) in enumerate(zip(unique, counts)):\n",
    "    print(f\"SÄ±nÄ±f {u} ({le.classes_[i]}): {c} Ã¶rnek\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48a9420",
   "metadata": {},
   "source": [
    "## Veri Dengeleme - AÅŸama 1\n",
    "\n",
    "Ä°lk aÅŸamada, Ã§ok az Ã¶rneÄŸe sahip sÄ±nÄ±flar iÃ§in RandomOverSampler kullanÄ±lÄ±yor. Bu aÅŸama, BorderlineSMOTE iÃ§in yeterli Ã¶rnek sayÄ±sÄ±na ulaÅŸmamÄ±zÄ± saÄŸlar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d82e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AdÄ±m 1: En az temsil edilen sÄ±nÄ±flar iÃ§in RandomOverSampler\n",
    "print('\\nAdÄ±m 1: AÅŸÄ±rÄ± az temsil edilen sÄ±nÄ±flar iÃ§in RandomOverSampler uygulanÄ±yor...')\n",
    "min_samples_threshold = 50  # BorderlineSMOTE iÃ§in gereken minimum Ã¶rnek sayÄ±sÄ±\n",
    "ros = RandomOverSampler(sampling_strategy={3: min_samples_threshold}, random_state=42)\n",
    "X_partial, y_partial = ros.fit_resample(X_train_for_resample, y_train_for_resample)\n",
    "\n",
    "# Ara sonuÃ§larÄ± gÃ¶ster\n",
    "unique_partial, counts_partial = np.unique(y_partial, return_counts=True)\n",
    "print(\"\\nRandomOverSampler sonrasÄ± daÄŸÄ±lÄ±m (ham sayÄ±lar):\")\n",
    "for i, (u, c) in enumerate(zip(unique_partial, counts_partial)):\n",
    "    print(f\"SÄ±nÄ±f {u} ({le.classes_[i]}): {c} Ã¶rnek\")\n",
    "\n",
    "plot_class_distribution(y_partial, le.classes_, 'RandomOverSampler SonrasÄ±')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b9d611",
   "metadata": {},
   "source": [
    "## Veri Dengeleme - AÅŸama 2\n",
    "\n",
    "Ä°kinci aÅŸamada, daha sofistike bir yaklaÅŸÄ±m olan BorderlineSMOTE kullanÄ±larak kalan sÄ±nÄ±flar dengeleniyor. Bu yÃ¶ntem, sadece rastgele kopyalama yerine sentetik Ã¶rnekler oluÅŸturur.\n",
    "\n",
    "Not: Bu aÅŸama, veri setinin yapÄ±sÄ±na baÄŸlÄ± olarak baÅŸarÄ±sÄ±z olabilir. Bu durumda, ilk aÅŸamadaki sonuÃ§lar kullanÄ±lacaktÄ±r."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0e95e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AdÄ±m 2: Kalan sÄ±nÄ±flar iÃ§in BorderlineSMOTE\n",
    "print('\\nAdÄ±m 2: Kalan sÄ±nÄ±flar iÃ§in BorderlineSMOTE uygulanÄ±yor...')\n",
    "borderline_smote = BorderlineSMOTE(random_state=42)\n",
    "\n",
    "try:\n",
    "    X_res, y_res = borderline_smote.fit_resample(X_partial, y_partial)\n",
    "    print(f'Kombine Ã¶rnekleme tamamlandÄ±: X_res {X_res.shape}, y_res {y_res.shape}')\n",
    "    \n",
    "    # Son daÄŸÄ±lÄ±mÄ± yazdÄ±r ve gÃ¶ster\n",
    "    unique_res, counts_res = np.unique(y_res, return_counts=True)\n",
    "    print(\"\\nSon DaÄŸÄ±lÄ±m (ham sayÄ±lar):\")\n",
    "    for i, (u, c) in enumerate(zip(unique_res, counts_res)):\n",
    "        print(f\"SÄ±nÄ±f {u} ({le.classes_[i]}): {c} Ã¶rnek\")\n",
    "    \n",
    "    plot_class_distribution(y_res, le.classes_, 'Son DengelenmiÅŸ DaÄŸÄ±lÄ±m')\n",
    "\n",
    "except Exception as e:\n",
    "    print(f'BorderlineSMOTE Ã¶rnekleme baÅŸarÄ±sÄ±z oldu: {e} - kÄ±smi Ã¶rneklenmiÅŸ veri kullanÄ±lÄ±yor')\n",
    "    X_res, y_res = X_partial, y_partial\n",
    "    plot_class_distribution(y_res, le.classes_, 'KÄ±smi Ã–rnekleme (BorderlineSMOTE baÅŸarÄ±sÄ±z)')\n",
    "\n",
    "print(\"\\nÄ°ÅŸlem hattÄ± tamamlandÄ±. Yeniden Ã¶rneklenmiÅŸ eÄŸitim verisi (X_res, y_res) ve test verisi (X_test, y_test) hazÄ±r.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291b507b",
   "metadata": {},
   "source": [
    "## Ã–zellik SeÃ§imi (K-Best Feature Selection)\n",
    "\n",
    "Model performansÄ±nÄ± artÄ±rmak ve aÅŸÄ±rÄ± Ã¶ÄŸrenmeyi (overfitting) azaltmak iÃ§in K-Best Ã¶zellik seÃ§imi algoritmasÄ±nÄ± uygulayacaÄŸÄ±z. Bu algoritma, her Ã¶zelliÄŸin hedef deÄŸiÅŸkenle olan istatistiksel iliÅŸkisini Ã¶lÃ§er ve en anlamlÄ± K Ã¶zelliÄŸi seÃ§er."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c6196b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Best Ã¶zellik seÃ§imi uygulamasÄ±\n",
    "print('\\nK-Best Ã¶zellik seÃ§imi uygulanÄ±yor...')\n",
    "\n",
    "k = 250  # SeÃ§ilecek Ã¶zellik sayÄ±sÄ±\n",
    "print(f\"Toplam Ã¶zellik sayÄ±sÄ±: {X_res.shape[1]}, SeÃ§ilecek Ã¶zellik sayÄ±sÄ±: {k}\")\n",
    "\n",
    "# SelectKBest ile Ã¶zellik seÃ§imi - Sadece resampled data Ã¼zerinde fit et\n",
    "selector = SelectKBest(score_func=f_classif, k=k)\n",
    "X_res_selected = selector.fit_transform(X_res, y_res)\n",
    "\n",
    "# Fitted selector ile validation ve test setlerini transform et\n",
    "X_val_clean_selected = selector.transform(X_val_clean)\n",
    "X_test_selected = selector.transform(X_test)\n",
    "\n",
    "# Hangi Ã¶zelliklerin seÃ§ildiÄŸini gÃ¶steren gÃ¶rselleÅŸtirme\n",
    "selected_mask = selector.get_support()\n",
    "scores = selector.scores_\n",
    "feature_indices = np.arange(len(selected_mask))\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(feature_indices, scores, alpha=0.3, color='g')\n",
    "plt.bar(feature_indices[selected_mask], scores[selected_mask], color='g')\n",
    "plt.title('Ã–zellik SkorlarÄ± ve SeÃ§ilen Ã–zellikler')\n",
    "plt.xlabel('Ã–zellik Ä°ndeksi')\n",
    "plt.ylabel('F-deÄŸeri (F-value)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Ã–zellik seÃ§imi tamamlandÄ±. SeÃ§ilen Ã¶zelliklerin boyutu: {X_res_selected.shape}\")\n",
    "print(f\"Validation set boyutu: {X_val_clean_selected.shape}\")\n",
    "print(f\"Test set boyutu: {X_test_selected.shape}\")\n",
    "\n",
    "# Orijinal veriyi gÃ¼ncellenmiÅŸ veri ile deÄŸiÅŸtirelim\n",
    "X_res = X_res_selected\n",
    "X_val_clean = X_val_clean_selected\n",
    "X_test = X_test_selected\n",
    "\n",
    "# Veri Ã–lÃ§eklendirme (StandardScaler) - Sadece resampled data Ã¼zerinde fit et\n",
    "print('\\nVeri Ã¶lÃ§eklendirme uygulanÄ±yor...')\n",
    "scaler = StandardScaler()\n",
    "X_res_scaled = scaler.fit_transform(X_res)\n",
    "X_val_clean_scaled = scaler.transform(X_val_clean)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"Veri Ã¶lÃ§eklendirme tamamlandÄ±.\")\n",
    "print(f\"Ã–lÃ§eklenmiÅŸ resampled eÄŸitim verisi boyutu: {X_res_scaled.shape}\")\n",
    "print(f\"Ã–lÃ§eklenmiÅŸ validation verisi boyutu: {X_val_clean_scaled.shape}\")\n",
    "print(f\"Ã–lÃ§eklenmiÅŸ test verisi boyutu: {X_test_scaled.shape}\")\n",
    "\n",
    "# Veriyi gÃ¼ncellenmiÅŸ Ã¶lÃ§eklenmiÅŸ veriler ile deÄŸiÅŸtir\n",
    "X_res = X_res_scaled\n",
    "X_val_clean = X_val_clean_scaled\n",
    "X_test = X_test_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69962f4f",
   "metadata": {},
   "source": [
    "*-----------------------------------------------------------------------------------*\n",
    "# PyTorch LSTM MODEL EÄÄ°TÄ°MÄ°\n",
    "*-----------------------------------------------------------------------------------*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340b3123",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nPyTorch LSTM Model EÄŸitimi BaÅŸlÄ±yor...\")\n",
    "\n",
    "# Veri yÃ¼kleme, Ã¶niÅŸleme, bÃ¶lme ve dengeleme adÄ±mlarÄ±nÄ±n tamamlandÄ±ÄŸÄ± varsayÄ±lÄ±r.\n",
    "# Bu noktada aÅŸaÄŸÄ±daki deÄŸiÅŸkenlerin mevcut olmasÄ± beklenir:\n",
    "# X_res, y_res (DengelenmiÅŸ eÄŸitim verisi)\n",
    "# X_val, y_val (DoÄŸrulama verisi)\n",
    "# X_test, y_test (Test verisi)\n",
    "# le (LabelEncoder nesnesi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9a8a07",
   "metadata": {},
   "source": [
    "## GeliÅŸmiÅŸ Ã–zellik MÃ¼hendisliÄŸi ve Model Optimizasyonu (Ä°steÄŸe BaÄŸlÄ±)\n",
    "\n",
    "Bu bÃ¶lÃ¼mde, model performansÄ±nÄ± artÄ±rmak iÃ§in geliÅŸmiÅŸ Ã¶zellik mÃ¼hendisliÄŸi tekniklerini ve model optimizasyonlarÄ±nÄ± gÃ¼venli bir ÅŸekilde uygulayabiliriz. Bu teknikler veri sÄ±zÄ±ntÄ±sÄ±nÄ± Ã¶nlemek iÃ§in dikkatli bir ÅŸekilde tasarlanmÄ±ÅŸtÄ±r."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2a356a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ä°steÄŸe baÄŸlÄ±: GÃ¼venli GeliÅŸmiÅŸ Ã–zellik MÃ¼hendisliÄŸi\n",
    "# Bu kÄ±smÄ± yalnÄ±zca mevcut pipeline'Ä±n performansÄ±nÄ± artÄ±rmak istiyorsanÄ±z Ã§alÄ±ÅŸtÄ±rÄ±n\n",
    "\n",
    "def enhanced_feature_engineering_pipeline(X_train, X_val, X_test, apply_pca=True, apply_poly=False, pca_variance=0.95, max_poly_features=10):\n",
    "    \"\"\"\n",
    "    GÃ¼venli geliÅŸmiÅŸ Ã¶zellik mÃ¼hendisliÄŸi pipeline'Ä±\n",
    "    - Veri sÄ±zÄ±ntÄ±sÄ±nÄ± Ã¶nler\n",
    "    - Sadece eÄŸitim verisi Ã¼zerinde fit edilir\n",
    "    - DoÄŸrulama ve test setleri transform edilir\n",
    "    \"\"\"\n",
    "    from sklearn.decomposition import PCA\n",
    "    from sklearn.preprocessing import PolynomialFeatures\n",
    "    \n",
    "    # Orijinal veriyi backup'la\n",
    "    X_train_backup = X_train.copy()\n",
    "    X_val_backup = X_val.copy()\n",
    "    X_test_backup = X_test.copy()\n",
    "    \n",
    "    enhanced_features_train = [X_train]\n",
    "    enhanced_features_val = [X_val]\n",
    "    enhanced_features_test = [X_test]\n",
    "    \n",
    "    try:\n",
    "        # 1. PCA for dimensionality reduction\n",
    "        if apply_pca:\n",
    "            print(f\"PCA uygulanÄ±yor (variance: {pca_variance})...\")\n",
    "            pca = PCA(n_components=pca_variance)\n",
    "            X_train_pca = pca.fit_transform(X_train)\n",
    "            X_val_pca = pca.transform(X_val)\n",
    "            X_test_pca = pca.transform(X_test)\n",
    "            \n",
    "            # Limit PCA features to prevent overfitting\n",
    "            max_pca_features = min(50, X_train_pca.shape[1])\n",
    "            enhanced_features_train.append(X_train_pca[:, :max_pca_features])\n",
    "            enhanced_features_val.append(X_val_pca[:, :max_pca_features])\n",
    "            enhanced_features_test.append(X_test_pca[:, :max_pca_features])\n",
    "            \n",
    "            print(f\"PCA tamamlandÄ±. {X_train_pca.shape[1]} bileÅŸenden {max_pca_features} tanesi kullanÄ±ldÄ±.\")\n",
    "        \n",
    "        # 2. Polynomial features (very conservative)\n",
    "        if apply_poly and max_poly_features > 0:\n",
    "            print(f\"Polinom Ã¶zellikleri uygulanÄ±yor (max {max_poly_features} Ã¶zellik)...\")\n",
    "            n_features_for_poly = min(max_poly_features, X_train.shape[1])\n",
    "            poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
    "            \n",
    "            X_train_poly = poly.fit_transform(X_train[:, :n_features_for_poly])\n",
    "            X_val_poly = poly.transform(X_val[:, :n_features_for_poly])\n",
    "            X_test_poly = poly.transform(X_test[:, :n_features_for_poly])\n",
    "            \n",
    "            # Limit polynomial features\n",
    "            max_poly_out = min(20, X_train_poly.shape[1])\n",
    "            enhanced_features_train.append(X_train_poly[:, :max_poly_out])\n",
    "            enhanced_features_val.append(X_val_poly[:, :max_poly_out])\n",
    "            enhanced_features_test.append(X_test_poly[:, :max_poly_out])\n",
    "            \n",
    "            print(f\"Polinom Ã¶zellikleri tamamlandÄ±. {X_train_poly.shape[1]} Ã¶zellikten {max_poly_out} tanesi kullanÄ±ldÄ±.\")\n",
    "        \n",
    "        # 3. Combine all features\n",
    "        X_train_enhanced = np.hstack(enhanced_features_train)\n",
    "        X_val_enhanced = np.hstack(enhanced_features_val)\n",
    "        X_test_enhanced = np.hstack(enhanced_features_test)\n",
    "        \n",
    "        # 4. Apply scaling to enhanced features\n",
    "        scaler_enhanced = StandardScaler()\n",
    "        X_train_final = scaler_enhanced.fit_transform(X_train_enhanced)\n",
    "        X_val_final = scaler_enhanced.transform(X_val_enhanced)\n",
    "        X_test_final = scaler_enhanced.transform(X_test_enhanced)\n",
    "        \n",
    "        print(f\"GeliÅŸmiÅŸ Ã¶zellik mÃ¼hendisliÄŸi baÅŸarÄ±lÄ±!\")\n",
    "        print(f\"Ã–zellik boyutlarÄ±: {X_train.shape[1]} -> {X_train_final.shape[1]}\")\n",
    "        \n",
    "        return X_train_final, X_val_final, X_test_final, True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"GeliÅŸmiÅŸ Ã¶zellik mÃ¼hendisliÄŸi baÅŸarÄ±sÄ±z: {e}\")\n",
    "        print(\"Orijinal veriler kullanÄ±lÄ±yor.\")\n",
    "        return X_train_backup, X_val_backup, X_test_backup, False\n",
    "\n",
    "# GeliÅŸmiÅŸ Ã¶zellik mÃ¼hendisliÄŸi ayarlarÄ±\n",
    "APPLY_ENHANCED_FEATURES = False  # Bu deÄŸeri True yaparak etkinleÅŸtirin\n",
    "APPLY_PCA = True\n",
    "APPLY_POLYNOMIAL = False  # Dikkatli olun, bu Ã§ok fazla Ã¶zellik yaratabilir\n",
    "PCA_VARIANCE = 0.95\n",
    "MAX_POLY_FEATURES = 5  # Ã‡ok kÃ¼Ã§Ã¼k tutun\n",
    "\n",
    "if APPLY_ENHANCED_FEATURES:\n",
    "    print(\"GeliÅŸmiÅŸ Ã¶zellik mÃ¼hendisliÄŸi uygulanÄ±yor...\")\n",
    "    \n",
    "    # Mevcut verileri enhanced features ile deÄŸiÅŸtir\n",
    "    X_res_enhanced, X_val_clean_enhanced, X_test_enhanced, success = enhanced_feature_engineering_pipeline(\n",
    "        X_res, X_val_clean, X_test, \n",
    "        apply_pca=APPLY_PCA, \n",
    "        apply_poly=APPLY_POLYNOMIAL,\n",
    "        pca_variance=PCA_VARIANCE,\n",
    "        max_poly_features=MAX_POLY_FEATURES\n",
    "    )\n",
    "    \n",
    "    if success:\n",
    "        X_res = X_res_enhanced\n",
    "        X_val_clean = X_val_clean_enhanced\n",
    "        X_test = X_test_enhanced\n",
    "        print(\"GeliÅŸmiÅŸ Ã¶zellikler baÅŸarÄ±yla uygulandÄ±.\")\n",
    "    else:\n",
    "        print(\"Orijinal Ã¶zellikler korundu.\")\n",
    "else:\n",
    "    print(\"GeliÅŸmiÅŸ Ã¶zellik mÃ¼hendisliÄŸi devre dÄ±ÅŸÄ±. Mevcut pipeline kullanÄ±lÄ±yor.\")\n",
    "\n",
    "print(f\"\\nFinal veri boyutlarÄ±:\")\n",
    "print(f\"Training (X_res): {X_res.shape}\")\n",
    "print(f\"Validation (X_val_clean): {X_val_clean.shape}\")\n",
    "print(f\"Test (X_test): {X_test.shape}\")\n",
    "\n",
    "# Model alternatiflerini test etme fonksiyonu\n",
    "def compare_model_architectures():\n",
    "    \"\"\"\n",
    "    FarklÄ± model mimarilerini karÅŸÄ±laÅŸtÄ±rma (isteÄŸe baÄŸlÄ±)\n",
    "    \"\"\"\n",
    "    print(\"\\n=== MODEL MÄ°MARÄ°SÄ° KARÅILAÅTIRMASI ===\")\n",
    "    print(\"Mevcut LSTM modeli yanÄ±nda ÅŸu alternatifleri deneyebilirsiniz:\")\n",
    "    print(\"1. Bidirectional LSTM\")\n",
    "    print(\"2. GRU (Gated Recurrent Unit)\")\n",
    "    print(\"3. 1D CNN + LSTM hibrit\")\n",
    "    print(\"4. Transformer tabanlÄ± model\")\n",
    "    print(\"5. Ensemble modeller\")\n",
    "    \n",
    "    print(\"\\nMevcut hiperparametrelerle deneme Ã¶nerileri:\")\n",
    "    print(\"- Hidden size: [64, 128, 256]\")\n",
    "    print(\"- Num layers: [1, 2, 3]\")\n",
    "    print(\"- Dropout: [0.2, 0.3, 0.5]\")\n",
    "    print(\"- Learning rate: [0.001, 0.01, 0.0001]\")\n",
    "\n",
    "compare_model_architectures()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f746e0b",
   "metadata": {},
   "source": [
    "## LSTM Modeli iÃ§in Veri HazÄ±rlÄ±ÄŸÄ±\n",
    "\n",
    "PyTorch LSTM modeli iÃ§in, veriyi uygun formata dÃ¶nÃ¼ÅŸtÃ¼rmemiz gerekir. LSTM modeller sÄ±ralÄ± veri bekler, bu nedenle Ã¶znitelik vektÃ¶rÃ¼nÃ¼ zamansal bir diziye dÃ¶nÃ¼ÅŸtÃ¼receÄŸiz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa7d165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch tensÃ¶rlerine dÃ¶nÃ¼ÅŸtÃ¼rme ve veri setlerini hazÄ±rlama\n",
    "def create_sequence_data(X, y, sequence_length=10):\n",
    "    \"\"\"\n",
    "    Ã–znitelik vektÃ¶rÃ¼nÃ¼ sÄ±ralÄ± verilere dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r.\n",
    "    FMA veri seti sÄ±ralÄ± yapÄ±da deÄŸil, bu nedenle yapay bir sÄ±ra oluÅŸturuyoruz.\n",
    "    \"\"\"\n",
    "    # Veri boyutlarÄ±nÄ± kontrol et\n",
    "    n_samples, n_features = X.shape\n",
    "    \n",
    "    # Veriyi yeniden ÅŸekillendirme\n",
    "    features_per_timestep = n_features // sequence_length\n",
    "    \n",
    "    if features_per_timestep == 0:\n",
    "        features_per_timestep = 1\n",
    "        sequence_length = min(sequence_length, n_features)\n",
    "    \n",
    "    # Son timestep'e sÄ±ÄŸmayan Ã¶zellikleri ele alma\n",
    "    remainder = n_features - (sequence_length * features_per_timestep)\n",
    "    \n",
    "    # Yeniden ÅŸekillendirilmiÅŸ veri iÃ§in array oluÅŸturma\n",
    "    X_seq = np.zeros((n_samples, sequence_length, features_per_timestep))\n",
    "    \n",
    "    # Veriyi yeniden ÅŸekillendirme\n",
    "    for i in range(n_samples):\n",
    "        for t in range(sequence_length):\n",
    "            start_idx = t * features_per_timestep\n",
    "            end_idx = min(start_idx + features_per_timestep, n_features)\n",
    "            \n",
    "            if start_idx < n_features:\n",
    "                X_seq[i, t, :end_idx-start_idx] = X[i, start_idx:end_idx]\n",
    "    \n",
    "    # PyTorch tensÃ¶rlerine dÃ¶nÃ¼ÅŸtÃ¼rme\n",
    "    X_tensor = torch.FloatTensor(X_seq)\n",
    "    y_tensor = torch.LongTensor(y)\n",
    "    \n",
    "    return X_tensor, y_tensor\n",
    "\n",
    "# SÄ±ralÄ± veri iÃ§in hiperparametre\n",
    "sequence_length = 5\n",
    "\n",
    "# Ã–lÃ§eklenmiÅŸ verileri sÄ±ralÄ± forma dÃ¶nÃ¼ÅŸtÃ¼rme - Yeni pipeline deÄŸiÅŸkenlerini kullan\n",
    "X_train_seq, y_train_tensor = create_sequence_data(X_res, y_res, sequence_length)\n",
    "X_val_seq, y_val_tensor = create_sequence_data(X_val_clean, y_val_clean, sequence_length)\n",
    "X_test_seq, y_test_tensor = create_sequence_data(X_test, y_test, sequence_length)\n",
    "\n",
    "print(f\"EÄŸitim veri boyutu: {X_train_seq.shape}\")\n",
    "print(f\"DoÄŸrulama veri boyutu: {X_val_seq.shape}\")\n",
    "print(f\"Test veri boyutu: {X_test_seq.shape}\")\n",
    "\n",
    "# PyTorch DataLoader oluÅŸturma\n",
    "batch_size = 512\n",
    "train_dataset = TensorDataset(X_train_seq, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_seq, y_val_tensor)\n",
    "test_dataset = TensorDataset(X_test_seq, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719d6bcd",
   "metadata": {},
   "source": [
    "## LSTM Model TanÄ±mÄ± ve EÄŸitimi\n",
    "\n",
    "AÅŸaÄŸÄ±da mÃ¼zik tÃ¼rÃ¼ sÄ±nÄ±flandÄ±rmasÄ± iÃ§in bir LSTM (Long Short-Term Memory) aÄŸÄ± tanÄ±mlÄ±yoruz. LSTM'ler, mÃ¼zik gibi sÄ±ralÄ± verilerde baÅŸarÄ±lÄ± olan bir derin Ã¶ÄŸrenme mimarisidir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6ce84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM model sÄ±nÄ±fÄ±nÄ± tanÄ±mlama\n",
    "class MusicGenreLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout=0.3):\n",
    "        super(MusicGenreLSTM, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # LSTM katmanlarÄ±\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size, \n",
    "            hidden_size=hidden_size, \n",
    "            num_layers=num_layers, \n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        # Batch normalization\n",
    "        self.batch_norm = nn.BatchNorm1d(hidden_size)\n",
    "        \n",
    "        # Dropout katmanÄ±\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Tam baÄŸlantÄ±lÄ± katmanlar\n",
    "        self.fc1 = nn.Linear(hidden_size, 128)  # Ä°lk tam baÄŸlantÄ±lÄ± katman\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "        \n",
    "        # Aktivasyon fonksiyonlarÄ±\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # LSTM katmanÄ±ndan geÃ§irme\n",
    "        # x ÅŸekli: (batch_size, sequence_length, input_size)\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        \n",
    "        # Son zaman adÄ±mÄ±nÄ±n Ã§Ä±ktÄ±sÄ±nÄ± al\n",
    "        # lstm_out ÅŸekli: (batch_size, sequence_length, hidden_size)\n",
    "        lstm_out = lstm_out[:, -1, :]\n",
    "        \n",
    "        # Batch normalization\n",
    "        batch_norm_out = self.batch_norm(lstm_out)\n",
    "        \n",
    "        # Ä°lk tam baÄŸlantÄ±lÄ± katman\n",
    "        fc1_out = self.fc1(batch_norm_out)\n",
    "        fc1_out = self.relu(fc1_out)\n",
    "        fc1_out = self.dropout(fc1_out)\n",
    "        \n",
    "        # Ä°kinci tam baÄŸlantÄ±lÄ± katman (Ã§Ä±kÄ±ÅŸ katmanÄ±)\n",
    "        out = self.fc2(fc1_out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "# Model parametreleri\n",
    "input_size = X_train_seq.shape[2]  # Bir zaman adÄ±mÄ±ndaki Ã¶zellik sayÄ±sÄ±\n",
    "hidden_size = 128  # LSTM gizli katman boyutu\n",
    "num_layers = 2  # LSTM katman sayÄ±sÄ±\n",
    "num_classes = len(le.classes_)  # SÄ±nÄ±f sayÄ±sÄ±\n",
    "dropout = 0.3\n",
    "\n",
    "# GPU kullanÄ±labilir mi kontrol et\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"KullanÄ±lan cihaz: {device}\")\n",
    "\n",
    "# Model oluÅŸturma\n",
    "model = MusicGenreLSTM(input_size, hidden_size, num_layers, num_classes, dropout).to(device)\n",
    "print(model)\n",
    "\n",
    "# KayÄ±p fonksiyonu ve optimize edici tanÄ±mlama\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
    "\n",
    "# EÄŸitim fonksiyonu\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=50, early_stopping_patience=5, min_improvement_threshold=0.001):\n",
    "    # Ã–lÃ§Ã¼m deÄŸerlerini saklayacak listeler\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accs = []\n",
    "    val_accs = []\n",
    "    \n",
    "    # En iyi doÄŸrulama kaybÄ±nÄ± ve modeli saklama\n",
    "    # min_improvement_threshold: DoÄŸrulama kaybÄ±ndaki minimum iyileÅŸme eÅŸiÄŸi, \n",
    "    # bunun altÄ±ndaki iyileÅŸmeler anlamlÄ± kabul edilmez ve erken durdurma sayacÄ± sÄ±fÄ±rlanmaz.\n",
    "    best_val_loss = float('inf')\n",
    "    best_model = None\n",
    "    \n",
    "    # Erken durdurma iÃ§in sayaÃ§ ve sabÄ±r parametresi\n",
    "    early_stopping_counter = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # EÄŸitim modu\n",
    "        model.train()\n",
    "        \n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            # GradyanlarÄ± sÄ±fÄ±rla\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Ä°leri geÃ§iÅŸ\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Geri yayÄ±lÄ±m ve optimize etme\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Ä°statistikleri gÃ¼ncelle\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            train_total += labels.size(0)\n",
    "            train_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        # DoÄŸrulama modu\n",
    "        model.eval()\n",
    "        \n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                \n",
    "                # Ä°leri geÃ§iÅŸ\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                # Ä°statistikleri gÃ¼ncelle\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        # Epoch sonuÃ§larÄ±nÄ± hesapla\n",
    "        epoch_train_loss = train_loss / len(train_loader.dataset)\n",
    "        epoch_val_loss = val_loss / len(val_loader.dataset)\n",
    "        epoch_train_acc = train_correct / train_total\n",
    "        epoch_val_acc = val_correct / val_total\n",
    "        \n",
    "        # Ã–ÄŸrenme oranÄ±nÄ± ayarla\n",
    "        scheduler.step(epoch_val_loss)\n",
    "        \n",
    "        # SonuÃ§larÄ± sakla\n",
    "        train_losses.append(epoch_train_loss)\n",
    "        val_losses.append(epoch_val_loss)\n",
    "        train_accs.append(epoch_train_acc)\n",
    "        val_accs.append(epoch_val_acc)\n",
    "        \n",
    "        # EÄŸitim durumunu yazdÄ±r\n",
    "        print(f'Epoch {epoch+1}/{num_epochs} - '\n",
    "              f'Train Loss: {epoch_train_loss:.4f}, Train Acc: {epoch_train_acc:.4f}, '\n",
    "              f'Val Loss: {epoch_val_loss:.4f}, Val Acc: {epoch_val_acc:.4f}')\n",
    "        \n",
    "        # En iyi modeli sakla ve erken durdurma durumunu kontrol et\n",
    "        # DoÄŸrulama kaybÄ±ndaki iyileÅŸme miktarÄ±nÄ± hesapla\n",
    "        improvement = best_val_loss - epoch_val_loss\n",
    "        \n",
    "        if epoch_val_loss < best_val_loss:\n",
    "            # EÄŸer iyileÅŸme miktarÄ± eÅŸik deÄŸerinden fazlaysa sayacÄ± sÄ±fÄ±rla\n",
    "            if improvement > min_improvement_threshold:\n",
    "                early_stopping_counter = 0  # Counter sÄ±fÄ±rla\n",
    "                print(f'Validation loss improved by {improvement:.6f}, which is above threshold ({min_improvement_threshold:.6f})')\n",
    "            else:\n",
    "                # Ä°yileÅŸme var ama eÅŸik deÄŸerinin altÄ±nda, bu durumda counter'Ä± artÄ±rÄ±yoruz\n",
    "                early_stopping_counter += 1\n",
    "                print(f'Validation loss improved by only {improvement:.6f}, which is below threshold ({min_improvement_threshold:.6f})')\n",
    "            \n",
    "            # En iyi modeli ve validation loss deÄŸerini her durumda gÃ¼ncelle\n",
    "            best_val_loss = epoch_val_loss\n",
    "            best_model = model.state_dict()\n",
    "        else:\n",
    "            early_stopping_counter += 1  # Counter artÄ±r\n",
    "            \n",
    "        # Erken durdurma kontrolÃ¼\n",
    "        if early_stopping_counter >= early_stopping_patience:\n",
    "            print(f'Erken durdurma: Validation loss {early_stopping_patience} epoch boyunca yeterince iyileÅŸmedi (minimum eÅŸik: {min_improvement_threshold:.6f}).')\n",
    "            break\n",
    "    \n",
    "    # En iyi model aÄŸÄ±rlÄ±klarÄ±nÄ± yÃ¼kle\n",
    "    model.load_state_dict(best_model)\n",
    "    \n",
    "    return model, train_losses, val_losses, train_accs, val_accs\n",
    "\n",
    "# Modeli eÄŸit\n",
    "print(\"Model eÄŸitimi baÅŸlÄ±yor...\")\n",
    "num_epochs = 50\n",
    "early_stopping_patience = 3  # Model belirli bir eÅŸik deÄŸerinden fazla iyileÅŸmezse, bu sayÄ±da epoch sonra eÄŸitimi durdur\n",
    "\n",
    "try:\n",
    "    # DoÄŸrulama kaybÄ±nda 0.02 altÄ±ndaki iyileÅŸmeleri Ã¶nemsiz olarak kabul et\n",
    "    min_improvement_threshold = 0.02  \n",
    "    \n",
    "    model, train_losses, val_losses, train_accs, val_accs = train_model(\n",
    "        model, train_loader, val_loader, criterion, optimizer, scheduler, \n",
    "        num_epochs=num_epochs, early_stopping_patience=early_stopping_patience,\n",
    "        min_improvement_threshold=min_improvement_threshold\n",
    "    )\n",
    "    print(\"Model eÄŸitimi tamamlandÄ±!\")\n",
    "except KeyboardInterrupt:\n",
    "    print(\"EÄŸitim kullanÄ±cÄ± tarafÄ±ndan durduruldu.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f720d377",
   "metadata": {},
   "source": [
    "## Model DeÄŸerlendirmesi ve GÃ¶rselleÅŸtirme\n",
    "\n",
    "Bu bÃ¶lÃ¼mde eÄŸitilmiÅŸ modeli test veri seti Ã¼zerinde deÄŸerlendirip, sonuÃ§larÄ± gÃ¶rselleÅŸtireceÄŸiz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5361a1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EÄŸitim sonuÃ§larÄ±nÄ± gÃ¶rselleÅŸtirme\n",
    "def plot_training_history(train_losses, val_losses, train_accs, val_accs):\n",
    "    plt.figure(figsize=(14, 5))\n",
    "    \n",
    "    # KayÄ±p grafiÄŸi\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses, label='EÄŸitim', marker='o')\n",
    "    plt.plot(val_losses, label='DoÄŸrulama', marker='*')\n",
    "    plt.title('Model KaybÄ±')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('KayÄ±p (Cross-Entropy)')\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "    \n",
    "    # DoÄŸruluk grafiÄŸi\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(train_accs, label='EÄŸitim', marker='o')\n",
    "    plt.plot(val_accs, label='DoÄŸrulama', marker='*')\n",
    "    plt.title('Model DoÄŸruluÄŸu')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('DoÄŸruluk')\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# EÄŸitim sonuÃ§larÄ±nÄ± gÃ¶rselleÅŸtir\n",
    "try:\n",
    "    plot_training_history(train_losses, val_losses, train_accs, val_accs)\n",
    "except NameError:\n",
    "    print(\"EÄŸitim geÃ§miÅŸi bulunamadÄ±. Ã–nce modeli eÄŸitin.\")\n",
    "\n",
    "# Test veri seti Ã¼zerinde deÄŸerlendirme\n",
    "def evaluate_model(model, test_loader, device):\n",
    "    model.eval()\n",
    "    \n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            \n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(predicted.cpu().numpy())\n",
    "    \n",
    "    # DoÄŸruluk hesapla\n",
    "    accuracy = np.mean(np.array(y_true) == np.array(y_pred))\n",
    "    \n",
    "    # SonuÃ§larÄ± yazdÄ±r\n",
    "    print(f\"Test DoÄŸruluÄŸu: {accuracy:.4f}\")\n",
    "    \n",
    "    # SÄ±nÄ±flandÄ±rma raporu\n",
    "    print(\"\\nSÄ±nÄ±flandÄ±rma Raporu:\")\n",
    "    print(classification_report(y_true, y_pred, target_names=le.classes_))\n",
    "    \n",
    "    # KarmaÅŸÄ±klÄ±k matrisi\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=le.classes_, yticklabels=le.classes_)\n",
    "    plt.title('KarmaÅŸÄ±klÄ±k Matrisi')\n",
    "    plt.xlabel('Tahmin Edilen Etiketler')\n",
    "    plt.ylabel('GerÃ§ek Etiketler')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return y_true, y_pred\n",
    "\n",
    "# Test veri seti Ã¼zerinde deÄŸerlendir\n",
    "try:\n",
    "    y_true, y_pred = evaluate_model(model, test_loader, device)\n",
    "except NameError:\n",
    "    print(\"Model bulunamadÄ±. Ã–nce modeli eÄŸitin.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a7e87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score, accuracy_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def get_model_probabilities(model, test_loader, device):\n",
    "    \"\"\"\n",
    "    Get prediction probabilities from the trained model\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_proba = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            probabilities = F.softmax(outputs, dim=1)\n",
    "            \n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_proba.extend(probabilities.cpu().numpy())\n",
    "    \n",
    "    return np.array(y_true), np.array(y_proba)\n",
    "\n",
    "def comprehensive_evaluation(model, test_loader, device, class_names):\n",
    "    \"\"\"\n",
    "    Provide detailed evaluation metrics for the trained model\n",
    "    \"\"\"\n",
    "    # Get true labels and prediction probabilities\n",
    "    y_true, y_proba = get_model_probabilities(model, test_loader, device)\n",
    "    \n",
    "    # Get predictions\n",
    "    y_pred = np.argmax(y_proba, axis=1)\n",
    "    \n",
    "    # Basic metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    \n",
    "    # Per-class metrics\n",
    "    precision, recall, f1, support = precision_recall_fscore_support(\n",
    "        y_true, y_pred, average=None, labels=range(len(class_names))\n",
    "    )\n",
    "    \n",
    "    # Macro and weighted averages\n",
    "    precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(\n",
    "        y_true, y_pred, average='macro'\n",
    "    )\n",
    "    precision_weighted, recall_weighted, f1_weighted, _ = precision_recall_fscore_support(\n",
    "        y_true, y_pred, average='weighted'\n",
    "    )\n",
    "    \n",
    "    # AUC-ROC for multiclass\n",
    "    y_true_binarized = label_binarize(y_true, classes=range(len(class_names)))\n",
    "    auc_scores = []\n",
    "    for i in range(len(class_names)):\n",
    "        if len(np.unique(y_true_binarized[:, i])) > 1:  # Check if class exists\n",
    "            auc = roc_auc_score(y_true_binarized[:, i], y_proba[:, i])\n",
    "            auc_scores.append(auc)\n",
    "    \n",
    "    # Create detailed report\n",
    "    results = {\n",
    "        'accuracy': accuracy,\n",
    "        'macro_precision': precision_macro,\n",
    "        'macro_recall': recall_macro,\n",
    "        'macro_f1': f1_macro,\n",
    "        'weighted_precision': precision_weighted,\n",
    "        'weighted_recall': recall_weighted,\n",
    "        'weighted_f1': f1_weighted,\n",
    "        'mean_auc': np.mean(auc_scores) if auc_scores else 0,\n",
    "        'per_class_metrics': {\n",
    "            class_names[i]: {\n",
    "                'precision': precision[i],\n",
    "                'recall': recall[i],\n",
    "                'f1': f1[i],\n",
    "                'support': support[i]\n",
    "            } for i in range(len(class_names))\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Use the comprehensive evaluation function\n",
    "try:\n",
    "    if 'model' in locals() and 'test_loader' in locals():\n",
    "        print(\"\\nDetaylÄ± model deÄŸerlendirmesi...\")\n",
    "        detailed_results = comprehensive_evaluation(model, test_loader, device, le.classes_)\n",
    "        \n",
    "        print(f\"\\nDetaylÄ± SonuÃ§lar:\")\n",
    "        print(f\"Accuracy: {detailed_results['accuracy']:.4f}\")\n",
    "        print(f\"Macro F1: {detailed_results['macro_f1']:.4f}\")\n",
    "        print(f\"Weighted F1: {detailed_results['weighted_f1']:.4f}\")\n",
    "        print(f\"Mean AUC: {detailed_results['mean_auc']:.4f}\")\n",
    "        \n",
    "        print(\"\\nSÄ±nÄ±f bazÄ±nda detaylar:\")\n",
    "        for class_name, metrics in detailed_results['per_class_metrics'].items():\n",
    "            print(f\"{class_name}: F1={metrics['f1']:.3f}, Precision={metrics['precision']:.3f}, Recall={metrics['recall']:.3f}\")\n",
    "    else:\n",
    "        print(\"Model henÃ¼z eÄŸitilmemiÅŸ. Ã–nce modeli eÄŸitin.\")\n",
    "except NameError:\n",
    "    print(\"Model bulunamadÄ±. Ã–nce modeli eÄŸitin.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f88f77",
   "metadata": {},
   "source": [
    "## Model DeÄŸerlendirmesi ve Ä°leriye DÃ¶nÃ¼k Ã‡alÄ±ÅŸmalar\n",
    "\n",
    "MÃ¼zik tÃ¼rÃ¼ sÄ±nÄ±flandÄ±rma modelimiz veriyi dengeledikten sonra eÄŸitilmiÅŸtir. SonuÃ§lar deÄŸerlendirilirken ÅŸunlar gÃ¶z Ã¶nÃ¼nde bulundurulmalÄ±dÄ±r:\n",
    "\n",
    "1. **Veri Kalitesi**: FMA veri setindeki Ã¶zellikler, ses dosyalarÄ±ndan Ã§Ä±karÄ±lmÄ±ÅŸ Ã¶zelliklerdir. Daha iyi sonuÃ§lar iÃ§in ham ses verileri Ã¼zerinde spektrogram analizi yapÄ±labilir.\n",
    "\n",
    "2. **Model Mimarisi**: LSTM modeli, sÄ±ralÄ± verilerde baÅŸarÄ±lÄ± olmasÄ±na raÄŸmen, mÃ¼zik tÃ¼rÃ¼ tanÄ±ma iÃ§in CNN (Convolutional Neural Network) veya CNN-LSTM hibrit modeller de kullanÄ±labilir.\n",
    "\n",
    "3. **Hiperparametreler**: FarklÄ± hiperparametreler (Ã¶rn. Ã¶ÄŸrenme oranÄ±, katman sayÄ±sÄ±, nÃ¶ron sayÄ±sÄ±) ile model performansÄ± artÄ±rÄ±labilir.\n",
    "\n",
    "4. **Veri Dengeleme**: KullandÄ±ÄŸÄ±mÄ±z veri dengeleme yÃ¶ntemleri, eÄŸitim setindeki sÄ±nÄ±f daÄŸÄ±lÄ±mÄ±nÄ± eÅŸitlemeye yardÄ±mcÄ± olur, ancak sentetik veri oluÅŸturma riskleri de taÅŸÄ±r.\n",
    "\n",
    "5. **Ã–zellik SeÃ§imi**: K-Best algoritmasÄ± ile seÃ§ilen Ã¶zellikler, modelin daha iyi genelleme yapmasÄ±na ve aÅŸÄ±rÄ± Ã¶ÄŸrenmesinin azalmasÄ±na yardÄ±mcÄ± olabilir. FarklÄ± K deÄŸerleri denenerek optimum Ã¶zellik sayÄ±sÄ± bulunabilir.\n",
    "\n",
    "Ä°leriye dÃ¶nÃ¼k Ã§alÄ±ÅŸmalarda, daha karmaÅŸÄ±k modeller, farklÄ± Ã¶zellik Ã§Ä±karma teknikleri ve daha bÃ¼yÃ¼k veri setleri kullanÄ±larak performans artÄ±rÄ±labilir."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d18bee",
   "metadata": {},
   "source": [
    "## Model Optimizasyonu ve Sorun Giderme\n",
    "\n",
    "Bu bÃ¶lÃ¼m, model performansÄ±nÄ± artÄ±rmak iÃ§in Ã§eÅŸitli optimizasyon teknikleri ve sorun giderme yÃ¶ntemlerini iÃ§erir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c4d430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Performans Analizi ve Ä°yileÅŸtirme Ã–nerileri\n",
    "\n",
    "def analyze_model_performance():\n",
    "    \"\"\"\n",
    "    Model performansÄ±nÄ± analiz et ve iyileÅŸtirme Ã¶nerileri sun\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if 'detailed_results' in locals() or 'detailed_results' in globals():\n",
    "            results = detailed_results\n",
    "            \n",
    "            print(\"\\n=== MODEL PERFORMANS ANALÄ°ZÄ° ===\")\n",
    "            print(f\"Genel DoÄŸruluk: {results['accuracy']:.4f}\")\n",
    "            print(f\"Macro F1 Skoru: {results['macro_f1']:.4f}\")\n",
    "            print(f\"Weighted F1 Skoru: {results['weighted_f1']:.4f}\")\n",
    "            print(f\"Ortalama AUC: {results['mean_auc']:.4f}\")\n",
    "            \n",
    "            # Performans deÄŸerlendirmesi ve Ã¶neriler\n",
    "            if results['accuracy'] < 0.6:\n",
    "                print(\"\\nâš ï¸  DÃœÅÃœK PERFORMANS TESPÄ°T EDÄ°LDÄ°\")\n",
    "                print(\"Ã–neriler:\")\n",
    "                print(\"1. Daha fazla veri toplama\")\n",
    "                print(\"2. FarklÄ± model mimarisi deneme (CNN, Transformer)\")\n",
    "                print(\"3. Hiperparametre optimizasyonu\")\n",
    "                print(\"4. Veri Ã¶n iÅŸleme tekniklerini gÃ¶zden geÃ§irme\")\n",
    "                \n",
    "            elif results['accuracy'] < 0.75:\n",
    "                print(\"\\nğŸ“Š ORTA SEVÄ°YE PERFORMANS\")\n",
    "                print(\"Ä°yileÅŸtirme Ã¶nerileri:\")\n",
    "                print(\"1. Ã–zellik mÃ¼hendisliÄŸi uygulama\")\n",
    "                print(\"2. Model ensemble teknikleri\")\n",
    "                print(\"3. Daha sofistike veri dengeleme\")\n",
    "                print(\"4. Regularization teknikleri\")\n",
    "                \n",
    "            else:\n",
    "                print(\"\\nâœ… Ä°YÄ° PERFORMANS\")\n",
    "                print(\"Model baÅŸarÄ±lÄ± bir ÅŸekilde Ã§alÄ±ÅŸÄ±yor.\")\n",
    "                \n",
    "            # SÄ±nÄ±f bazÄ±nda performans analizi\n",
    "            print(\"\\n=== SINIF BAZINDA PERFORMANS ===\")\n",
    "            poor_classes = []\n",
    "            for class_name, metrics in results['per_class_metrics'].items():\n",
    "                if metrics['f1'] < 0.5:\n",
    "                    poor_classes.append(class_name)\n",
    "                    \n",
    "            if poor_classes:\n",
    "                print(f\"DÃ¼ÅŸÃ¼k performanslÄ± sÄ±nÄ±flar: {', '.join(poor_classes)}\")\n",
    "                print(\"Bu sÄ±nÄ±flar iÃ§in:\")\n",
    "                print(\"- Daha fazla veri toplama\")\n",
    "                print(\"- SÄ±nÄ±f aÄŸÄ±rlÄ±klandÄ±rma\")\n",
    "                print(\"- Focal loss kullanma\")\n",
    "                \n",
    "        else:\n",
    "            print(\"Model deÄŸerlendirmesi henÃ¼z yapÄ±lmamÄ±ÅŸ.\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Performans analizi sÄ±rasÄ±nda hata: {e}\")\n",
    "\n",
    "def get_improvement_suggestions():\n",
    "    \"\"\"\n",
    "    GeliÅŸmiÅŸ iyileÅŸtirme Ã¶nerileri\n",
    "    \"\"\"\n",
    "    suggestions = {\n",
    "        \"Veri Ä°yileÅŸtirmeleri\": [\n",
    "            \"Veri temizleme ve outlier detection\",\n",
    "            \"Feature scaling yÃ¶ntemlerini karÅŸÄ±laÅŸtÄ±rma (RobustScaler, MinMaxScaler)\",\n",
    "            \"Veri artÄ±rma teknikleri (audio augmentation)\"\n",
    "        ],\n",
    "        \"Model Ä°yileÅŸtirmeleri\": [\n",
    "            \"Bidirectional LSTM kullanma\",\n",
    "            \"Attention mechanism ekleme\",\n",
    "            \"Residual connections\",\n",
    "            \"Batch normalization optimizasyonu\"\n",
    "        ],\n",
    "        \"EÄŸitim Ä°yileÅŸtirmeleri\": [\n",
    "            \"Learning rate scheduling\",\n",
    "            \"Gradient clipping\",\n",
    "            \"Warm-up strategies\",\n",
    "            \"Cyclical learning rates\"\n",
    "        ],\n",
    "        \"Ensemble YÃ¶ntemleri\": [\n",
    "            \"FarklÄ± model mimarilerini birleÅŸtirme\",\n",
    "            \"Voting classifiers\",\n",
    "            \"Stacking\",\n",
    "            \"Bagging\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    print(\"\\n=== GELÄ°ÅMÄ°Å Ä°YÄ°LEÅTÄ°RME Ã–NERÄ°LERÄ° ===\")\n",
    "    for category, items in suggestions.items():\n",
    "        print(f\"\\n{category}:\")\n",
    "        for item in items:\n",
    "            print(f\"  â€¢ {item}\")\n",
    "\n",
    "# Performans analizini Ã§alÄ±ÅŸtÄ±r\n",
    "analyze_model_performance()\n",
    "get_improvement_suggestions()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL EÄÄ°TÄ°MÄ° VE DEÄERLENDÄ°RMESÄ° TAMAMLANDI\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f66fac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "def pytorch_model_evaluation(model, val_loader, device):\n",
    "    \"\"\"\n",
    "    Evaluate PyTorch model on validation set and return accuracy.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    return correct / total\n",
    "\n",
    "def cross_validate_lstm(X, y, model_params, cv_folds=3, num_epochs=10):\n",
    "    \"\"\"\n",
    "    K-fold cross validation for PyTorch LSTM model\n",
    "    \"\"\"\n",
    "    skf = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=42)\n",
    "    cv_scores = []\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
    "        print(f\"Training fold {fold + 1}/{cv_folds}\")\n",
    "        \n",
    "        X_fold_train, X_fold_val = X[train_idx], X[val_idx]\n",
    "        y_fold_train, y_fold_val = y[train_idx], y[val_idx]\n",
    "        \n",
    "        # Create sequence data for this fold\n",
    "        X_train_seq, y_train_tensor = create_sequence_data(X_fold_train, y_fold_train, sequence_length=5)\n",
    "        X_val_seq, y_val_tensor = create_sequence_data(X_fold_val, y_fold_val, sequence_length=5)\n",
    "        \n",
    "        # Create data loaders\n",
    "        train_dataset = TensorDataset(X_train_seq, y_train_tensor)\n",
    "        val_dataset = TensorDataset(X_val_seq, y_val_tensor)\n",
    "        \n",
    "        fold_train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "        fold_val_loader = DataLoader(val_dataset, batch_size=256, shuffle=False)\n",
    "        \n",
    "        # Create and train model for this fold\n",
    "        fold_model = MusicGenreLSTM(**model_params).to(device)\n",
    "        fold_criterion = nn.CrossEntropyLoss()\n",
    "        fold_optimizer = optim.Adam(fold_model.parameters(), lr=0.001)\n",
    "        \n",
    "        # Train for fewer epochs in CV\n",
    "        for epoch in range(num_epochs):\n",
    "            fold_model.train()\n",
    "            for inputs, labels in fold_train_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                fold_optimizer.zero_grad()\n",
    "                outputs = fold_model(inputs)\n",
    "                loss = fold_criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                fold_optimizer.step()\n",
    "        \n",
    "        # Evaluate on validation set\n",
    "        accuracy = pytorch_model_evaluation(fold_model, fold_val_loader, device)\n",
    "        cv_scores.append(accuracy)\n",
    "        print(f\"Fold {fold + 1} accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    return np.mean(cv_scores), np.std(cv_scores)\n",
    "\n",
    "# Example usage for cross-validation (optional)\n",
    "if False:  # Set to True to run cross-validation\n",
    "    print(\"\\nPerforming cross-validation...\")\n",
    "    model_params = {\n",
    "        'input_size': X_train_seq.shape[2],\n",
    "        'hidden_size': 128,\n",
    "        'num_layers': 2,\n",
    "        'num_classes': len(le.classes_),\n",
    "        'dropout': 0.3\n",
    "    }\n",
    "    \n",
    "    cv_mean, cv_std = cross_validate_lstm(X_res, y_res, model_params)\n",
    "    print(f\"Cross-validation results: {cv_mean:.4f} (+/- {cv_std:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adafbaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "import itertools\n",
    "\n",
    "def train_and_evaluate_lstm(hidden_size, num_layers, dropout, learning_rate, X_train, y_train, X_val, y_val):\n",
    "    \"\"\"\n",
    "    Train and evaluate LSTM model with given hyperparameters.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create sequence data\n",
    "        X_train_seq, y_train_tensor = create_sequence_data(X_train, y_train, sequence_length=5)\n",
    "        X_val_seq, y_val_tensor = create_sequence_data(X_val, y_val, sequence_length=5)\n",
    "        \n",
    "        # Create data loaders\n",
    "        train_dataset = TensorDataset(X_train_seq, y_train_tensor)\n",
    "        val_dataset = TensorDataset(X_val_seq, y_val_tensor)\n",
    "        \n",
    "        train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=256, shuffle=False)\n",
    "        \n",
    "        # Create model\n",
    "        model_params = {\n",
    "            'input_size': X_train_seq.shape[2],\n",
    "            'hidden_size': hidden_size,\n",
    "            'num_layers': num_layers,\n",
    "            'num_classes': len(le.classes_),\n",
    "            'dropout': dropout\n",
    "        }\n",
    "        \n",
    "        tuning_model = MusicGenreLSTM(**model_params).to(device)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(tuning_model.parameters(), lr=learning_rate)\n",
    "        \n",
    "        # Train for a limited number of epochs\n",
    "        num_epochs = 5\n",
    "        for epoch in range(num_epochs):\n",
    "            tuning_model.train()\n",
    "            for inputs, labels in train_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = tuning_model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        \n",
    "        # Evaluate\n",
    "        accuracy = pytorch_model_evaluation(tuning_model, val_loader, device)\n",
    "        return accuracy\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in hyperparameter tuning: {e}\")\n",
    "        return 0.0\n",
    "\n",
    "def tune_hyperparameters_lstm(X_train, y_train, X_val, y_val):\n",
    "    \"\"\"\n",
    "    Grid search for optimal LSTM hyperparameters\n",
    "    \"\"\"\n",
    "    # Define parameter grid (reduced for faster execution)\n",
    "    param_grid = {\n",
    "        'hidden_size': [64, 128],\n",
    "        'num_layers': [2, 3],\n",
    "        'dropout': [0.3, 0.5],\n",
    "        'learning_rate': [0.001, 0.01]\n",
    "    }\n",
    "    \n",
    "    best_params = {}\n",
    "    best_score = 0\n",
    "    \n",
    "    print(\"Starting hyperparameter tuning...\")\n",
    "    \n",
    "    # Generate all combinations\n",
    "    keys = list(param_grid.keys())\n",
    "    values = list(param_grid.values())\n",
    "    \n",
    "    for i, combination in enumerate(itertools.product(*values)):\n",
    "        params = dict(zip(keys, combination))\n",
    "        print(f\"Testing combination {i+1}: {params}\")\n",
    "        \n",
    "        score = train_and_evaluate_lstm(\n",
    "            params['hidden_size'], \n",
    "            params['num_layers'], \n",
    "            params['dropout'],\n",
    "            params['learning_rate'],\n",
    "            X_train, y_train, X_val, y_val\n",
    "        )\n",
    "        \n",
    "        print(f\"Score: {score:.4f}\")\n",
    "        \n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_params = params.copy()\n",
    "    \n",
    "    return best_params, best_score\n",
    "\n",
    "# Example usage for hyperparameter tuning (optional)\n",
    "if False:  # Set to True to run hyperparameter tuning\n",
    "    print(\"\\nPerforming hyperparameter tuning...\")\n",
    "    best_params, best_score = tune_hyperparameters_lstm(X_res, y_res, X_val_clean, y_val_clean)\n",
    "    print(f\"\\nBest parameters: {best_params}\")\n",
    "    print(f\"Best score: {best_score:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pydebian (3.11.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
