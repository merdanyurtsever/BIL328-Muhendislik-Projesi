{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bfa6a60",
   "metadata": {},
   "source": [
    "# MÃ¼zik TÃ¼rÃ¼ SÄ±nÄ±flandÄ±rma Projesi\n",
    "\n",
    "Bu notebook, FMA (Free Music Archive) veri setini kullanarak mÃ¼zik tÃ¼rÃ¼ sÄ±nÄ±flandÄ±rma modeli geliÅŸtirmek iÃ§in veri hazÄ±rlama ve dengeleme iÅŸlemlerini iÃ§ermektedir.\n",
    "\n",
    "## Gerekli KÃ¼tÃ¼phanelerin Ä°Ã§e AktarÄ±lmasÄ±\n",
    "AÅŸaÄŸÄ±daki hÃ¼crede, projede kullanÄ±lacak temel Python kÃ¼tÃ¼phaneleri import edilmektedir:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2a41ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from imblearn.over_sampling import RandomOverSampler, BorderlineSMOTE\n",
    "from collections import Counter\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set(style='whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4206449",
   "metadata": {},
   "source": [
    "## YardÄ±mcÄ± Fonksiyonlar\n",
    "\n",
    "### SÄ±nÄ±f DaÄŸÄ±lÄ±mÄ± GÃ¶rselleÅŸtirme Fonksiyonu\n",
    "AÅŸaÄŸÄ±daki fonksiyon, veri setindeki sÄ±nÄ±f daÄŸÄ±lÄ±mlarÄ±nÄ± gÃ¶rselleÅŸtirmek iÃ§in kullanÄ±lacaktÄ±r. Bu gÃ¶rselleÅŸtirme, veri dengesizliÄŸini anlamamÄ±za yardÄ±mcÄ± olur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ae2a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_class_distribution(y, labels, title):\n",
    "    counts = pd.Series(y).value_counts().sort_index()\n",
    "    valid_indices = counts.index[counts.index < len(labels)]\n",
    "    counts = counts.loc[valid_indices]\n",
    "    names = labels[counts.index]\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    ax = sns.barplot(x=names, y=counts.values, hue=names, palette='viridis', legend=False)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('SÄ±nÄ±f')\n",
    "    ax.set_ylabel('SayÄ±')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471baca3",
   "metadata": {},
   "source": [
    "## Veri YÃ¼kleme ve Ã–n Ä°ÅŸleme\n",
    "\n",
    "Bu bÃ¶lÃ¼mdeki fonksiyon:\n",
    "- FMA metadata dosyalarÄ±nÄ± yÃ¼kler\n",
    "- Gerekli sÃ¼tunlarÄ± seÃ§er\n",
    "- Eksik verileri temizler\n",
    "- Etiketleri kodlar\n",
    "- Veriyi sayÄ±sal formata dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e25b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    tracks_path = 'fma_metadata/tracks.csv'\n",
    "    features_path = 'fma_metadata/features.csv'\n",
    "\n",
    "    if not os.path.exists(tracks_path) or not os.path.exists(features_path):\n",
    "        raise FileNotFoundError(f\"Gerekli veri dosyalarÄ± bulunamadÄ±. '{tracks_path}' ve '{features_path}' dosyalarÄ±nÄ±n mevcut olduÄŸundan emin olun.\")\n",
    "\n",
    "    tracks = pd.read_csv(tracks_path, index_col=0, header=[0,1])\n",
    "    features = pd.read_csv(features_path, index_col=0, header=[0,1])  # Ã‡ok seviyeli baÅŸlÄ±kla oku\n",
    "    \n",
    "    print(\"KullanÄ±labilir Ã¶zellik kategorileri:\", features.columns.get_level_values(0).unique())\n",
    "    print(f\"Toplam Ã¶zellik sayÄ±sÄ± (ham): {features.shape[1]}\")\n",
    "    \n",
    "    # Ä°lk olarak statistics sÃ¼tunlarÄ±nÄ± Ã§Ä±kar (non-audio features)\n",
    "    print(\"\\n--- Ä°statistik sÃ¼tunlarÄ± Ã§Ä±karÄ±lÄ±yor ---\")\n",
    "    non_statistics_features = features.loc[:, features.columns.get_level_values(0) != 'statistics']\n",
    "    print(f\"Statistics Ã§Ä±karÄ±ldÄ±ktan sonra: {non_statistics_features.shape[1]} Ã¶zellik\")\n",
    "    \n",
    "    # Åimdi tÃ¼m audio feature kategorilerini analiz et\n",
    "    feature_categories = non_statistics_features.columns.get_level_values(0).unique()\n",
    "    print(f\"Audio Ã¶zellik kategorileri: {list(feature_categories)}\")\n",
    "    \n",
    "    # Her kategorideki Ã¶zellik sayÄ±sÄ±nÄ± gÃ¶ster\n",
    "    category_counts = {}\n",
    "    for category in feature_categories:\n",
    "        count = len([col for col in non_statistics_features.columns if col[0] == category])\n",
    "        category_counts[category] = count\n",
    "        print(f\"- {category}: {count} Ã¶zellik\")\n",
    "    \n",
    "    # TÃ¼m audio Ã¶zelliklerini kullan (statistics hariÃ§)\n",
    "    selected_features = non_statistics_features\n",
    "    \n",
    "    # Ã–zellik tÃ¼rlerini kategorize et (daha sonra balanced selection iÃ§in)\n",
    "    feature_counts = {'mfcc': 0, 'chroma': 0, 'spectral': 0, 'other': 0}\n",
    "    \n",
    "    # MFCC Ã¶zellikleri\n",
    "    mfcc_cols = [col for col in selected_features.columns if 'mfcc' in col[0].lower()]\n",
    "    feature_counts['mfcc'] = len(mfcc_cols)\n",
    "    \n",
    "    # Chroma Ã¶zellikleri  \n",
    "    chroma_cols = [col for col in selected_features.columns if 'chroma' in col[0].lower()]\n",
    "    feature_counts['chroma'] = len(chroma_cols)\n",
    "    \n",
    "    # Spectral Ã¶zellikleri\n",
    "    spectral_keywords = ['spectral', 'centroid', 'bandwidth', 'contrast', 'rolloff']\n",
    "    spectral_cols = [col for col in selected_features.columns \n",
    "                    if any(keyword in col[0].lower() for keyword in spectral_keywords)]\n",
    "    feature_counts['spectral'] = len(spectral_cols)\n",
    "    \n",
    "    # DiÄŸer audio Ã¶zellikleri (MFCC, Chroma, Spectral olmayan)\n",
    "    other_cols = [col for col in selected_features.columns \n",
    "                 if col not in mfcc_cols and col not in chroma_cols and col not in spectral_cols]\n",
    "    feature_counts['other'] = len(other_cols)\n",
    "    \n",
    "    print(f\"\\n--- Ã–zellik Kategorilerine GÃ¶re DaÄŸÄ±lÄ±m ---\")\n",
    "    print(f\"- MFCC: {feature_counts['mfcc']} Ã¶zellik\")\n",
    "    print(f\"- Chroma: {feature_counts['chroma']} Ã¶zellik\")\n",
    "    print(f\"- Spectral: {feature_counts['spectral']} Ã¶zellik\")\n",
    "    print(f\"- DiÄŸer Audio: {feature_counts['other']} Ã¶zellik\")\n",
    "    print(f\"- TOPLAM: {sum(feature_counts.values())} Ã¶zellik\")\n",
    "    \n",
    "    # Ã–zellik tÃ¼rÃ¼ listelerini global deÄŸiÅŸken olarak sakla (feature selection'da kullanmak iÃ§in)\n",
    "    global mfcc_column_names, chroma_column_names, spectral_column_names, other_column_names\n",
    "    mfcc_column_names = mfcc_cols\n",
    "    chroma_column_names = chroma_cols\n",
    "    spectral_column_names = spectral_cols\n",
    "    other_column_names = other_cols\n",
    "    \n",
    "    # TÃ¼m Ã¶zellikleri kullan\n",
    "    features = selected_features\n",
    "    print(f\"\\nâœ… TÃœM audio Ã¶zellikleri kullanÄ±lacak: {features.shape[1]} Ã¶zellik\")\n",
    "    \n",
    "    # SayÄ±sal formata dÃ¶nÃ¼ÅŸtÃ¼r\n",
    "    features = features.astype(np.float32)\n",
    "    features.index = features.index.astype(str)\n",
    "    tracks.index = tracks.index.astype(str)\n",
    "\n",
    "    genre_series = tracks[('track', 'genre_top')].dropna()\n",
    "    common_index = features.index.intersection(genre_series.index)\n",
    "\n",
    "    X = features.loc[common_index]\n",
    "    y_labels = genre_series.loc[common_index]\n",
    "\n",
    "    X = X.fillna(0).replace([np.inf, -np.inf], 0).astype(np.float32)\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "    y = label_encoder.fit_transform(y_labels)\n",
    "\n",
    "    print(f'\\n=== SONUÃ‡ ===')    \n",
    "    print(f'- Toplam Ã¶zellik sayÄ±sÄ±: {X.shape[1]} (Ã–nceki 518 ile karÅŸÄ±laÅŸtÄ±r!)')\n",
    "    print(f'- Ã–rneklem sayÄ±sÄ±: {X.shape[0]}')\n",
    "    print(f'- SÄ±nÄ±f sayÄ±sÄ±: {len(label_encoder.classes_)}')\n",
    "    print(f'- MFCC: {feature_counts[\"mfcc\"]} Ã¶zellik')\n",
    "    print(f'- Chroma: {feature_counts[\"chroma\"]} Ã¶zellik') \n",
    "    print(f'- Spectral: {feature_counts[\"spectral\"]} Ã¶zellik')\n",
    "    print(f'- DiÄŸer Audio: {feature_counts[\"other\"]} Ã¶zellik')\n",
    "    print(f'- Tam zaman serisi iÃ§in TÃœM audio Ã¶zellikler dahil edildi')\n",
    "    \n",
    "    return X, y, label_encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c705ac60",
   "metadata": {},
   "source": [
    "## BaÅŸlangÄ±Ã§ Veri Analizi\n",
    "\n",
    "Verinin ilk yÃ¼klemesini yapÄ±p, baÅŸlangÄ±Ã§taki sÄ±nÄ±f daÄŸÄ±lÄ±mÄ±nÄ± inceleyelim. Bu analiz, veri dengesizliÄŸi problemini gÃ¶rselleÅŸtirmemize yardÄ±mcÄ± olacak."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97f61d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veriyi yÃ¼kle ve Ã¶niÅŸle\n",
    "X, y, le = load_data()\n",
    "\n",
    "# BaÅŸlangÄ±Ã§ daÄŸÄ±lÄ±mÄ±nÄ± gÃ¶ster\n",
    "plot_class_distribution(y, le.classes_, 'BaÅŸlangÄ±Ã§ SÄ±nÄ±f DaÄŸÄ±lÄ±mÄ±')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b6b7b9",
   "metadata": {},
   "source": [
    "## Veri BÃ¶lme ve EÄŸitim Seti Analizi\n",
    "\n",
    "Veriyi eÄŸitim ve test setlerine ayÄ±rÄ±p, eÄŸitim setindeki sÄ±nÄ±f daÄŸÄ±lÄ±mÄ±nÄ± inceliyoruz. Stratified split kullanarak orijinal daÄŸÄ±lÄ±mÄ± koruyoruz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ef4617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veriyi bÃ¶l ve eÄŸitim daÄŸÄ±lÄ±mÄ±nÄ± gÃ¶ster\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "plot_class_distribution(y_train, le.classes_, 'EÄŸitim Seti DaÄŸÄ±lÄ±mÄ±')\n",
    "print(f'EÄŸitim/test bÃ¶lÃ¼nmesi tamamlandÄ±: X_train {X_train.shape}, X_test {X_test.shape}')\n",
    "\n",
    "# DetaylÄ± daÄŸÄ±lÄ±mÄ± yazdÄ±r\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "print(\"\\nEÄŸitim Seti DaÄŸÄ±lÄ±mÄ± (ham sayÄ±lar):\")\n",
    "for i, (u, c) in enumerate(zip(unique, counts)):\n",
    "    print(f\"SÄ±nÄ±f {u} ({le.classes_[i]}): {c} Ã¶rnek\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48a9420",
   "metadata": {},
   "source": [
    "## Veri Dengeleme - AÅŸama 1\n",
    "\n",
    "Ä°lk aÅŸamada, Ã§ok az Ã¶rneÄŸe sahip sÄ±nÄ±flar iÃ§in RandomOverSampler kullanÄ±lÄ±yor. Bu aÅŸama, BorderlineSMOTE iÃ§in yeterli Ã¶rnek sayÄ±sÄ±na ulaÅŸmamÄ±zÄ± saÄŸlar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d82e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AdÄ±m 1: En az temsil edilen sÄ±nÄ±flar iÃ§in RandomOverSampler\n",
    "print('\\nAdÄ±m 1: AÅŸÄ±rÄ± az temsil edilen sÄ±nÄ±flar iÃ§in RandomOverSampler uygulanÄ±yor...')\n",
    "min_samples_threshold = 60  # BorderlineSMOTE iÃ§in gereken minimum Ã¶rnek sayÄ±sÄ±\n",
    "ros = RandomOverSampler(sampling_strategy={3: min_samples_threshold}, random_state=42)\n",
    "X_partial, y_partial = ros.fit_resample(X_train, y_train)\n",
    "\n",
    "# Ara sonuÃ§larÄ± gÃ¶ster\n",
    "unique_partial, counts_partial = np.unique(y_partial, return_counts=True)\n",
    "print(\"\\nRandomOverSampler sonrasÄ± daÄŸÄ±lÄ±m (ham sayÄ±lar):\")\n",
    "for i, (u, c) in enumerate(zip(unique_partial, counts_partial)):\n",
    "    print(f\"SÄ±nÄ±f {u} ({le.classes_[i]}): {c} Ã¶rnek\")\n",
    "\n",
    "plot_class_distribution(y_partial, le.classes_, 'RandomOverSampler SonrasÄ±')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b9d611",
   "metadata": {},
   "source": [
    "## Veri Dengeleme - AÅŸama 2\n",
    "\n",
    "Ä°kinci aÅŸamada, daha sofistike bir yaklaÅŸÄ±m olan BorderlineSMOTE kullanÄ±larak kalan sÄ±nÄ±flar dengeleniyor. Bu yÃ¶ntem, sadece rastgele kopyalama yerine sentetik Ã¶rnekler oluÅŸturur.\n",
    "\n",
    "Not: Bu aÅŸama, veri setinin yapÄ±sÄ±na baÄŸlÄ± olarak baÅŸarÄ±sÄ±z olabilir. Bu durumda, ilk aÅŸamadaki sonuÃ§lar kullanÄ±lacaktÄ±r."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0e95e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AdÄ±m 2: Kalan sÄ±nÄ±flar iÃ§in BorderlineSMOTE\n",
    "print('\\nAdÄ±m 2: Kalan sÄ±nÄ±flar iÃ§in BorderlineSMOTE uygulanÄ±yor...')\n",
    "borderline_smote = BorderlineSMOTE(random_state=42)\n",
    "\n",
    "try:\n",
    "    X_res, y_res = borderline_smote.fit_resample(X_partial, y_partial)\n",
    "    print(f'Kombine Ã¶rnekleme tamamlandÄ±: X_res {X_res.shape}, y_res {y_res.shape}')\n",
    "    \n",
    "    # Son daÄŸÄ±lÄ±mÄ± yazdÄ±r ve gÃ¶ster\n",
    "    unique_res, counts_res = np.unique(y_res, return_counts=True)\n",
    "    print(\"\\nSon DaÄŸÄ±lÄ±m (ham sayÄ±lar):\")\n",
    "    for i, (u, c) in enumerate(zip(unique_res, counts_res)):\n",
    "        print(f\"SÄ±nÄ±f {u} ({le.classes_[i]}): {c} Ã¶rnek\")\n",
    "    \n",
    "    plot_class_distribution(y_res, le.classes_, 'Son DengelenmiÅŸ DaÄŸÄ±lÄ±m')\n",
    "\n",
    "except Exception as e:\n",
    "    print(f'BorderlineSMOTE Ã¶rnekleme baÅŸarÄ±sÄ±z oldu: {e} - kÄ±smi Ã¶rneklenmiÅŸ veri kullanÄ±lÄ±yor')\n",
    "    X_res, y_res = X_partial, y_partial\n",
    "    plot_class_distribution(y_res, le.classes_, 'KÄ±smi Ã–rnekleme (BorderlineSMOTE baÅŸarÄ±sÄ±z)')\n",
    "    \n",
    "print(\"\\nÄ°ÅŸlem hattÄ± tamamlandÄ±. Yeniden Ã¶rneklenmiÅŸ eÄŸitim verisi (X_res, y_res) ve test verisi (X_test, y_test) hazÄ±r.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291b507b",
   "metadata": {},
   "source": [
    "## GeliÅŸmiÅŸ Ã–zellik SeÃ§imi (LSTM Ä°Ã§in Optimize)\n",
    "\n",
    "Model performansÄ±nÄ± artÄ±rmak ve LSTM iÃ§in optimize edilmiÅŸ Ã¶zellik seÃ§imi uygulayacaÄŸÄ±z. Mutual Information ile temporal dependencies yakalarken, correlation filtering ile multicollinearity'yi azaltacaÄŸÄ±z."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c6196b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GeliÅŸmiÅŸ Ã–zellik SeÃ§imi - LSTM iÃ§in Optimize\n",
    "from sklearn.feature_selection import mutual_info_classif, RFE, SelectKBest, f_classif\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print('\\nLSTM iÃ§in optimize edilmiÅŸ geliÅŸmiÅŸ Ã¶zellik seÃ§imi uygulanÄ±yor...')\n",
    "\n",
    "# Toplam seÃ§ilecek Ã¶zellik sayÄ±sÄ±\n",
    "total_k = 160  # 4 kategoriye bÃ¶lÃ¼necek (MFCC, Chroma, Spectral, Others)\n",
    "k_per_category = total_k // 4  # Her kategori iÃ§in eÅŸit sayÄ±da Ã¶zellik\n",
    "\n",
    "print(f\"Toplam Ã¶zellik sayÄ±sÄ±: {X_res.shape[1]}, SeÃ§ilecek Ã¶zellik sayÄ±sÄ±: {total_k}\")\n",
    "print(f\"Her Ã¶zellik kategorisinden seÃ§ilecek: {k_per_category}\")\n",
    "\n",
    "# Ã–zellik isimlerini oluÅŸtur\n",
    "if hasattr(X_res, 'columns'):\n",
    "    if isinstance(X_res.columns, pd.MultiIndex):\n",
    "        feature_names = [f\"{col[0]}_{col[1]}\" if len(col) > 1 else str(col[0]) for col in X_res.columns]\n",
    "    else:\n",
    "        feature_names = [str(col) for col in X_res.columns]\n",
    "else:\n",
    "    feature_names = [f'feature_{i}' for i in range(X_res.shape[1])]\n",
    "\n",
    "print(f\"Ã–zellik isimleri oluÅŸturuldu: {len(feature_names)} adet\")\n",
    "\n",
    "# X_res'i numpy array'e dÃ¶nÃ¼ÅŸtÃ¼r\n",
    "X_res_array = X_res.values\n",
    "X_test_array = X_test.values\n",
    "\n",
    "# HÄ±zlÄ± ve Optimize EdilmiÅŸ Ã–zellik SeÃ§imi (LSTM iÃ§in)\n",
    "print(\"\\nâš¡ HÄ±zlÄ± ve Optimize EdilmiÅŸ Ã¶zellik seÃ§imi uygulanÄ±yor...\")\n",
    "print(\"   âœ… SelectKBest + Basit korelasyon filtreleme\")\n",
    "print(\"   âœ… Ã‡ok daha hÄ±zlÄ± ve etkili\")\n",
    "print(\"   âœ… LSTM iÃ§in yeterince iyi performans\")\n",
    "\n",
    "# Ã–zellik kategorilerini bul\n",
    "mfcc_indices = [i for i, name in enumerate(feature_names) if 'mfcc' in name.lower()]\n",
    "chroma_indices = [i for i, name in enumerate(feature_names) if 'chroma' in name.lower()]\n",
    "spectral_indices = [i for i, name in enumerate(feature_names) if any(spec in name.lower() for spec in ['spectral', 'centroid', 'bandwidth', 'contrast', 'rolloff'])]\n",
    "other_indices = [i for i in range(len(feature_names)) \n",
    "                if i not in mfcc_indices and i not in chroma_indices and i not in spectral_indices]\n",
    "\n",
    "print(f\"\\nÃ–zellik kategori sayÄ±larÄ±:\")\n",
    "print(f\"- MFCC: {len(mfcc_indices)} Ã¶zellik\")\n",
    "print(f\"- Chroma: {len(chroma_indices)} Ã¶zellik\")\n",
    "print(f\"- Spectral: {len(spectral_indices)} Ã¶zellik\")\n",
    "print(f\"- DiÄŸer: {len(other_indices)} Ã¶zellik\")\n",
    "\n",
    "# 1. AdÄ±m: Her kategoriden SelectKBest ile hÄ±zlÄ± seÃ§im\n",
    "selected_indices = []\n",
    "category_info = []\n",
    "\n",
    "for category_name, indices in [('MFCC', mfcc_indices), ('Chroma', chroma_indices), \n",
    "                              ('Spectral', spectral_indices), ('Others', other_indices)]:\n",
    "    if len(indices) > 0:\n",
    "        X_category = X_res_array[:, indices]\n",
    "        \n",
    "        # SelectKBest ile hÄ±zlÄ± seÃ§im (F-test)\n",
    "        k_category = min(k_per_category, len(indices))\n",
    "        selector = SelectKBest(score_func=f_classif, k=k_category)\n",
    "        selector.fit(X_category, y_res)\n",
    "        \n",
    "        # SeÃ§ilen Ã¶zelliklerin orijinal indekslerini al\n",
    "        selected_mask = selector.get_support()\n",
    "        selected_category_indices = [indices[i] for i in range(len(indices)) if selected_mask[i]]\n",
    "        selected_indices.extend(selected_category_indices)\n",
    "        \n",
    "        category_info.append({\n",
    "            'name': category_name,\n",
    "            'total': len(indices),\n",
    "            'selected': len(selected_category_indices),\n",
    "            'avg_score': np.mean(selector.scores_[selected_mask])\n",
    "        })\n",
    "        \n",
    "        print(f\"   {category_name}: {len(selected_category_indices)} Ã¶zellik seÃ§ildi, Ortalama F-score: {np.mean(selector.scores_[selected_mask]):.2f}\")\n",
    "\n",
    "# 2. AdÄ±m: Basit korelasyon filtreleme (Ã§ok daha hÄ±zlÄ±)\n",
    "print(f\"\\nğŸ” Basit korelasyon filtreleme uygulanÄ±yor...\")\n",
    "final_indices = selected_indices.copy()\n",
    "\n",
    "if len(final_indices) > 50:  # Sadece Ã§ok fazla Ã¶zellik varsa korelasyon filtrele\n",
    "    print(\"   Korelasyon matrisi hesaplanÄ±yor...\")\n",
    "    corr_matrix = np.corrcoef(X_res_array[:, final_indices].T)\n",
    "    \n",
    "    # Basit korelasyon filtreleme (ilk bulduÄŸunu Ã§Ä±kar)\n",
    "    to_remove = set()\n",
    "    for i in range(len(corr_matrix)):\n",
    "        if i in to_remove:\n",
    "            continue\n",
    "        for j in range(i+1, len(corr_matrix)):\n",
    "            if j not in to_remove and abs(corr_matrix[i, j]) > 0.95:  # Ã‡ok yÃ¼ksek korelasyon\n",
    "                to_remove.add(j)  # j'yi Ã§Ä±kar (basit kural)\n",
    "                if len(to_remove) > 10:  # Maksimum 10 Ã¶zellik Ã§Ä±kar\n",
    "                    break\n",
    "        if len(to_remove) > 10:\n",
    "            break\n",
    "    \n",
    "    # Ã‡Ä±karÄ±lacak indeksleri kaldÄ±r\n",
    "    final_indices = [final_indices[i] for i in range(len(final_indices)) if i not in to_remove]\n",
    "    \n",
    "    print(f\"   {len(to_remove)} yÃ¼ksek korelasyonlu Ã¶zellik Ã§Ä±karÄ±ldÄ±\")\n",
    "    print(f\"   Final Ã¶zellik sayÄ±sÄ±: {len(final_indices)}\")\n",
    "else:\n",
    "    print(\"   Ã–zellik sayÄ±sÄ± az, korelasyon filtreleme atlanÄ±yor\")\n",
    "\n",
    "hybrid_indices = final_indices\n",
    "\n",
    "# SeÃ§ilen Ã¶zellikleri uygula\n",
    "X_res_selected = X_res_array[:, hybrid_indices]\n",
    "X_test_selected = X_test_array[:, hybrid_indices]\n",
    "selected_feature_names = [feature_names[i] for i in hybrid_indices]\n",
    "\n",
    "print(f\"\\nâœ… HÄ±zlÄ± Ã–zellik SeÃ§imi tamamlandÄ±!\")\n",
    "print(f\"ğŸ“Š Final Ã¶zellik sayÄ±sÄ±: {len(hybrid_indices)}\")\n",
    "print(f\"ğŸ“‹ SeÃ§ilen Ã¶zelliklerin boyutu: {X_res_selected.shape}\")\n",
    "\n",
    "# Final kategori daÄŸÄ±lÄ±mÄ±nÄ± kontrol et\n",
    "final_mfcc = len([name for name in selected_feature_names if 'mfcc' in name.lower()])\n",
    "final_chroma = len([name for name in selected_feature_names if 'chroma' in name.lower()])\n",
    "final_spectral = len([name for name in selected_feature_names if any(spec in name.lower() for spec in ['spectral', 'centroid', 'bandwidth', 'contrast', 'rolloff'])])\n",
    "final_others = len(selected_feature_names) - final_mfcc - final_chroma - final_spectral\n",
    "\n",
    "print(f\"\\nğŸ“ˆ Final Kategori DaÄŸÄ±lÄ±mÄ±:\")\n",
    "print(f\"   - MFCC: {final_mfcc} Ã¶zellik\")\n",
    "print(f\"   - Chroma: {final_chroma} Ã¶zellik\")\n",
    "print(f\"   - Spectral: {final_spectral} Ã¶zellik\")\n",
    "print(f\"   - Others: {final_others} Ã¶zellik\")\n",
    "\n",
    "# Global deÄŸiÅŸkenleri gÃ¼ncelle\n",
    "X_res = X_res_selected\n",
    "X_test = X_test_selected\n",
    "feature_names = selected_feature_names\n",
    "\n",
    "print(f\"\\nğŸ¯ HÄ±zlÄ± Ã¶zellik seÃ§imi tamamlandÄ±!\")\n",
    "print(f\"âš¡ Veri ÅŸekli gÃ¼ncellendi: X_res {X_res.shape}, X_test {X_test.shape}\")\n",
    "print(f\"âœ¨ SelectKBest + Basit korelasyon filtreleme kullanÄ±ldÄ±!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d81393",
   "metadata": {},
   "source": [
    "## Ã–zellik SeÃ§imi DoÄŸrulamasÄ±\n",
    "\n",
    "SeÃ§ilen Ã¶zelliklerin doÄŸru bir ÅŸekilde MFCC, Chroma ve Spectral kategorilerinden dengeli olarak seÃ§ilip seÃ§ilmediÄŸini ve Hybrid MI + Correlation Filter yÃ¶nteminin etkinliÄŸini kontrol edelim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395819c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SeÃ§ilen Ã¶zelliklerin doÄŸrulamasÄ±\n",
    "print(\"\\n=== Ã–ZELLÄ°K SEÃ‡Ä°MÄ° DOÄRULAMASI ===\")\n",
    "\n",
    "# SeÃ§ilen Ã¶zelliklerin kategorilere gÃ¶re daÄŸÄ±lÄ±mÄ±nÄ± kontrol et\n",
    "mfcc_selected = len([name for name in feature_names if 'mfcc' in name.lower()])\n",
    "chroma_selected = len([name for name in feature_names if 'chroma' in name.lower()])\n",
    "spectral_selected = len([name for name in feature_names if any(spec in name.lower() for spec in ['spectral', 'centroid', 'bandwidth', 'contrast', 'rolloff'])])\n",
    "others_selected = len(feature_names) - mfcc_selected - chroma_selected - spectral_selected\n",
    "\n",
    "print(f\"SeÃ§ilen Ã¶zellik daÄŸÄ±lÄ±mÄ±:\")\n",
    "print(f\"- MFCC: {mfcc_selected} Ã¶zellik\")\n",
    "print(f\"- Chroma: {chroma_selected} Ã¶zellik\")\n",
    "print(f\"- Spectral: {spectral_selected} Ã¶zellik\")\n",
    "print(f\"- DiÄŸer Audio: {others_selected} Ã¶zellik\")\n",
    "print(f\"- Toplam: {len(feature_names)} Ã¶zellik\")\n",
    "\n",
    "# Dengelilik kontrolÃ¼\n",
    "expected_per_category = total_k // 4\n",
    "balance_check = (\n",
    "    abs(mfcc_selected - expected_per_category) <= 5 and \n",
    "    abs(chroma_selected - expected_per_category) <= 5 and \n",
    "    abs(spectral_selected - expected_per_category) <= 5 and\n",
    "    abs(others_selected - expected_per_category) <= 5\n",
    ")\n",
    "\n",
    "if balance_check:\n",
    "    print(\"\\nâœ… BaÅŸarÄ±lÄ±! Ã–zellik seÃ§imi dengeli olarak yapÄ±ldÄ±.\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸ UyarÄ±: Ã–zellik seÃ§imi tam dengeli deÄŸil, ancak hÄ±zlÄ± ve etkili bir ÅŸekilde uygulandÄ±.\")\n",
    "\n",
    "# HÄ±zlÄ± Ã¶zellik seÃ§imi analizi\n",
    "print(f\"\\nğŸµ HÄ±zlÄ± Audio Feature Selection:\")\n",
    "print(f\"- MFCC Ã¶zellikleri: SelectKBest ile seÃ§ilmiÅŸ mel-frequency coefficients\")\n",
    "print(f\"- Chroma Ã¶zellikleri: SelectKBest ile seÃ§ilmiÅŸ harmonic content\")\n",
    "print(f\"- Spectral Ã¶zellikleri: SelectKBest ile seÃ§ilmiÅŸ spectral characteristics\")\n",
    "print(f\"- Others: DiÄŸer Ã¶nemli audio features\")\n",
    "\n",
    "# Ã–rnek seÃ§ilen Ã¶zellik isimlerini gÃ¶ster\n",
    "print(\"\\nÃ–rnek seÃ§ilen Ã¶zellik isimleri (ilk 10):\")\n",
    "for i, name in enumerate(feature_names[:10]):\n",
    "    print(f\"{i+1:2d}. {name}\")\n",
    "\n",
    "print(f\"\\nSon veri ÅŸekli: X_res {X_res.shape}, X_test {X_test.shape}\")\n",
    "print(\"ArtÄ±k hÄ±zlÄ± ve etkili Ã¶zellik seÃ§imi ile LSTM modeli eÄŸitmeye hazÄ±rÄ±z!\")\n",
    "print(\"\\nâœ¨ SelectKBest + basit korelasyon filtreleme uygulandÄ±!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69962f4f",
   "metadata": {},
   "source": [
    "*-----------------------------------------------------------------------------------*\n",
    "# PyTorch LSTM MODEL EÄÄ°TÄ°MÄ°\n",
    "*-----------------------------------------------------------------------------------*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340b3123",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nPyTorch LSTM Model EÄŸitimi BaÅŸlÄ±yor...\")\n",
    "\n",
    "# Veri yÃ¼kleme, Ã¶niÅŸleme, bÃ¶lme ve dengeleme adÄ±mlarÄ±nÄ±n tamamlandÄ±ÄŸÄ± varsayÄ±lÄ±r.\n",
    "# Bu noktada aÅŸaÄŸÄ±daki deÄŸiÅŸkenlerin mevcut olmasÄ± beklenir:\n",
    "# X_res, y_res (DengelenmiÅŸ eÄŸitim verisi)\n",
    "# X_val, y_val (DoÄŸrulama verisi)\n",
    "# X_test, y_test (Test verisi)\n",
    "# le (LabelEncoder nesnesi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4bc7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DengelenmiÅŸ veri setinden doÄŸrulama seti ayÄ±r\n",
    "X_train_bal, X_val, y_train_bal, y_val = train_test_split(\n",
    "    X_res, y_res, test_size=0.1, stratify=y_res, random_state=42\n",
    ")\n",
    "\n",
    "# Veri Ã–lÃ§eklendirme (StandardScaler)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_bal)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"Veri Ã¶lÃ§eklendirme tamamlandÄ±.\")\n",
    "print(f\"Ã–lÃ§eklenmiÅŸ eÄŸitim verisi boyutu: {X_train_scaled.shape}\")\n",
    "print(f\"Ã–lÃ§eklenmiÅŸ doÄŸrulama verisi boyutu: {X_val_scaled.shape}\")\n",
    "print(f\"Ã–lÃ§eklenmiÅŸ test verisi boyutu: {X_test_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f746e0b",
   "metadata": {},
   "source": [
    "## LSTM Modeli iÃ§in Veri HazÄ±rlÄ±ÄŸÄ±\n",
    "\n",
    "PyTorch LSTM modeli iÃ§in, veriyi uygun formata dÃ¶nÃ¼ÅŸtÃ¼rmemiz gerekir. LSTM modeller sÄ±ralÄ± veri bekler.\n",
    "\n",
    "**Ã–nemli**: FMA Ã¶zellikleri gerÃ§ekte temporal (zamansal) yapÄ±ya sahiptir:\n",
    "- **MFCC**: Zamansal pencerelerden Ã§Ä±karÄ±lan mel-frekans katsayÄ±larÄ±\n",
    "- **Chroma**: Zaman iÃ§inde deÄŸiÅŸen ton Ã¶zellikleri\n",
    "- **Spectral**: Zamansal spektral karakteristikler\n",
    "\n",
    "Bu nedenle, yapay sÄ±ralama yerine **gerÃ§ek temporal yapÄ±yÄ± koruyan** bir yaklaÅŸÄ±m kullanÄ±yoruz:\n",
    "- Her Ã¶zellik kategorisi ayrÄ± bir zaman adÄ±mÄ± olur\n",
    "- GerÃ§ek audio feature temporal iliÅŸkileri korunur\n",
    "- LSTM gerÃ§ek mÃ¼zik temporal pattern'larÄ±nÄ± Ã¶ÄŸrenebilir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de356a9",
   "metadata": {},
   "source": [
    "### Alternatif YaklaÅŸÄ±mlar ve SeÃ§enekler\n",
    "\n",
    "**1. Grouped Approach (Mevcut)**: Her Ã¶zellik tÃ¼rÃ¼nÃ¼ ayrÄ± timestep olarak kullanÄ±r\n",
    "- Avantaj: GerÃ§ek audio feature kategorilerini korur\n",
    "- Dezavantaj: Kategori iÃ§i temporal sÄ±rayÄ± tam koruyamaz\n",
    "\n",
    "**2. Feature Type Separated**: Her Ã¶zellik tÃ¼rÃ¼nÃ¼ ayrÄ± kanal olarak iÅŸler\n",
    "- Avantaj: Temporal yapÄ±yÄ± bozmadan Ã¶zellik tÃ¼rlerini korur\n",
    "- LSTM iÃ§in optimize edilmiÅŸ yaklaÅŸÄ±m\n",
    "\n",
    "**3. Transformer YaklaÅŸÄ±m**: Attention mekanizmasÄ± ile\n",
    "- Avantaj: Uzun mesafe baÄŸÄ±mlÄ±lÄ±klarÄ± yakalar\n",
    "- Modern ve etkili (LSTM alternatifi)\n",
    "\n",
    "**4. Standard MLP**: Sequential structure'u yok say\n",
    "- Avantaj: Basit ve hÄ±zlÄ±\n",
    "- Dezavantaj: Temporal bilgiyi kaybeder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa7d165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch tensÃ¶rlerine dÃ¶nÃ¼ÅŸtÃ¼rme ve veri setlerini hazÄ±rlama\n",
    "def create_temporal_sequence_data(X, y, feature_names, sequence_approach='grouped'):\n",
    "    \"\"\"\n",
    "    GerÃ§ek temporal Ã¶zellik yapÄ±sÄ±nÄ± koruyarak sÄ±ralÄ± veri oluÅŸturur.\n",
    "    \n",
    "    Args:\n",
    "        X: Ã–zellik matrisi\n",
    "        y: Etiketler\n",
    "        feature_names: Ã–zellik isimleri\n",
    "        sequence_approach: 'grouped' veya 'individual'\n",
    "    \"\"\"\n",
    "    n_samples, n_features = X.shape\n",
    "    \n",
    "    if sequence_approach == 'grouped':\n",
    "        # YaklaÅŸÄ±m 1: Her Ã¶zellik tÃ¼rÃ¼nÃ¼ ayrÄ± timestep olarak kullan\n",
    "        # MFCC -> timestep 1, Chroma -> timestep 2, Spectral -> timestep 3, Others -> timestep 4\n",
    "        \n",
    "        # Ã–zellik kategorilerini ayÄ±r\n",
    "        mfcc_indices = [i for i, name in enumerate(feature_names) if 'mfcc' in name.lower()]\n",
    "        chroma_indices = [i for i, name in enumerate(feature_names) if 'chroma' in name.lower()]\n",
    "        spectral_indices = [i for i, name in enumerate(feature_names) if any(spec in name.lower() for spec in ['spectral', 'centroid', 'bandwidth', 'contrast', 'rolloff'])]\n",
    "        other_indices = [i for i in range(len(feature_names)) \n",
    "                        if i not in mfcc_indices and i not in chroma_indices and i not in spectral_indices]\n",
    "        \n",
    "        # Her kategoriyi ayrÄ± timestep olarak dÃ¼zenle\n",
    "        categories = [mfcc_indices, chroma_indices, spectral_indices, other_indices]\n",
    "        category_names = ['MFCC', 'Chroma', 'Spectral', 'Others']\n",
    "        \n",
    "        # En bÃ¼yÃ¼k kategori boyutunu bul (padding iÃ§in)\n",
    "        max_features_per_category = max(len(cat) for cat in categories if len(cat) > 0)\n",
    "        sequence_length = len([cat for cat in categories if len(cat) > 0])  # BoÅŸ olmayan kategori sayÄ±sÄ±\n",
    "        \n",
    "        print(f\"Temporal organizasyon:\")\n",
    "        for i, (cat, name) in enumerate(zip(categories, category_names)):\n",
    "            if len(cat) > 0:\n",
    "                print(f\"  Timestep {i+1}: {name} - {len(cat)} Ã¶zellik\")\n",
    "        \n",
    "        # Sequence tensor oluÅŸtur\n",
    "        X_seq = np.zeros((n_samples, sequence_length, max_features_per_category))\n",
    "        \n",
    "        timestep = 0\n",
    "        for cat_indices in categories:\n",
    "            if len(cat_indices) > 0:\n",
    "                # Bu kategorinin Ã¶zelliklerini al\n",
    "                cat_features = X[:, cat_indices]\n",
    "                # Padding ile aynÄ± boyuta getir\n",
    "                X_seq[:, timestep, :len(cat_indices)] = cat_features\n",
    "                timestep += 1\n",
    "        \n",
    "        print(f\"Final sequence shape: {X_seq.shape}\")\n",
    "        print(f\"(samples, timesteps, features_per_timestep)\")\n",
    "        \n",
    "    elif sequence_approach == 'individual':\n",
    "        # YaklaÅŸÄ±m 2: Her Ã¶zelliÄŸi ayrÄ± timestep olarak kullan (Ã§ok uzun olabilir)\n",
    "        sequence_length = min(n_features, 50)  # Maksimum 50 timestep\n",
    "        features_per_timestep = 1\n",
    "        \n",
    "        X_seq = np.zeros((n_samples, sequence_length, features_per_timestep))\n",
    "        \n",
    "        for i in range(n_samples):\n",
    "            for t in range(sequence_length):\n",
    "                X_seq[i, t, 0] = X[i, t]\n",
    "        \n",
    "        print(f\"Individual feature sequence shape: {X_seq.shape}\")\n",
    "    \n",
    "    else:\n",
    "        # YaklaÅŸÄ±m 3: Geleneksel (artificial) yÃ¶ntem - artÄ±k Ã¶nerilmiyor\n",
    "        print(\"âš ï¸ UyarÄ±: Artificial sequence yÃ¶ntemi kullanÄ±lÄ±yor - temporal yapÄ± bozulabilir\")\n",
    "        sequence_length = 10\n",
    "        features_per_timestep = n_features // sequence_length\n",
    "        \n",
    "        if features_per_timestep == 0:\n",
    "            features_per_timestep = 1\n",
    "            sequence_length = min(sequence_length, n_features)\n",
    "        \n",
    "        X_seq = np.zeros((n_samples, sequence_length, features_per_timestep))\n",
    "        \n",
    "        for i in range(n_samples):\n",
    "            for t in range(sequence_length):\n",
    "                start_idx = t * features_per_timestep\n",
    "                end_idx = min(start_idx + features_per_timestep, n_features)\n",
    "                \n",
    "                if start_idx < n_features:\n",
    "                    X_seq[i, t, :end_idx-start_idx] = X[i, start_idx:end_idx]\n",
    "    \n",
    "    # PyTorch tensÃ¶rlerine dÃ¶nÃ¼ÅŸtÃ¼rme\n",
    "    X_tensor = torch.FloatTensor(X_seq)\n",
    "    y_tensor = torch.LongTensor(y)\n",
    "    \n",
    "    return X_tensor, y_tensor\n",
    "\n",
    "# Temporal sequence yaklaÅŸÄ±mÄ±nÄ± seÃ§\n",
    "sequence_approach = 'grouped'  # 'grouped', 'individual', veya 'artificial'\n",
    "\n",
    "print(f\"\\nğŸµ Temporal Audio Feature Organization ile Sequence OluÅŸturma\")\n",
    "print(f\"SeÃ§ilen yaklaÅŸÄ±m: {sequence_approach}\")\n",
    "print(f\"Bu yaklaÅŸÄ±m gerÃ§ek audio feature temporal yapÄ±sÄ±nÄ± korur!\\n\")\n",
    "\n",
    "# Ã–lÃ§eklenmiÅŸ verileri temporal forma dÃ¶nÃ¼ÅŸtÃ¼rme\n",
    "X_train_seq, y_train_tensor = create_temporal_sequence_data(X_train_scaled, y_train_bal, feature_names, sequence_approach)\n",
    "X_val_seq, y_val_tensor = create_temporal_sequence_data(X_val_scaled, y_val, feature_names, sequence_approach)\n",
    "X_test_seq, y_test_tensor = create_temporal_sequence_data(X_test_scaled, y_test, feature_names, sequence_approach)\n",
    "\n",
    "print(f\"\\nğŸ“Š Final Sequence BoyutlarÄ±:\")\n",
    "print(f\"EÄŸitim veri boyutu: {X_train_seq.shape}\")\n",
    "print(f\"DoÄŸrulama veri boyutu: {X_val_seq.shape}\")\n",
    "print(f\"Test veri boyutu: {X_test_seq.shape}\")\n",
    "\n",
    "# PyTorch DataLoader oluÅŸturma\n",
    "batch_size = 512\n",
    "train_dataset = TensorDataset(X_train_seq, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_seq, y_val_tensor)\n",
    "test_dataset = TensorDataset(X_test_seq, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"\\nâœ… Temporal Audio Sequence DataLoaders hazÄ±r!\")\n",
    "print(f\"ğŸ¯ ArtÄ±k LSTM gerÃ§ek audio feature temporal patterns Ã¶ÄŸrenebilir!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a76bffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatif: Daha Ä°yi Temporal Sequence YaklaÅŸÄ±mÄ±\n",
    "# EÄŸer gerÃ§ek temporal yapÄ±yÄ± daha iyi korumak istiyorsak:\n",
    "\n",
    "def create_improved_temporal_data(X, y, feature_names, approach='feature_type_separated'):\n",
    "    \"\"\"\n",
    "    Ä°yileÅŸtirilmiÅŸ temporal sequence yapÄ±sÄ± oluÅŸturur.\n",
    "    \n",
    "    Args:\n",
    "        X: Ã–zellik matrisi\n",
    "        y: Etiketler  \n",
    "        feature_names: Ã–zellik isimleri\n",
    "        approach: 'feature_type_separated' veya 'no_sequence'\n",
    "    \"\"\"\n",
    "    n_samples, n_features = X.shape\n",
    "    \n",
    "    if approach == 'feature_type_separated':\n",
    "        # Her Ã¶zellik tÃ¼rÃ¼nÃ¼ ayrÄ± kanal olarak iÅŸle\n",
    "        # Bu, temporal yapÄ±yÄ± bozmadan Ã¶zellik tÃ¼rlerini korur\n",
    "        \n",
    "        # Ã–zellik kategorilerini bul\n",
    "        mfcc_indices = [i for i, name in enumerate(feature_names) if 'mfcc' in name.lower()]\n",
    "        chroma_indices = [i for i, name in enumerate(feature_names) if 'chroma' in name.lower()]\n",
    "        spectral_indices = [i for i, name in enumerate(feature_names) if any(spec in name.lower() for spec in ['spectral', 'centroid', 'bandwidth', 'contrast', 'rolloff'])]\n",
    "        other_indices = [i for i in range(len(feature_names)) \n",
    "                        if i not in mfcc_indices and i not in chroma_indices and i not in spectral_indices]\n",
    "        \n",
    "        # Her kategoriyi ayrÄ± tensor olarak hazÄ±rla\n",
    "        print(f\"Feature Type Separated yaklaÅŸÄ±mÄ±:\")\n",
    "        print(f\"- MFCC: {len(mfcc_indices)} Ã¶zellik\")\n",
    "        print(f\"- Chroma: {len(chroma_indices)} Ã¶zellik\")\n",
    "        print(f\"- Spectral: {len(spectral_indices)} Ã¶zellik\")\n",
    "        print(f\"- Others: {len(other_indices)} Ã¶zellik\")\n",
    "        \n",
    "        # En bÃ¼yÃ¼k kategori boyutunu bul\n",
    "        max_features = max(len(mfcc_indices), len(chroma_indices), len(spectral_indices), len(other_indices))\n",
    "        \n",
    "        # 4 kanal (feature type) x max_features boyutunda tensor\n",
    "        X_seq = np.zeros((n_samples, 4, max_features))\n",
    "        \n",
    "        # Her kategoriyi ayrÄ± kanala yerleÅŸtir\n",
    "        if len(mfcc_indices) > 0:\n",
    "            X_seq[:, 0, :len(mfcc_indices)] = X[:, mfcc_indices]\n",
    "        if len(chroma_indices) > 0:\n",
    "            X_seq[:, 1, :len(chroma_indices)] = X[:, chroma_indices]\n",
    "        if len(spectral_indices) > 0:\n",
    "            X_seq[:, 2, :len(spectral_indices)] = X[:, spectral_indices]\n",
    "        if len(other_indices) > 0:\n",
    "            X_seq[:, 3, :len(other_indices)] = X[:, other_indices]\n",
    "            \n",
    "        print(f\"Output shape: {X_seq.shape} (samples, feature_types, max_features_per_type)\")\n",
    "        \n",
    "    else:  # 'no_sequence'\n",
    "        # LSTM'e gerek yok, standart Dense layer yaklaÅŸÄ±mÄ±\n",
    "        # Bu, temporal yapÄ±yÄ± tamamen yok sayar ama performans aÃ§Ä±sÄ±ndan daha iyi olabilir\n",
    "        print(f\"âš ï¸ No Sequence yaklaÅŸÄ±mÄ±: Temporal yapÄ± tamamen yok sayÄ±lÄ±yor\")\n",
    "        X_seq = X.copy()  # Orijinal boyutlarda tut\n",
    "        print(f\"Output shape: {X_seq.shape} (samples, features) - Standard MLP iÃ§in uygun\")\n",
    "    \n",
    "    # PyTorch tensors\n",
    "    X_tensor = torch.FloatTensor(X_seq)\n",
    "    y_tensor = torch.LongTensor(y)\n",
    "    \n",
    "    return X_tensor, y_tensor, approach\n",
    "\n",
    "print(\"\\nğŸ”§ Alternatif Temporal YaklaÅŸÄ±mlar HazÄ±r!\")\n",
    "print(\"SeÃ§enekler:\")\n",
    "print(\"1. 'feature_type_separated' - Her Ã¶zellik tÃ¼rÃ¼ ayrÄ± kanal\")\n",
    "print(\"2. 'no_sequence' - Temporal yapÄ±yÄ± yok say, standard MLP\")\n",
    "print(\"\\nÃ–nerimiz: Ã–nce 'no_sequence' deneyin, daha sonra 'feature_type_separated' ile karÅŸÄ±laÅŸtÄ±rÄ±n!\")\n",
    "\n",
    "# Performans karÅŸÄ±laÅŸtÄ±rmasÄ± iÃ§in her yaklaÅŸÄ±mÄ± hazÄ±rla\n",
    "# Åimdilik mevcut 'grouped' yaklaÅŸÄ±mÄ±nÄ± kullanÄ±yoruz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719d6bcd",
   "metadata": {},
   "source": [
    "## LSTM Model TanÄ±mÄ± ve EÄŸitimi\n",
    "\n",
    "AÅŸaÄŸÄ±da mÃ¼zik tÃ¼rÃ¼ sÄ±nÄ±flandÄ±rmasÄ± iÃ§in bir LSTM (Long Short-Term Memory) aÄŸÄ± tanÄ±mlÄ±yoruz. LSTM'ler, mÃ¼zik gibi sÄ±ralÄ± verilerde baÅŸarÄ±lÄ± olan bir derin Ã¶ÄŸrenme mimarisidir.\n",
    "\n",
    "Model, LSTM katmanlarÄ±ndan sonra bir **attention mekanizmasÄ±** iÃ§erir. Bu mekanizma, modelin yapay olarak oluÅŸturulan zaman adÄ±mlarÄ±nÄ±n hangilerinin daha bilgilendirici olduÄŸunu Ã¶ÄŸrenmesine yardÄ±mcÄ± olur ve her zaman adÄ±mÄ±na farklÄ± aÄŸÄ±rlÄ±klar vererek daha etkili bir Ã¶zellik kombinasyonu oluÅŸturur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d73d18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention mekanizmasÄ± sÄ±nÄ±fÄ±\n",
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Attention aÄŸÄ±rlÄ±klarÄ±nÄ± hesaplamak iÃ§in linear katmanlar\n",
    "        self.attention_linear = nn.Linear(hidden_size, hidden_size)\n",
    "        self.context_vector = nn.Linear(hidden_size, 1, bias=False)\n",
    "        \n",
    "    def forward(self, lstm_outputs):\n",
    "        # lstm_outputs ÅŸekli: (batch_size, sequence_length, hidden_size)\n",
    "        \n",
    "        # Her zaman adÄ±mÄ± iÃ§in attention skorlarÄ± hesapla\n",
    "        attention_weights = self.attention_linear(lstm_outputs)  # (batch_size, seq_len, hidden_size)\n",
    "        attention_weights = torch.tanh(attention_weights)\n",
    "        attention_scores = self.context_vector(attention_weights)  # (batch_size, seq_len, 1)\n",
    "        attention_scores = attention_scores.squeeze(2)  # (batch_size, seq_len)\n",
    "        \n",
    "        # Softmax ile normalize et\n",
    "        attention_weights = torch.softmax(attention_scores, dim=1)  # (batch_size, seq_len)\n",
    "        \n",
    "        # Weighted sum hesapla\n",
    "        # attention_weights: (batch_size, seq_len) -> (batch_size, seq_len, 1)\n",
    "        attention_weights = attention_weights.unsqueeze(2)\n",
    "        \n",
    "        # Weighted combination of LSTM outputs\n",
    "        attended_output = torch.sum(lstm_outputs * attention_weights, dim=1)  # (batch_size, hidden_size)\n",
    "        \n",
    "        return attended_output, attention_weights.squeeze(2)\n",
    "\n",
    "# Bidirectional LSTM model sÄ±nÄ±fÄ±nÄ± tanÄ±mlama (Attention ile)\n",
    "class MusicGenreLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout=0.3, bidirectional=True):\n",
    "        super(MusicGenreLSTM, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional = bidirectional\n",
    "        \n",
    "        # Bidirectional LSTM katmanlarÄ±\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size, \n",
    "            hidden_size=hidden_size, \n",
    "            num_layers=num_layers, \n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=bidirectional\n",
    "        )\n",
    "        \n",
    "        # Bidirectional LSTM Ã§Ä±kÄ±ÅŸ boyutunu hesapla\n",
    "        lstm_output_size = hidden_size * 2 if bidirectional else hidden_size\n",
    "        \n",
    "        # Attention katmanÄ± (bidirectional output iÃ§in)\n",
    "        self.attention = AttentionLayer(lstm_output_size)\n",
    "        \n",
    "        # Batch normalization\n",
    "        self.batch_norm = nn.BatchNorm1d(lstm_output_size)\n",
    "        \n",
    "        # Dropout katmanÄ±\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Tam baÄŸlantÄ±lÄ± katmanlar\n",
    "        self.fc1 = nn.Linear(lstm_output_size, 128)  # Ä°lk tam baÄŸlantÄ±lÄ± katman\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "        \n",
    "        # Aktivasyon fonksiyonlarÄ±\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Bidirectional LSTM katmanÄ±ndan geÃ§irme\n",
    "        # x ÅŸekli: (batch_size, sequence_length, input_size)\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        \n",
    "        # lstm_out ÅŸekli: (batch_size, sequence_length, hidden_size * 2) for bidirectional\n",
    "        # veya (batch_size, sequence_length, hidden_size) for unidirectional\n",
    "        \n",
    "        # Attention mekanizmasÄ± uygula\n",
    "        attended_output, attention_weights = self.attention(lstm_out)\n",
    "        \n",
    "        # Batch normalization\n",
    "        batch_norm_out = self.batch_norm(attended_output)\n",
    "        \n",
    "        # Ä°lk tam baÄŸlantÄ±lÄ± katman\n",
    "        fc1_out = self.fc1(batch_norm_out)\n",
    "        fc1_out = self.relu(fc1_out)\n",
    "        fc1_out = self.dropout(fc1_out)\n",
    "        \n",
    "        # Ä°kinci tam baÄŸlantÄ±lÄ± katman (Ã§Ä±kÄ±ÅŸ katmanÄ±)\n",
    "        out = self.fc2(fc1_out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "# Learning Rate Warmup Scheduler\n",
    "class WarmupScheduler:\n",
    "    def __init__(self, optimizer, warmup_epochs, max_lr, min_lr=1e-7):\n",
    "        self.optimizer = optimizer\n",
    "        self.warmup_epochs = warmup_epochs\n",
    "        self.max_lr = max_lr\n",
    "        self.min_lr = min_lr\n",
    "        self.current_epoch = 0\n",
    "        \n",
    "    def step(self):\n",
    "        if self.current_epoch < self.warmup_epochs:\n",
    "            # Linear warmup\n",
    "            lr = self.min_lr + (self.max_lr - self.min_lr) * (self.current_epoch / self.warmup_epochs)\n",
    "            for param_group in self.optimizer.param_groups:\n",
    "                param_group['lr'] = lr\n",
    "        self.current_epoch += 1\n",
    "        \n",
    "    def get_lr(self):\n",
    "        return self.optimizer.param_groups[0]['lr']\n",
    "\n",
    "# Model parametreleri\n",
    "input_size = X_train_seq.shape[2]  # Bir zaman adÄ±mÄ±ndaki Ã¶zellik sayÄ±sÄ±\n",
    "hidden_size = 64  # LSTM gizli katman boyutu\n",
    "num_layers = 2  # LSTM katman sayÄ±sÄ±\n",
    "num_classes = len(le.classes_)  # SÄ±nÄ±f sayÄ±sÄ±\n",
    "dropout = 0.3\n",
    "bidirectional = True  # Bidirectional LSTM kullan\n",
    "\n",
    "# GPU kullanÄ±labilir mi kontrol et\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"KullanÄ±lan cihaz: {device}\")\n",
    "\n",
    "# Model oluÅŸturma\n",
    "model = MusicGenreLSTM(input_size, hidden_size, num_layers, num_classes, dropout, bidirectional).to(device)\n",
    "print(f\"\\nModel yapÄ±sÄ± (Bidirectional: {bidirectional}):\")\n",
    "print(model)\n",
    "\n",
    "# Model parametrelerinin sayÄ±sÄ±nÄ± hesapla\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"\\nToplam parametre sayÄ±sÄ±: {total_params:,}\")\n",
    "print(f\"EÄŸitilebilir parametre sayÄ±sÄ±: {trainable_params:,}\")\n",
    "\n",
    "# GeliÅŸmiÅŸ Learning Rate AyarlarÄ±\n",
    "initial_lr = 0.0001\n",
    "max_lr = 0.01  # Maksimum Ã¶ÄŸrenme oranÄ±\n",
    "min_lr = 1e-6  # Minimum Ã¶ÄŸrenme oranÄ±\n",
    "warmup_epochs = 3  # Ä°lk 3 epoch'ta yavaÅŸÃ§a arttÄ±r\n",
    "\n",
    "# KayÄ±p fonksiyonu ve optimize edici tanÄ±mlama\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=min_lr)  # DÃ¼ÅŸÃ¼k lr ile baÅŸla\n",
    "\n",
    "# Ã‡oklu scheduler sistemi\n",
    "# 1. Warmup scheduler - Ä°lk birkaÃ§ epoch'ta learning rate'i yavaÅŸÃ§a arttÄ±rÄ±r\n",
    "warmup_scheduler = WarmupScheduler(optimizer, warmup_epochs=warmup_epochs, max_lr=max_lr, min_lr=min_lr)\n",
    "\n",
    "# 2. ReduceLROnPlateau - Validation loss plateau'ya ulaÅŸtÄ±ÄŸÄ±nda LR'yi azaltÄ±r\n",
    "reduce_scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=3, min_lr=min_lr\n",
    ")\n",
    "\n",
    "# 3. CosineAnnealing - Cosine fonksiyonu ile LR'yi dÃ¼zenli olarak deÄŸiÅŸtirir\n",
    "cosine_scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer, T_max=20, eta_min=min_lr\n",
    ")\n",
    "\n",
    "print(f\"\\nLearning Rate AyarlarÄ±:\")\n",
    "print(f\"BaÅŸlangÄ±Ã§ LR: {min_lr}\")\n",
    "print(f\"Maksimum LR: {max_lr}\")\n",
    "print(f\"Minimum LR: {min_lr}\")\n",
    "print(f\"Warmup Epochs: {warmup_epochs}\")\n",
    "\n",
    "# GeliÅŸmiÅŸ eÄŸitim fonksiyonu\n",
    "def train_model_with_advanced_lr(model, train_loader, val_loader, criterion, optimizer, \n",
    "                                warmup_scheduler, reduce_scheduler, cosine_scheduler,\n",
    "                                num_epochs=50, early_stopping_patience=3, \n",
    "                                min_improvement_threshold=0.025, use_cosine_after_warmup=True):\n",
    "    \"\"\"\n",
    "    GeliÅŸmiÅŸ learning rate scheduling ile model eÄŸitimi\n",
    "    \"\"\"\n",
    "    # Ã–lÃ§Ã¼m deÄŸerlerini saklayacak listeler\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accs = []\n",
    "    val_accs = []\n",
    "    learning_rates = []  # LR geÃ§miÅŸini takip et\n",
    "    \n",
    "    # En iyi doÄŸrulama kaybÄ±nÄ± ve modeli saklama\n",
    "    best_val_loss = float('inf')\n",
    "    best_model = None\n",
    "    \n",
    "    # Erken durdurma iÃ§in sayaÃ§\n",
    "    early_stopping_counter = 0\n",
    "    \n",
    "    print(\"\\n=== GeliÅŸmiÅŸ Learning Rate Scheduling ile EÄŸitim BaÅŸlÄ±yor ===\")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Learning Rate Scheduling\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        if epoch < warmup_scheduler.warmup_epochs:\n",
    "            # Warmup phase\n",
    "            warmup_scheduler.step()\n",
    "            schedule_info = f\"Warmup Phase (Epoch {epoch+1}/{warmup_scheduler.warmup_epochs})\"\n",
    "        elif use_cosine_after_warmup and epoch >= warmup_scheduler.warmup_epochs:\n",
    "            # Cosine annealing after warmup\n",
    "            cosine_scheduler.step()\n",
    "            schedule_info = \"Cosine Annealing\"\n",
    "        else:\n",
    "            schedule_info = \"ReduceLROnPlateau (will be applied after validation)\"\n",
    "        \n",
    "        new_lr = optimizer.param_groups[0]['lr']\n",
    "        learning_rates.append(new_lr)\n",
    "        \n",
    "        # EÄŸitim modu\n",
    "        model.train()\n",
    "        \n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            # GradyanlarÄ± sÄ±fÄ±rla\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Ä°leri geÃ§iÅŸ\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Geri yayÄ±lÄ±m ve optimize etme\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping (exploding gradient problemini Ã¶nler)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            # Ä°statistikleri gÃ¼ncelle\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            train_total += labels.size(0)\n",
    "            train_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        # DoÄŸrulama modu\n",
    "        model.eval()\n",
    "        \n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                \n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        # Epoch sonuÃ§larÄ±nÄ± hesapla\n",
    "        epoch_train_loss = train_loss / len(train_loader.dataset)\n",
    "        epoch_val_loss = val_loss / len(val_loader.dataset)\n",
    "        epoch_train_acc = train_correct / train_total\n",
    "        epoch_val_acc = val_correct / val_total\n",
    "        \n",
    "        # ReduceLROnPlateau scheduler'Ä± warmup sonrasÄ± uygula\n",
    "        if epoch >= warmup_scheduler.warmup_epochs and not use_cosine_after_warmup:\n",
    "            reduce_scheduler.step(epoch_val_loss)\n",
    "        \n",
    "        # SonuÃ§larÄ± sakla\n",
    "        train_losses.append(epoch_train_loss)\n",
    "        val_losses.append(epoch_val_loss)\n",
    "        train_accs.append(epoch_train_acc)\n",
    "        val_accs.append(epoch_val_acc)\n",
    "        \n",
    "        # EÄŸitim durumunu yazdÄ±r\n",
    "        print(f'Epoch {epoch+1:2d}/{num_epochs} | '\n",
    "              f'Train Loss: {epoch_train_loss:.4f} | Train Acc: {epoch_train_acc:.4f} | '\n",
    "              f'Val Loss: {epoch_val_loss:.4f} | Val Acc: {epoch_val_acc:.4f} | '\n",
    "              f'LR: {new_lr:.2e} | {schedule_info}')\n",
    "        \n",
    "        # En iyi modeli sakla ve erken durdurma kontrolÃ¼\n",
    "        improvement = best_val_loss - epoch_val_loss\n",
    "        \n",
    "        if epoch_val_loss < best_val_loss:\n",
    "            if improvement > min_improvement_threshold:\n",
    "                early_stopping_counter = 0\n",
    "                improvement_msg = f\"âœ“ Significant improvement: {improvement:.6f}\"\n",
    "            else:\n",
    "                early_stopping_counter += 1\n",
    "                improvement_msg = f\"âš  Minor improvement: {improvement:.6f}\"\n",
    "            \n",
    "            best_val_loss = epoch_val_loss\n",
    "            best_model = model.state_dict()\n",
    "            print(f\"  {improvement_msg}\")\n",
    "        else:\n",
    "            early_stopping_counter += 1\n",
    "            print(f\"  âœ— No improvement (counter: {early_stopping_counter}/{early_stopping_patience})\")\n",
    "            \n",
    "        # Erken durdurma kontrolÃ¼\n",
    "        if early_stopping_counter >= early_stopping_patience:\n",
    "            print(f'\\nğŸ›‘ Erken durdurma: {early_stopping_patience} epoch boyunca yeterli iyileÅŸme yok.')\n",
    "            break\n",
    "    \n",
    "    # En iyi model aÄŸÄ±rlÄ±klarÄ±nÄ± yÃ¼kle\n",
    "    if best_model is not None:\n",
    "        model.load_state_dict(best_model)\n",
    "        print(f\"\\nâœ… En iyi model yÃ¼klendi (Val Loss: {best_val_loss:.6f})\")\n",
    "    \n",
    "    return model, train_losses, val_losses, train_accs, val_accs, learning_rates\n",
    "\n",
    "# Modeli geliÅŸmiÅŸ LR scheduling ile eÄŸit\n",
    "print(\"\\nğŸš€ Bidirectional LSTM model eÄŸitimi (GeliÅŸmiÅŸ LR Scheduling) baÅŸlÄ±yor...\")\n",
    "num_epochs = 50\n",
    "early_stopping_patience = 3  # Erken durdurma iÃ§in sabÄ±r sayÄ±sÄ±\n",
    "min_improvement_threshold = 0.025  # Ä°yileÅŸme eÅŸiÄŸi\n",
    "\n",
    "try:\n",
    "    model, train_losses, val_losses, train_accs, val_accs, learning_rates = train_model_with_advanced_lr(\n",
    "        model, train_loader, val_loader, criterion, optimizer, \n",
    "        warmup_scheduler, reduce_scheduler, cosine_scheduler,\n",
    "        num_epochs=num_epochs, \n",
    "        early_stopping_patience=early_stopping_patience,\n",
    "        min_improvement_threshold=min_improvement_threshold,\n",
    "        use_cosine_after_warmup=True  # Warmup sonrasÄ± cosine annealing kullan\n",
    "    )\n",
    "    print(\"\\nğŸ‰ Model eÄŸitimi baÅŸarÄ±yla tamamlandÄ±!\")\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nâ¹ï¸ EÄŸitim kullanÄ±cÄ± tarafÄ±ndan durduruldu.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f720d377",
   "metadata": {},
   "source": [
    "## Model DeÄŸerlendirmesi ve GÃ¶rselleÅŸtirme\n",
    "\n",
    "Bu bÃ¶lÃ¼mde eÄŸitilmiÅŸ modeli test veri seti Ã¼zerinde deÄŸerlendirip, sonuÃ§larÄ± gÃ¶rselleÅŸtireceÄŸiz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5361a1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EÄŸitim sonuÃ§larÄ±nÄ± gÃ¶rselleÅŸtirme\n",
    "def plot_training_history(train_losses, val_losses, train_accs, val_accs, learning_rates=None):\n",
    "    if learning_rates is not None:\n",
    "        plt.figure(figsize=(18, 6))\n",
    "        \n",
    "        # KayÄ±p grafiÄŸi\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.plot(train_losses, label='EÄŸitim', marker='o', alpha=0.7)\n",
    "        plt.plot(val_losses, label='DoÄŸrulama', marker='*', alpha=0.7)\n",
    "        plt.title('Model KaybÄ±')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('KayÄ±p (Cross-Entropy)')\n",
    "        plt.legend()\n",
    "        plt.grid(True, linestyle='--', alpha=0.6)\n",
    "        \n",
    "        # DoÄŸruluk grafiÄŸi\n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.plot(train_accs, label='EÄŸitim', marker='o', alpha=0.7)\n",
    "        plt.plot(val_accs, label='DoÄŸrulama', marker='*', alpha=0.7)\n",
    "        plt.title('Model DoÄŸruluÄŸu')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('DoÄŸruluk')\n",
    "        plt.legend()\n",
    "        plt.grid(True, linestyle='--', alpha=0.6)\n",
    "        \n",
    "        # Learning Rate grafiÄŸi\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.plot(learning_rates, marker='s', alpha=0.7, color='red')\n",
    "        plt.title('Learning Rate DeÄŸiÅŸimi')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Learning Rate')\n",
    "        plt.yscale('log')  # Log scale for better visualization\n",
    "        plt.grid(True, linestyle='--', alpha=0.6)\n",
    "    else:\n",
    "        plt.figure(figsize=(14, 5))\n",
    "        \n",
    "        # KayÄ±p grafiÄŸi\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(train_losses, label='EÄŸitim', marker='o')\n",
    "        plt.plot(val_losses, label='DoÄŸrulama', marker='*')\n",
    "        plt.title('Model KaybÄ±')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('KayÄ±p (Cross-Entropy)')\n",
    "        plt.legend()\n",
    "        plt.grid(True, linestyle='--', alpha=0.6)\n",
    "        \n",
    "        # DoÄŸruluk grafiÄŸi\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(train_accs, label='EÄŸitim', marker='o')\n",
    "        plt.plot(val_accs, label='DoÄŸrulama', marker='*')\n",
    "        plt.title('Model DoÄŸruluÄŸu')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('DoÄŸruluk')\n",
    "        plt.legend()\n",
    "        plt.grid(True, linestyle='--', alpha=0.6)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# EÄŸitim sonuÃ§larÄ±nÄ± gÃ¶rselleÅŸtir\n",
    "try:\n",
    "    if 'learning_rates' in locals():\n",
    "        plot_training_history(train_losses, val_losses, train_accs, val_accs, learning_rates)\n",
    "        \n",
    "        # DetaylÄ± learning rate analizi\n",
    "        print(f\"\\nğŸ“Š Learning Rate Ä°statistikleri:\")\n",
    "        print(f\"BaÅŸlangÄ±Ã§ LR: {learning_rates[0]:.2e}\")\n",
    "        print(f\"Maksimum LR: {max(learning_rates):.2e}\")\n",
    "        print(f\"Son LR: {learning_rates[-1]:.2e}\")\n",
    "        print(f\"Ortalama LR: {np.mean(learning_rates):.2e}\")\n",
    "    else:\n",
    "        plot_training_history(train_losses, val_losses, train_accs, val_accs)\n",
    "except NameError:\n",
    "    print(\"EÄŸitim geÃ§miÅŸi bulunamadÄ±. Ã–nce modeli eÄŸitin.\")\n",
    "\n",
    "# Test veri seti Ã¼zerinde deÄŸerlendirme\n",
    "def evaluate_model(model, test_loader, device):\n",
    "    model.eval()\n",
    "    \n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            \n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(predicted.cpu().numpy())\n",
    "    \n",
    "    # DoÄŸruluk hesapla\n",
    "    accuracy = np.mean(np.array(y_true) == np.array(y_pred))\n",
    "    \n",
    "    # SonuÃ§larÄ± yazdÄ±r\n",
    "    print(f\"\\nğŸ¯ Test DoÄŸruluÄŸu: {accuracy:.4f}\")\n",
    "    \n",
    "    # SÄ±nÄ±flandÄ±rma raporu\n",
    "    print(\"\\nğŸ“‹ SÄ±nÄ±flandÄ±rma Raporu:\")\n",
    "    print(classification_report(y_true, y_pred, target_names=le.classes_))\n",
    "    \n",
    "    # KarmaÅŸÄ±klÄ±k matrisi\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=le.classes_, yticklabels=le.classes_)\n",
    "    plt.title('KarmaÅŸÄ±klÄ±k Matrisi')\n",
    "    plt.xlabel('Tahmin Edilen Etiketler')\n",
    "    plt.ylabel('GerÃ§ek Etiketler')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return y_true, y_pred\n",
    "\n",
    "# Test veri seti Ã¼zerinde deÄŸerlendir\n",
    "try:\n",
    "    y_true, y_pred = evaluate_model(model, test_loader, device)\n",
    "except NameError:\n",
    "    print(\"Model bulunamadÄ±. Ã–nce modeli eÄŸitin.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pydebian (3.11.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
