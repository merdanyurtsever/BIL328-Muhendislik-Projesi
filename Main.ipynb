{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bfa6a60",
   "metadata": {},
   "source": [
    "# MÃ¼zik TÃ¼rÃ¼ SÄ±nÄ±flandÄ±rma Projesi\n",
    "\n",
    "Bu notebook, FMA (Free Music Archive) veri setini kullanarak mÃ¼zik tÃ¼rÃ¼ sÄ±nÄ±flandÄ±rma modeli geliÅŸtirmek iÃ§in veri hazÄ±rlama ve dengeleme iÅŸlemlerini iÃ§ermektedir.\n",
    "\n",
    "## Gerekli KÃ¼tÃ¼phanelerin Ä°Ã§e AktarÄ±lmasÄ±\n",
    "AÅŸaÄŸÄ±daki hÃ¼crede, projede kullanÄ±lacak temel Python kÃ¼tÃ¼phaneleri import edilmektedir:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ecc757",
   "metadata": {},
   "source": [
    "# Music Genre Classification with LSTM and BorderlineSMOTE\n",
    "\n",
    "This notebook demonstrates a robust workflow for music genre classification using deep learning (LSTM) and advanced data balancing (BorderlineSMOTE). Key steps:\n",
    "- Data loading and exploration\n",
    "- Feature selection and scaling\n",
    "- Class balancing with BorderlineSMOTE\n",
    "- LSTM model training and evaluation\n",
    "- Experiment logging, explainability, and creative enhancements\n",
    "\n",
    "---\n",
    "\n",
    "*All code and explanations are provided in both Turkish and English for clarity and accessibility.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672ed28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CONFIGURATION ===\n",
    "# All key parameters and paths in one place for reproducibility and easy tuning\n",
    "DATA_DIR = 'fma_metadata'\n",
    "TRACKS_PATH = f'{DATA_DIR}/tracks.csv'\n",
    "FEATURES_PATH = f'{DATA_DIR}/features.csv'\n",
    "\n",
    "# Feature Selection\n",
    "K_BEST = 225  # Number of features to select\n",
    "\n",
    "# Data Balancing\n",
    "MIN_SAMPLES_THRESHOLD = 20  # For BorderlineSMOTE (used in RandomOverSampler step)\n",
    "\n",
    "# LSTM Model & Training\n",
    "SEQUENCE_LENGTH = 10  # Sequence length for LSTM\n",
    "HIDDEN_SIZE = 64 # Hidden size of LSTM layers\n",
    "NUM_LAYERS = 2     # Number of LSTM layers\n",
    "DROPOUT_PROB = 0.3 # Dropout probability in LSTM\n",
    "BIDIRECTIONAL = True # Whether to use a bidirectional LSTM\n",
    "LEARNING_RATE = 0.001\n",
    "EPOCHS = 100 # Max epochs (EarlyStopping will be used)\n",
    "BATCH_SIZE = 512\n",
    "PATIENCE_EARLY_STOPPING = 5 # Patience for early stopping\n",
    "\n",
    "# General\n",
    "RANDOM_STATE = 42\n",
    "MODEL_SAVE_PATH = 'best_optimized_lstm.pth'\n",
    "SCALER_SAVE_PATH = 'scaler.pkl'\n",
    "ENCODER_SAVE_PATH = 'label_encoder.pkl'\n",
    "LOG_FILE = 'experiment_log.json'\n",
    "\n",
    "print('Configuration loaded.')\n",
    "print(f\"K_BEST: {K_BEST}, SEQUENCE_LENGTH: {SEQUENCE_LENGTH}, BATCH_SIZE: {BATCH_SIZE}\")\n",
    "print(f\"LSTM: HIDDEN_SIZE={HIDDEN_SIZE}, NUM_LAYERS={NUM_LAYERS}, DROPOUT_PROB={DROPOUT_PROB}, BIDIRECTIONAL={BIDIRECTIONAL}\")\n",
    "print(f\"TRAINING: LR={LEARNING_RATE}, EPOCHS={EPOCHS}, PATIENCE_EARLY_STOPPING={PATIENCE_EARLY_STOPPING}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c6cad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ENVIRONMENT & PACKAGE VERSIONS ===\n",
    "import sys\n",
    "import platform\n",
    "import sklearn\n",
    "import torch\n",
    "import imblearn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import seaborn\n",
    "\n",
    "print('Python version:', sys.version)\n",
    "print('Platform:', platform.platform())\n",
    "print('scikit-learn:', sklearn.__version__)\n",
    "print('PyTorch:', torch.__version__)\n",
    "print('imbalanced-learn:', imblearn.__version__)\n",
    "print('pandas:', pd.__version__)\n",
    "print('numpy:', np.__version__)\n",
    "print('matplotlib:', matplotlib.__version__)\n",
    "print('seaborn:', seaborn.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44273a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === DEVICE SELECTION ===\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "if torch.cuda.is_available():\n",
    "    print('CUDA device name:', torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd9f343",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from imblearn.over_sampling import RandomOverSampler, BorderlineSMOTE\n",
    "from collections import Counter\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set(style='whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98ccac1",
   "metadata": {},
   "source": [
    "# === Helper Functions ===\n",
    "# This section consolidates various utility functions used throughout the notebook\n",
    "# for plotting, seeding, saving/loading artifacts, logging, and custom metrics.\n",
    "# These functions depend on global variables defined in the Configuration cell (e.g., paths)\n",
    "# and the Device Selection cell (e.g., `device`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function imports\n",
    "import joblib\n",
    "import json\n",
    "from datetime import datetime\n",
    "import random\n",
    "from sklearn.metrics import f1_score, roc_auc_score # Added for advanced_metrics\n",
    "\n",
    "# --- Plotting ---\n",
    "def plot_class_distribution(y_numeric_labels, class_name_array, title):\n",
    "    \"\"\"Plots the distribution of classes.\n",
    "\n",
    "    Args:\n",
    "        y_numeric_labels: A pandas Series or numpy array of numeric class labels.\n",
    "        class_name_array: An array or list of class names, where the index corresponds to the numeric label.\n",
    "        title: The title for the plot.\n",
    "    \"\"\"\n",
    "    names_map = {i: class_name_array[i] for i in range(len(class_name_array))}\n",
    "    mapped_names_counts = pd.Series(y_numeric_labels).map(names_map).value_counts()\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    ax = sns.barplot(x=mapped_names_counts.index, y=mapped_names_counts.values, hue=mapped_names_counts.index, palette='viridis', legend=False)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('SÄ±nÄ±f') # Class (Turkish)\n",
    "    ax.set_ylabel('SayÄ±')  # Count (Turkish)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# --- PyTorch Model Training Utilities ---\n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "    def __init__(self, patience=PATIENCE_EARLY_STOPPING, verbose=False, delta=0, path=MODEL_SAVE_PATH):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        score = -val_loss\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model to {self.path} ...')\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_loss_min = val_loss\n",
    "\n",
    "# --- Reproducibility ---\n",
    "def set_seed(seed_value=RANDOM_STATE):\n",
    "    \"\"\"Sets the seed for reproducibility.\"\"\"\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed_value)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    print(f\"Global seed set to {seed_value}\")\n",
    "\n",
    "# --- Artifact Saving/Loading ---\n",
    "def save_scaler_and_encoder(scaler_obj, encoder_obj, scaler_path_param=SCALER_SAVE_PATH, encoder_path_param=ENCODER_SAVE_PATH):\n",
    "    \"\"\"Saves scaler and label encoder objects.\"\"\"\n",
    "    joblib.dump(scaler_obj, scaler_path_param)\n",
    "    joblib.dump(encoder_obj, encoder_path_param)\n",
    "    print(f'Scaler saved to {scaler_path_param}, encoder saved to {encoder_path_param}')\n",
    "\n",
    "def save_model(model_to_save, path_param=MODEL_SAVE_PATH):\n",
    "    \"\"\"Saves a PyTorch model state_dict.\"\"\"\n",
    "    torch.save(model_to_save.state_dict(), path_param)\n",
    "    print(f\"Model saved to {path_param}\")\n",
    "\n",
    "def load_model(model_instance, path_param=MODEL_SAVE_PATH):\n",
    "    \"\"\"Loads a PyTorch model state_dict into a model instance.\"\"\"\n",
    "    model_instance.load_state_dict(torch.load(path_param, map_location=device))\n",
    "    model_instance.eval() # Set to evaluation mode\n",
    "    print(f\"Model loaded from {path_param} to {device}\")\n",
    "    return model_instance\n",
    "\n",
    "# --- Experiment Logging ---\n",
    "def log_experiment(params_dict, metrics_dict, filename_param=LOG_FILE):\n",
    "    \"\"\"Logs experiment parameters and metrics to a JSON file.\"\"\"\n",
    "    log_entry = {\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"params\": params_dict,\n",
    "        \"metrics\": metrics_dict\n",
    "    }\n",
    "    try:\n",
    "        with open(filename_param, \"a\") as f: # Append mode\n",
    "            f.write(json.dumps(log_entry) + \"\\n\")\n",
    "        print(f\"Experiment logged to {filename_param}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Logging failed: {e}\")\n",
    "\n",
    "# --- Evaluation Metrics & Plotting ---\n",
    "def plot_confusion_matrix_and_report(y_true_labels, y_pred_labels, class_names_list, title='Confusion Matrix'):\n",
    "    \"\"\"Plots confusion matrix and prints classification report.\"\"\"\n",
    "    cm = confusion_matrix(y_true_labels, y_pred_labels)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names_list, yticklabels=class_names_list)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.ylabel('True Labels')\n",
    "    plt.show()\n",
    "    print('\\nClassification Report:')\n",
    "    print(classification_report(y_true_labels, y_pred_labels, labels=np.arange(len(class_names_list)), target_names=class_names_list, zero_division=0))\n",
    "\n",
    "def advanced_metrics(y_true_labels, y_pred_labels, y_probabilities, num_classes_for_roc):\n",
    "    \"\"\"Calculates and prints weighted F1-score and ROC-AUC.\"\"\"\n",
    "    f1 = f1_score(y_true_labels, y_pred_labels, average='weighted', zero_division=0)\n",
    "    print(f'Weighted F1-score: {f1:.4f}')\n",
    "    try:\n",
    "        if y_probabilities is not None and y_probabilities.ndim == 2 and y_probabilities.shape[0] == len(y_true_labels):\n",
    "            unique_labels_in_true = np.unique(y_true_labels)\n",
    "            if len(unique_labels_in_true) > 1:\n",
    "                roc_auc = roc_auc_score(y_true_labels, y_probabilities, multi_class='ovr', average='weighted', labels=np.arange(num_classes_for_roc))\n",
    "                print(f'ROC-AUC (OvR, Weighted): {roc_auc:.4f}')\n",
    "            else:\n",
    "                print('ROC-AUC not calculated: Only one class present in y_true.')\n",
    "        else:\n",
    "            print('ROC-AUC not calculated: y_probabilities are not in the correct format or not provided.')\n",
    "    except ValueError as e:\n",
    "        print(f'ROC-AUC calculation error: {e}')\n",
    "    except Exception as e:\n",
    "        print(f'An unexpected error occurred during ROC-AUC calculation: {e}')\n",
    "\n",
    "print(\"Helper functions defined and ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968d3831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set global random seed for reproducibility using value from config\n",
    "set_seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471baca3",
   "metadata": {},
   "source": [
    "## Veri YÃ¼kleme ve Ã–n Ä°ÅŸleme\n",
    "\n",
    "Bu bÃ¶lÃ¼mdeki fonksiyon:\n",
    "- FMA metadata dosyalarÄ±nÄ± yÃ¼kler\n",
    "- Gerekli sÃ¼tunlarÄ± seÃ§er\n",
    "- Eksik verileri temizler\n",
    "- Etiketleri kodlar\n",
    "- Veriyi sayÄ±sal formata dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6868595d",
   "metadata": {},
   "source": [
    "# 3. VERÄ° SETÄ°\n",
    "\n",
    "## 3.1. TanÄ±m ve Temin\n",
    "\n",
    "Bu Ã§alÄ±ÅŸmada mÃ¼zik tÃ¼rÃ¼ sÄ±nÄ±flandÄ±rma amacÄ±yla **FMA (Free Music Archive) Small** veri seti kullanÄ±lmÄ±ÅŸtÄ±r. FMA, telif hakkÄ± iÃ§ermeyen binlerce mÃ¼zik dosyasÄ±nÄ± barÄ±ndÄ±ran bir aÃ§Ä±k veri kÃ¼mesidir ve farklÄ± boyutlarda (small, medium, large, full) sunulmaktadÄ±r. Bu projede kullanÄ±lan \"FMA Small\" versiyonu, toplam **8,000 adet ses parÃ§asÄ±** iÃ§ermekte ve her biri **30 saniyelik** Ã¶rneklerden oluÅŸmaktadÄ±r. Veri seti, M. Deffayet ve arkadaÅŸlarÄ± tarafÄ±ndan derlenen bir Ã§alÄ±ÅŸmanÄ±n parÃ§asÄ±dÄ±r [8].\n",
    "\n",
    "### Veri Seti Ã–zellikleri:\n",
    "- **Toplam Dosya SayÄ±sÄ±**: 8,000 adet mÃ¼zik parÃ§asÄ±\n",
    "- **Dosya SÃ¼resi**: Her bir parÃ§a 30 saniye\n",
    "- **Ã–znitelik SayÄ±sÄ±**: 518 Ã¶nceden Ã§Ä±karÄ±lmÄ±ÅŸ Ã¶znitelik\n",
    "- **Format**: CSV dosyalarÄ± (tracks.csv, features.csv)\n",
    "- **Lisans**: Creative Commons lisansÄ± ile Ã¶zgÃ¼rce kullanÄ±labilir\n",
    "\n",
    "## 3.2. Ã–znitelik YapÄ±sÄ±\n",
    "\n",
    "FMA Small veri seti iÃ§inde her bir mÃ¼zik parÃ§asÄ±na ait **Ã¶nceden Ã§Ä±karÄ±lmÄ±ÅŸ 518 Ã¶znitelik (feature)** bulunmaktadÄ±r. Bu Ã¶znitelikler Python ortamÄ±nda `features.csv` dosyasÄ± aracÄ±lÄ±ÄŸÄ±yla saÄŸlanmakta ve sesin zaman-frekans alanÄ±ndaki temsilini iÃ§ermektedir.\n",
    "\n",
    "Ã–znitelikler, iÃ§eriklerine gÃ¶re aÅŸaÄŸÄ±daki ana gruplara ayrÄ±lmaktadÄ±r:\n",
    "\n",
    "### 3.2.1. Chroma Ã–znitelikleri (`chroma_`)\n",
    "- **AmaÃ§**: Tonalite ve akor bilgilerini temsil eder\n",
    "- **Ä°Ã§erik**: MÃ¼zikal armoniyi yansÄ±tÄ±r\n",
    "- **KullanÄ±m**: MÃ¼zik tÃ¼rleri arasÄ±ndaki tonal farklÄ±lÄ±klarÄ± yakalamak iÃ§in kritik\n",
    "\n",
    "### 3.2.2. MFCC Ã–znitelikleri (`mfcc_`)\n",
    "- **Tam AdÄ±**: Mel-Frequency Cepstral Coefficients\n",
    "- **AmaÃ§**: Sesin mel frekans alanÄ±ndaki cepstral bileÅŸenleri\n",
    "- **Ã–zellik**: Ses tanÄ±ma ve sÄ±nÄ±flandÄ±rmada yaygÄ±n olarak kullanÄ±lÄ±r\n",
    "- **Avantaj**: Ä°nsan kulaÄŸÄ±nÄ±n frekans algÄ±sÄ±nÄ± taklit eder\n",
    "\n",
    "### 3.2.3. Spektral Ã–znitelikler (`spectral_`)\n",
    "- **Ä°Ã§erik**: Spektral yoÄŸunluk, enerji ve frekans daÄŸÄ±lÄ±mlarÄ±\n",
    "- **Bilgi**: Sesin frekans alanÄ±ndaki karakteristik Ã¶zellikleri\n",
    "- **KullanÄ±m**: MÃ¼zik tÃ¼rlerinin frekans imzalarÄ±nÄ± ayÄ±rt etmek iÃ§in\n",
    "\n",
    "### 3.2.4. Tonnetz Ã–znitelikleri (`tonnetz_`)\n",
    "- **AmaÃ§**: Tonal merkezler ve armonik iliÅŸkiler\n",
    "- **Ä°Ã§erik**: MÃ¼zikal yapÄ± ve harmoni bilgileri\n",
    "- **Ã–zellik**: Soyut mÃ¼zikal kavramlarÄ± sayÄ±sal veriye dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r\n",
    "\n",
    "## 3.3. Veri Setinin AvantajlarÄ±\n",
    "\n",
    "Bu Ã¶zniteliklerin geniÅŸliÄŸi sayesinde, model sadece sesin fiziksel frekans Ã¶zelliklerini deÄŸil; aynÄ± zamanda **mÃ¼zikal yapÄ± ve armoni** gibi soyut bilgileri de Ã¶ÄŸrenebilmektedir. Bu kapsamlÄ± Ã¶znitelik seti, mÃ¼zik tÃ¼rÃ¼ sÄ±nÄ±flandÄ±rma gÃ¶revinde yÃ¼ksek performans elde etmemizi saÄŸlamaktadÄ±r.\n",
    "\n",
    "### Teknik Avantajlar:\n",
    "- **Ã‡ok boyutlu temsil**: 518 farklÄ± Ã¶znitelik ile zengin veri temsili\n",
    "- **Ã–nceden iÅŸlenmiÅŸ**: Manuel Ã¶znitelik Ã§Ä±karÄ±mÄ± gerektirmez\n",
    "- **StandartlaÅŸtÄ±rÄ±lmÄ±ÅŸ**: TÃ¼m Ã¶rnekler aynÄ± format ve sÃ¼rede\n",
    "- **Dengeli kategorizasyon**: FarklÄ± mÃ¼zik tÃ¼rlerini kapsayan geniÅŸ etiket seti"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7f0c60",
   "metadata": {},
   "source": [
    "## 3.4. Veri Seti KeÅŸfi ve Ä°nceleme\n",
    "\n",
    "AÅŸaÄŸÄ±daki kod bloklarÄ± ile FMA veri setinin yapÄ±sÄ±nÄ±, Ã¶znitelik daÄŸÄ±lÄ±mÄ±nÄ± ve sÄ±nÄ±f bilgilerini praktik olarak inceleyelim:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8653fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FMA veri setinin dosya yapÄ±sÄ±nÄ± kontrol et\n",
    "import os\n",
    "\n",
    "print(\"=== FMA Veri Seti Dosya YapÄ±sÄ± ===\")\n",
    "print(f\"Metadata klasÃ¶rÃ¼ mevcut: {os.path.exists('fma_metadata')}\")\n",
    "print(f\"Audio klasÃ¶rÃ¼ mevcut: {os.path.exists('fma_small')}\")\n",
    "\n",
    "if os.path.exists('fma_metadata'):\n",
    "    metadata_files = os.listdir('fma_metadata')\n",
    "    print(f\"\\nMetadata dosyalarÄ± ({len(metadata_files)} adet):\")\n",
    "    for file in sorted(metadata_files):\n",
    "        file_path = os.path.join('fma_metadata', file)\n",
    "        if os.path.isfile(file_path):\n",
    "            file_size = os.path.getsize(file_path) / (1024*1024)  # MB cinsinden\n",
    "            print(f\"  - {file}: {file_size:.2f} MB\")\n",
    "\n",
    "if os.path.exists('fma_small'):\n",
    "    # Ses dosyalarÄ±nÄ±n sayÄ±sÄ±nÄ± hesapla\n",
    "    audio_count = 0\n",
    "    for root, dirs, files in os.walk('fma_small'):\n",
    "        audio_count += len([f for f in files if f.endswith('.mp3')])\n",
    "    print(f\"\\nToplam ses dosyasÄ± sayÄ±sÄ±: {audio_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cea8309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features.csv dosyasÄ±nÄ±n yapÄ±sÄ±nÄ± incele\n",
    "if os.path.exists('fma_metadata/features.csv'):\n",
    "    print(\"=== Features.csv Dosya YapÄ±sÄ± ===\")\n",
    "    \n",
    "    # Ä°lk birkaÃ§ satÄ±rÄ± oku (Ã§ok seviyeli baÅŸlÄ±k yapÄ±sÄ±)\n",
    "    features_sample = pd.read_csv('fma_metadata/features.csv', nrows=5)\n",
    "    print(f\"Features dosyasÄ± boyutu: {features_sample.shape}\")\n",
    "    print(f\"SÃ¼tun sayÄ±sÄ±: {len(features_sample.columns)}\")\n",
    "    \n",
    "    # Ã‡ok seviyeli baÅŸlÄ±k yapÄ±sÄ±nÄ± doÄŸru ÅŸekilde oku\n",
    "    features_full = pd.read_csv('fma_metadata/features.csv', index_col=0, header=[0,1])\n",
    "    \n",
    "    print(f\"\\nToplam Ã¶rneklem sayÄ±sÄ±: {len(features_full)}\")\n",
    "    print(f\"Toplam Ã¶znitelik sayÄ±sÄ±: {len(features_full.columns)}\")\n",
    "    \n",
    "    # Ã–znitelik gruplarÄ±nÄ± analiz et\n",
    "    feature_groups = {}\n",
    "    for col in features_full.columns:\n",
    "        if len(col) >= 2:\n",
    "            group = col[0]  # Ä°lk seviye (ana grup)\n",
    "            if group not in feature_groups:\n",
    "                feature_groups[group] = 0\n",
    "            feature_groups[group] += 1\n",
    "    \n",
    "    print(\"\\n=== Ã–znitelik GruplarÄ± ===\")\n",
    "    for group, count in sorted(feature_groups.items()):\n",
    "        print(f\"{group}: {count} Ã¶znitelik\")\n",
    "        \n",
    "    # Ä°lk birkaÃ§ Ã¶znitelik Ã¶rneÄŸi\n",
    "    print(\"\\n=== Ã–rnek Ã–znitelikler (Ä°lk 10) ===\")\n",
    "    for i, col in enumerate(features_full.columns[:10]):\n",
    "        print(f\"{i+1}. {col}\")\n",
    "else:\n",
    "    print(\"Features.csv dosyasÄ± bulunamadÄ±!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5d6961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tracks.csv dosyasÄ±nÄ± ve tÃ¼r bilgilerini incele\n",
    "if os.path.exists('fma_metadata/tracks.csv'):\n",
    "    print(\"=== Tracks.csv ve TÃ¼r Bilgileri ===\")\n",
    "    \n",
    "    # Tracks dosyasÄ±nÄ± oku\n",
    "    tracks = pd.read_csv('fma_metadata/tracks.csv', index_col=0, header=[0,1])\n",
    "    print(f\"Tracks dosyasÄ± boyutu: {tracks.shape}\")\n",
    "    \n",
    "    # TÃ¼r bilgilerini analiz et\n",
    "    if ('track', 'genre_top') in tracks.columns:\n",
    "        genres = tracks[('track', 'genre_top')].dropna()\n",
    "        print(f\"\\nTÃ¼r bilgisi olan parÃ§a sayÄ±sÄ±: {len(genres)}\")\n",
    "        \n",
    "        # TÃ¼r daÄŸÄ±lÄ±mÄ±\n",
    "        genre_counts = genres.value_counts()\n",
    "        print(f\"\\nToplam farklÄ± tÃ¼r sayÄ±sÄ±: {len(genre_counts)}\")\n",
    "        \n",
    "        print(\"\\n=== TÃ¼r DaÄŸÄ±lÄ±mÄ± (Ä°lk 10) ===\")\n",
    "        for genre, count in genre_counts.head(10).items():\n",
    "            percentage = (count / len(genres)) * 100\n",
    "            print(f\"{genre}: {count} parÃ§a ({percentage:.1f}%)\")\n",
    "            \n",
    "        # En az ve en Ã§ok Ã¶rnekli tÃ¼rler\n",
    "        print(f\"\\nEn Ã§ok Ã¶rnekli tÃ¼r: {genre_counts.index[0]} ({genre_counts.iloc[0]} parÃ§a)\")\n",
    "        print(f\"En az Ã¶rnekli tÃ¼r: {genre_counts.index[-1]} ({genre_counts.iloc[-1]} parÃ§a)\")\n",
    "        \n",
    "    else:\n",
    "        print(\"TÃ¼r bilgisi sÃ¼tunu bulunamadÄ±!\")\n",
    "else:\n",
    "    print(\"Tracks.csv dosyasÄ± bulunamadÄ±!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b18b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ã–znitelik gruplarÄ±nÄ±n detaylÄ± analizi\n",
    "if os.path.exists('fma_metadata/features.csv'):\n",
    "    print(\"=== Ã–znitelik GruplarÄ± DetaylÄ± Analiz ===\")\n",
    "    \n",
    "    features = pd.read_csv('fma_metadata/features.csv', index_col=0, header=[0,1])\n",
    "    \n",
    "    # Her grup iÃ§in Ã¶znitelik sayÄ±sÄ±nÄ± ve Ã¶rnek isimleri gÃ¶ster\n",
    "    detailed_groups = {}\n",
    "    for col in features.columns:\n",
    "        if len(col) >= 2:\n",
    "            group = col[0]\n",
    "            subfeature = col[1] if len(col) > 1 else 'unknown'\n",
    "            \n",
    "            if group not in detailed_groups:\n",
    "                detailed_groups[group] = []\n",
    "            detailed_groups[group].append(subfeature)\n",
    "    \n",
    "    print(\"\\n=== Her Gruptaki Ã–znitelik TÃ¼rleri ===\")\n",
    "    for group, subfeatures in sorted(detailed_groups.items()):\n",
    "        print(f\"\\n{group.upper()} Grubu ({len(subfeatures)} Ã¶znitelik):\")\n",
    "        \n",
    "        # Benzersiz alt Ã¶znitelik tÃ¼rlerini gÃ¶ster\n",
    "        unique_subfeatures = list(set(subfeatures))\n",
    "        for subf in sorted(unique_subfeatures)[:5]:  # Ä°lk 5 tanesi\n",
    "            count = subfeatures.count(subf)\n",
    "            print(f\"  - {subf}: {count} adet\")\n",
    "        \n",
    "        if len(unique_subfeatures) > 5:\n",
    "            print(f\"  ... ve {len(unique_subfeatures) - 5} tÃ¼r daha\")\n",
    "    \n",
    "    print(\"\\n=== Veri Seti Ã–zeti ===\")\n",
    "    print(f\"âœ“ Toplam mÃ¼zik parÃ§asÄ±: {len(features):,}\")\n",
    "    print(f\"âœ“ Toplam Ã¶znitelik sayÄ±sÄ±: {len(features.columns):,}\")\n",
    "    print(f\"âœ“ FarklÄ± Ã¶znitelik grubu: {len(detailed_groups)}\")\n",
    "    print(f\"âœ“ Veri boyutu: {features.memory_usage(deep=True).sum() / (1024**2):.1f} MB\")\n",
    "    \n",
    "    # Eksik veri kontrolÃ¼\n",
    "    missing_data = features.isnull().sum().sum()\n",
    "    print(f\"âœ“ Eksik veri: {missing_data:,} hÃ¼cre ({(missing_data/(len(features)*len(features.columns)))*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89ec12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ã–znitelik gruplarÄ±nÄ±n gÃ¶rsel analizi\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "if os.path.exists('fma_metadata/features.csv') and os.path.exists('fma_metadata/tracks.csv'):\n",
    "    \n",
    "    # Features ve tracks verilerini yÃ¼kle\n",
    "    features = pd.read_csv('fma_metadata/features.csv', index_col=0, header=[0,1])\n",
    "    tracks = pd.read_csv('fma_metadata/tracks.csv', index_col=0, header=[0,1])\n",
    "    \n",
    "    # Ã–znitelik gruplarÄ±nÄ±n daÄŸÄ±lÄ±mÄ±nÄ± gÃ¶rselleÅŸtir\n",
    "    feature_groups = {}\n",
    "    for col in features.columns:\n",
    "        if len(col) >= 2:\n",
    "            group = col[0]\n",
    "            if group not in feature_groups:\n",
    "                feature_groups[group] = 0\n",
    "            feature_groups[group] += 1\n",
    "    \n",
    "    # Grafik 1: Ã–znitelik gruplarÄ±nÄ±n daÄŸÄ±lÄ±mÄ±\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    groups = list(feature_groups.keys())\n",
    "    counts = list(feature_groups.values())\n",
    "    colors = plt.cm.Set3(np.linspace(0, 1, len(groups)))\n",
    "    \n",
    "    bars = plt.bar(groups, counts, color=colors)\n",
    "    plt.title('Ã–znitelik GruplarÄ±nÄ±n DaÄŸÄ±lÄ±mÄ±', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Ã–znitelik Grubu')\n",
    "    plt.ylabel('Ã–znitelik SayÄ±sÄ±')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # Bar Ã¼zerinde deÄŸerleri gÃ¶ster\n",
    "    for bar, count in zip(bars, counts):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, \n",
    "                str(count), ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # Grafik 2: TÃ¼r daÄŸÄ±lÄ±mÄ± (eÄŸer mevcut ise)\n",
    "    plt.subplot(1, 2, 2)\n",
    "    if ('track', 'genre_top') in tracks.columns:\n",
    "        genres = tracks[('track', 'genre_top')].dropna()\n",
    "        top_genres = genres.value_counts().head(8)\n",
    "        \n",
    "        colors_genre = plt.cm.Paired(np.linspace(0, 1, len(top_genres)))\n",
    "        bars = plt.bar(range(len(top_genres)), top_genres.values, color=colors_genre)\n",
    "        plt.title('En YaygÄ±n 8 MÃ¼zik TÃ¼rÃ¼', fontsize=14, fontweight='bold')\n",
    "        plt.xlabel('MÃ¼zik TÃ¼rÃ¼')\n",
    "        plt.ylabel('ParÃ§a SayÄ±sÄ±')\n",
    "        plt.xticks(range(len(top_genres)), top_genres.index, rotation=45, ha='right')\n",
    "        \n",
    "        # Bar Ã¼zerinde deÄŸerleri gÃ¶ster\n",
    "        for bar, count in zip(bars, top_genres.values):\n",
    "            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 10, \n",
    "                    str(count), ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nðŸ“Š Veri seti gÃ¶rselleÅŸtirme tamamlandÄ±!\")\n",
    "else:\n",
    "    print(\"âŒ Gerekli dosyalar bulunamadÄ± - gÃ¶rselleÅŸtirme yapÄ±lamÄ±yor.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f453d21",
   "metadata": {},
   "source": [
    "# 4. Ã–ZNÄ°TELÄ°K SEÃ‡Ä°MÄ° (FEATURE SELECTION)\n",
    "\n",
    "## 4.1. Ã–znitelik SeÃ§iminin GerekliliÄŸi\n",
    "\n",
    "Makine Ã¶ÄŸrenmesi projelerinde yÃ¼ksek boyutlu veri setleriyle Ã§alÄ±ÅŸmak, hem **hesaplama yÃ¼kÃ¼nÃ¼ artÄ±rmakta** hem de modelin Ã¶ÄŸrenme sÃ¼recinde **aÅŸÄ±rÄ± uyum (overfitting)** riskini beraberinde getirmektedir. Bu nedenle, bu projede **Ã¶znitelik seÃ§imi (feature selection)** iÅŸlemi uygulanarak sadece en bilgilendirici Ã¶znitelikler modele dahil edilmiÅŸtir.\n",
    "\n",
    "### YÃ¼ksek Boyutluluk Problemleri:\n",
    "- **Hesaplama KarmaÅŸÄ±klÄ±ÄŸÄ±**: 518 Ã¶znitelik iÅŸlem sÃ¼resini Ã¶nemli Ã¶lÃ§Ã¼de artÄ±rÄ±r\n",
    "- **AÅŸÄ±rÄ± Uyum Riski**: Gereksiz Ã¶znitelikler modelin genelleme kabiliyetini azaltÄ±r\n",
    "- **GÃ¼rÃ¼ltÃ¼ Etkisi**: Ä°lgisiz Ã¶znitelikler sÄ±nÄ±flandÄ±rma performansÄ±nÄ± olumsuz etkiler\n",
    "- **Bellek KullanÄ±mÄ±**: YÃ¼ksek boyutlu veriler daha fazla bellek gerektirir\n",
    "\n",
    "## 4.2. SelectKBest YÃ¶ntemi\n",
    "\n",
    "FMA Small veri seti, her bir mÃ¼zik parÃ§asÄ± iÃ§in toplam **518 Ã¶znitelik** iÃ§ermektedir. Ancak bu Ã¶zniteliklerin tamamÄ±nÄ±n sÄ±nÄ±flandÄ±rma aÃ§Ä±sÄ±ndan eÅŸit Ã¶neme sahip olmadÄ±ÄŸÄ± bilinmektedir. Bu nedenle, **SelectKBest yÃ¶ntemi** kullanÄ±larak en etkili **225 Ã¶znitelik** seÃ§ilmiÅŸtir.\n",
    "\n",
    "### YÃ¶ntem DetaylarÄ±:\n",
    "- **Algoritma**: SelectKBest (Scikit-learn)\n",
    "- **Skorlama Fonksiyonu**: f_classif (ANOVA F-test)\n",
    "- **SeÃ§ilen Ã–znitelik SayÄ±sÄ±**: 225 (orijinal 518'den)\n",
    "- **Boyut Azaltma OranÄ±**: %56.6 azaltma\n",
    "\n",
    "### f_classif Skorlama Fonksiyonu:\n",
    "Bu iÅŸlem sÄ±rasÄ±nda **f_classif skorlama fonksiyonu** kullanÄ±lmÄ±ÅŸ ve her Ã¶zniteliÄŸin sÄ±nÄ±f ayrÄ±mÄ±na katkÄ±sÄ± istatistiksel olarak Ã¶lÃ§Ã¼lmÃ¼ÅŸtÃ¼r. f_classif, her Ã¶znitelik iÃ§in ANOVA F-test deÄŸeri hesaplayarak:\n",
    "- SÄ±nÄ±flar arasÄ± varyansÄ± sÄ±nÄ±f iÃ§i varyansa oranlar\n",
    "- YÃ¼ksek F-skoru = Daha iyi sÄ±nÄ±f ayrÄ±m gÃ¼cÃ¼\n",
    "- DÃ¼ÅŸÃ¼k p-deÄŸeri = Ä°statistiksel olarak anlamlÄ± Ã¶znitelik\n",
    "\n",
    "## 4.3. Beklenen Faydalar\n",
    "\n",
    "Bu sÃ¼reÃ§ sonucunda oluÅŸturulan eÄŸitim verisi:\n",
    "- âœ… **Hesaplama aÃ§Ä±sÄ±ndan daha verimli** hale getirilmiÅŸ\n",
    "- âœ… **Bilgi yoÄŸunluÄŸu artÄ±rÄ±lmÄ±ÅŸ** bir yapÄ± kazanmÄ±ÅŸ\n",
    "- âœ… **AÅŸÄ±rÄ± uyum riskini azaltmÄ±ÅŸ**\n",
    "- âœ… **Model genelleme kabiliyetini artÄ±rmÄ±ÅŸ**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156bee0f",
   "metadata": {},
   "source": [
    "## 4.4. Ã–znitelik SeÃ§imi UygulamasÄ±\n",
    "\n",
    "AÅŸaÄŸÄ±daki kod bloklarÄ± ile Ã¶znitelik seÃ§imi iÅŸlemini gerÃ§ekleÅŸtirip, sonuÃ§larÄ± analiz edelim:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d5d385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ã–znitelik seÃ§imi iÃ§in gerekli kÃ¼tÃ¼phaneler\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Verinin yÃ¼klenmesi ve Ã¶n iÅŸlenmesi\n",
    "def load_and_preprocess_data():\n",
    "    \"\"\"\n",
    "    FMA veri setini yÃ¼kler ve Ã¶znitelik seÃ§imi iÃ§in hazÄ±rlar.\n",
    "    Uses TRACKS_PATH and FEATURES_PATH from the global configuration.\n",
    "    \"\"\"\n",
    "    print(\"=== Veri YÃ¼kleme ve Ã–n Ä°ÅŸleme (Global Config Paths) ===\")\n",
    "    \n",
    "    # Dosya varlÄ±ÄŸÄ± kontrolÃ¼ (using global config)\n",
    "    if not os.path.exists(TRACKS_PATH) or not os.path.exists(FEATURES_PATH):\n",
    "        raise FileNotFoundError(f\"Gerekli veri dosyalarÄ± bulunamadÄ±! Paths: {TRACKS_PATH}, {FEATURES_PATH}\")\n",
    "    \n",
    "    # Tracks ve features dosyalarÄ±nÄ± yÃ¼kle\n",
    "    tracks = pd.read_csv(TRACKS_PATH, index_col=0, header=[0,1])\n",
    "    features_df = pd.read_csv(FEATURES_PATH, index_col=0, header=[0,1]) # Renamed to avoid conflict\n",
    "    \n",
    "    # Statistics sÃ¼tunlarÄ±nÄ± kaldÄ±r ve sayÄ±sal formata dÃ¶nÃ¼ÅŸtÃ¼r\n",
    "    # Ensure features_df is used here\n",
    "    features_df = features_df.loc[:, features_df.columns.get_level_values(0) != 'statistics']\n",
    "    features_df = features_df.astype(np.float32)\n",
    "    \n",
    "    # Ä°ndeksleri string formatÄ±na dÃ¶nÃ¼ÅŸtÃ¼r\n",
    "    features_df.index = features_df.index.astype(str)\n",
    "    tracks.index = tracks.index.astype(str)\n",
    "    \n",
    "    # TÃ¼r bilgilerini al\n",
    "    genre_series = tracks[('track', 'genre_top')].dropna()\n",
    "    \n",
    "    # Ortak indeksleri bul\n",
    "    common_index = features_df.index.intersection(genre_series.index)\n",
    "    \n",
    "    # Veriyi filtrele\n",
    "    X = features_df.loc[common_index]\n",
    "    y_labels = genre_series.loc[common_index]\n",
    "    \n",
    "    # Eksik ve sonsuz deÄŸerleri temizle\n",
    "    X = X.fillna(0).replace([np.inf, -np.inf], 0).astype(np.float32)\n",
    "    \n",
    "    # Etiketleri sayÄ±sallaÅŸtÄ±r\n",
    "    label_encoder = LabelEncoder()\n",
    "    y = label_encoder.fit_transform(y_labels)\n",
    "    \n",
    "    print(f\"âœ… Orijinal veri boyutu (X): {X.shape}\")\n",
    "    print(f\"âœ… Toplam sÄ±nÄ±f sayÄ±sÄ±: {len(label_encoder.classes_)}\")\n",
    "    print(f\"âœ… SÄ±nÄ±flar: {', '.join(label_encoder.classes_)}\")\n",
    "    \n",
    "    return X, y, label_encoder, features_df.columns # Return original X, y, encoder, and original feature columns\n",
    "\n",
    "# Veriyi yÃ¼kle\n",
    "X_original, y_original_labels, label_encoder_global, original_feature_columns = load_and_preprocess_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94798f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ã–znitelik seÃ§imi uygulama\n",
    "print(\"=== Ã–znitelik SeÃ§imi (SelectKBest) ===\")\n",
    "\n",
    "# SelectKBest ile en iyi K_BEST Ã¶zniteliÄŸi seÃ§ (K_BEST from global config)\n",
    "selector = SelectKBest(score_func=f_classif, k=K_BEST)\n",
    "\n",
    "# Ã–znitelik seÃ§imini uygula (using X_original and y_original_labels from fc93bb40)\n",
    "X_selected = selector.fit_transform(X_original, y_original_labels) # Use y_original_labels (numeric) for fitting\n",
    "\n",
    "# SeÃ§ilen Ã¶zniteliklerin indekslerini al\n",
    "selected_features_mask = selector.get_support()\n",
    "# Use original_feature_columns from fc93bb40\n",
    "selected_feature_names = original_feature_columns[selected_features_mask] \n",
    "\n",
    "# F-skorlarÄ±nÄ± al\n",
    "f_scores = selector.scores_\n",
    "selected_f_scores = f_scores[selected_features_mask]\n",
    "\n",
    "print(f\"\\nðŸ“Š Ã–znitelik SeÃ§imi SonuÃ§larÄ±:\")\n",
    "print(f\"   â€¢ Orijinal Ã¶znitelik sayÄ±sÄ±: {X_original.shape[1]}\")\n",
    "print(f\"   â€¢ SeÃ§ilen Ã¶znitelik sayÄ±sÄ±: {X_selected.shape[1]} (K_BEST={K_BEST})\")\n",
    "print(f\"   â€¢ Boyut azaltma oranÄ±: {((X_original.shape[1] - X_selected.shape[1]) / X_original.shape[1] * 100):.1f}%\")\n",
    "print(f\"   â€¢ Veri boyutu deÄŸiÅŸimi: {X_original.shape} â†’ {X_selected.shape}\")\n",
    "\n",
    "# En yÃ¼ksek F-Skorlu Ã¶znitelikleri gÃ¶ster\n",
    "print(f\"\\nðŸ† En YÃ¼ksek F-Skorlu 10 Ã–znitelik:\")\n",
    "# Sort selected_f_scores and get top indices from that sorted list\n",
    "sorted_indices_selected = np.argsort(selected_f_scores)[::-1] # Sort descending\n",
    "top_indices_selected = sorted_indices_selected[:10]\n",
    "\n",
    "for i, idx in enumerate(top_indices_selected):\n",
    "    feature_name = selected_feature_names[idx]\n",
    "    score = selected_f_scores[idx]\n",
    "    print(f\"   {i+1:2d}. {feature_name}: {score:.2f}\")\n",
    "\n",
    "# X_selected and selected_feature_names are now available for downstream use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf43ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SeÃ§ilen Ã¶zniteliklerin grup analizi\n",
    "print(\"\\n=== SeÃ§ilen Ã–zniteliklerin Grup Analizi ===\")\n",
    "\n",
    "# Orijinal ve seÃ§ilen Ã¶zniteliklerin grup daÄŸÄ±lÄ±mÄ±nÄ± karÅŸÄ±laÅŸtÄ±r\n",
    "original_groups = {}\n",
    "selected_groups = {}\n",
    "\n",
    "# Orijinal Ã¶znitelik gruplarÄ±nÄ± say (using original_feature_columns from fc93bb40)\n",
    "for col in original_feature_columns:\n",
    "    if len(col) >= 2:\n",
    "        group = col[0]\n",
    "        if group not in original_groups:\n",
    "            original_groups[group] = 0\n",
    "        original_groups[group] += 1\n",
    "\n",
    "# SeÃ§ilen Ã¶znitelik gruplarÄ±nÄ± say (using selected_feature_names from 980a711c)\n",
    "for col in selected_feature_names:\n",
    "    if len(col) >= 2:\n",
    "        group = col[0]\n",
    "        if group not in selected_groups:\n",
    "            selected_groups[group] = 0\n",
    "        selected_groups[group] += 1\n",
    "\n",
    "# KarÅŸÄ±laÅŸtÄ±rma tablosu oluÅŸtur\n",
    "comparison_data = []\n",
    "for group in original_groups.keys():\n",
    "    original_count = original_groups[group]\n",
    "    selected_count = selected_groups.get(group, 0)\n",
    "    selection_rate = (selected_count / original_count) * 100 if original_count > 0 else 0\n",
    "    \n",
    "    comparison_data.append({\n",
    "        'Ã–znitelik Grubu': group,\n",
    "        'Orijinal SayÄ±': original_count,\n",
    "        'SeÃ§ilen SayÄ±': selected_count,\n",
    "        'SeÃ§im OranÄ± (%)': f\"{selection_rate:.1f}%\"\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(\"\\nðŸ“‹ Ã–znitelik GruplarÄ± KarÅŸÄ±laÅŸtÄ±rmasÄ±:\")\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Hangi gruplarÄ±n daha Ã§ok seÃ§ildiÄŸini analiz et\n",
    "print(\"\\nðŸŽ¯ Grup SeÃ§im Analizi:\")\n",
    "for _, row in comparison_df.iterrows():\n",
    "    group = row['Ã–znitelik Grubu']\n",
    "    rate = float(row['SeÃ§im OranÄ± (%)'].replace('%', ''))\n",
    "    if rate >= 50:\n",
    "        print(f\"   âœ… {group}: YÃ¼ksek seÃ§im oranÄ± ({rate:.1f}%) - Bu grup sÄ±nÄ±flandÄ±rma iÃ§in Ã¶nemli\")\n",
    "    elif rate >= 30:\n",
    "        print(f\"   ðŸ”¶ {group}: Orta seÃ§im oranÄ± ({rate:.1f}%) - KÄ±smen bilgilendirici\")\n",
    "    else:\n",
    "        print(f\"   âŒ {group}: DÃ¼ÅŸÃ¼k seÃ§im oranÄ± ({rate:.1f}%) - SÄ±nÄ±flandÄ±rma iÃ§in az Ã¶nemli\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd0e902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ã–znitelik seÃ§imi sonuÃ§larÄ±nÄ±n gÃ¶rselleÅŸtirilmesi\n",
    "print(\"\\n=== Ã–znitelik SeÃ§imi GÃ¶rselleÅŸtirme ===\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('Ã–znitelik SeÃ§imi Analiz SonuÃ§larÄ±', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. F-skorlarÄ±nÄ±n daÄŸÄ±lÄ±mÄ±\n",
    "ax1 = axes[0, 0]\n",
    "ax1.hist(f_scores, bins=50, alpha=0.7, color='lightblue', edgecolor='black')\n",
    "# Use K_BEST from config for the threshold line\n",
    "ax1.axvline(np.sort(f_scores)[-K_BEST], color='red', linestyle='--', linewidth=2, label=f'SeÃ§im EÅŸiÄŸi (Top {K_BEST})')\n",
    "ax1.set_title('F-SkorlarÄ±nÄ±n DaÄŸÄ±lÄ±mÄ±')\n",
    "ax1.set_xlabel('F-Skoru')\n",
    "ax1.set_ylabel('Frekans')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Ã–znitelik gruplarÄ±nÄ±n karÅŸÄ±laÅŸtÄ±rmasÄ±\n",
    "ax2 = axes[0, 1]\n",
    "groups = list(original_groups.keys())\n",
    "x_pos = np.arange(len(groups))\n",
    "original_counts = [original_groups[g] for g in groups]\n",
    "selected_counts = [selected_groups.get(g, 0) for g in groups]\n",
    "\n",
    "width = 0.35\n",
    "ax2.bar(x_pos - width/2, original_counts, width, label='Orijinal', alpha=0.7, color='lightcoral')\n",
    "ax2.bar(x_pos + width/2, selected_counts, width, label='SeÃ§ilen', alpha=0.7, color='lightgreen')\n",
    "ax2.set_title('Ã–znitelik GruplarÄ±: Orijinal vs SeÃ§ilen')\n",
    "ax2.set_xlabel('Ã–znitelik Grubu')\n",
    "ax2.set_ylabel('Ã–znitelik SayÄ±sÄ±')\n",
    "ax2.set_xticks(x_pos)\n",
    "ax2.set_xticklabels(groups, rotation=45, ha='right')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. En yÃ¼ksek F-skorlu Ã¶znitelikler\n",
    "ax3 = axes[1, 0]\n",
    "top_n = 15\n",
    "# Use selected_f_scores and selected_feature_names from cell 980a711c\n",
    "sorted_indices_selected_viz = np.argsort(selected_f_scores)[-top_n:] # Get indices of top N from selected_f_scores\n",
    "top_scores_viz = selected_f_scores[sorted_indices_selected_viz]\n",
    "top_names_viz = [str(selected_feature_names[i]) for i in sorted_indices_selected_viz]\n",
    "\n",
    "# Ã–znitelik isimlerini kÄ±salt\n",
    "top_names_short = [name.replace('(', '').replace(')', '').replace(',', '_')[:20] + '...' if len(str(name)) > 23 else str(name) for name in top_names_viz]\n",
    "\n",
    "y_pos = np.arange(len(top_names_short))\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(top_scores_viz)))\n",
    "ax3.barh(y_pos, top_scores_viz, color=colors)\n",
    "ax3.set_title(f'En YÃ¼ksek F-Skorlu {top_n} Ã–znitelik')\n",
    "ax3.set_xlabel('F-Skoru')\n",
    "ax3.set_yticks(y_pos)\n",
    "ax3.set_yticklabels(top_names_short, fontsize=8)\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Boyut azaltma etkisi\n",
    "ax4 = axes[1, 1]\n",
    "labels = ['Orijinal Boyut', 'SeÃ§ilen Boyut']\n",
    "sizes = [X_original.shape[1], X_selected.shape[1]]\n",
    "colors = ['#ff9999', '#66b3ff']\n",
    "explode = (0, 0.1)  # SeÃ§ilen kÄ±smÄ± vurgula\n",
    "\n",
    "ax4.pie(sizes, explode=explode, labels=labels, colors=colors, autopct='%1.1f%%',\n",
    "        shadow=True, startangle=90)\n",
    "ax4.set_title('Boyut Azaltma Etkisi')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ… Ã–znitelik seÃ§imi gÃ¶rselleÅŸtirmesi tamamlandÄ±!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2046ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ã–znitelik seÃ§iminin etkilerinin detaylÄ± analizi\n",
    "print(\"\\n=== Ã–znitelik SeÃ§imi Etki Analizi ===\")\n",
    "\n",
    "# Bellek kullanÄ±mÄ± karÅŸÄ±laÅŸtÄ±rmasÄ±\n",
    "original_memory = X_original.memory_usage(deep=True).sum() / (1024**2)  # MB\n",
    "selected_memory = pd.DataFrame(X_selected).memory_usage(deep=True).sum() / (1024**2)  # MB\n",
    "memory_reduction = ((original_memory - selected_memory) / original_memory) * 100\n",
    "\n",
    "print(f\"\\nðŸ’¾ Bellek KullanÄ±mÄ± Analizi:\")\n",
    "print(f\"   â€¢ Orijinal veri bellek kullanÄ±mÄ±: {original_memory:.1f} MB\")\n",
    "print(f\"   â€¢ SeÃ§ilen veri bellek kullanÄ±mÄ±: {selected_memory:.1f} MB\")\n",
    "print(f\"   â€¢ Bellek tasarrufu: {memory_reduction:.1f}% ({original_memory-selected_memory:.1f} MB)\")\n",
    "\n",
    "# Hesaplama karmaÅŸÄ±klÄ±ÄŸÄ± tahmini\n",
    "original_complexity = X_original.shape[0] * X_original.shape[1]\n",
    "selected_complexity = X_selected.shape[0] * X_selected.shape[1]\n",
    "complexity_reduction = ((original_complexity - selected_complexity) / original_complexity) * 100\n",
    "\n",
    "print(f\"\\nâš¡ Hesaplama KarmaÅŸÄ±klÄ±ÄŸÄ±:\")\n",
    "print(f\"   â€¢ Orijinal iÅŸlem sayÄ±sÄ±: {original_complexity:,}\")\n",
    "print(f\"   â€¢ SeÃ§ilen iÅŸlem sayÄ±sÄ±: {selected_complexity:,}\")\n",
    "print(f\"   â€¢ Hesaplama azalmasÄ±: {complexity_reduction:.1f}%\")\n",
    "\n",
    "# En bilgilendirici Ã¶znitelik gruplarÄ±nÄ± belirle\n",
    "print(f\"\\nðŸŽ¯ En Bilgilendirici Ã–znitelik GruplarÄ±:\")\n",
    "group_importance = {}\n",
    "for i, col in enumerate(selected_feature_names):\n",
    "    if len(col) >= 2:\n",
    "        group = col[0]\n",
    "        score = selected_f_scores[i]\n",
    "        if group not in group_importance:\n",
    "            group_importance[group] = []\n",
    "        group_importance[group].append(score)\n",
    "\n",
    "# Her grup iÃ§in ortalama F-skoru hesapla\n",
    "group_avg_scores = {}\n",
    "for group, scores in group_importance.items():\n",
    "    group_avg_scores[group] = np.mean(scores)\n",
    "\n",
    "# GruplarÄ± Ã¶nem sÄ±rasÄ±na gÃ¶re sÄ±rala\n",
    "sorted_groups = sorted(group_avg_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "for i, (group, avg_score) in enumerate(sorted_groups):\n",
    "    count = len(group_importance[group])\n",
    "    selection_rate = (count / original_groups[group]) * 100\n",
    "    print(f\"   {i+1}. {group}: Ort. F-skoru={avg_score:.2f}, SeÃ§ilen={count}/{original_groups[group]} ({selection_rate:.1f}%)\")\n",
    "\n",
    "print(f\"\\nðŸ“ˆ Ã–zet:\")\n",
    "print(f\"   âœ… Veri boyutu {X_original.shape[1]} â†’ {X_selected.shape[1]} Ã¶zniteliÄŸe dÃ¼ÅŸÃ¼rÃ¼ldÃ¼\")\n",
    "print(f\"   âœ… {memory_reduction:.1f}% bellek tasarrufu saÄŸlandÄ±\")\n",
    "print(f\"   âœ… {complexity_reduction:.1f}% hesaplama karmaÅŸÄ±klÄ±ÄŸÄ± azaltÄ±ldÄ±\")\n",
    "print(f\"   âœ… En bilgilendirici grup: {sorted_groups[0][0]} (Ort. F-skoru: {sorted_groups[0][1]:.2f})\")\n",
    "print(f\"   âœ… Model aÅŸÄ±rÄ± uyum riski azaltÄ±ldÄ±\")\n",
    "print(f\"   âœ… EÄŸitim sÃ¼resi optimize edildi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2d68df",
   "metadata": {},
   "source": [
    "## 4.5. Sonraki AdÄ±mlar\n",
    "\n",
    "Ã–znitelik seÃ§imi iÅŸlemi tamamlandÄ±ktan sonra, veri seti ÅŸu ÅŸekilde hazÄ±r hale gelmiÅŸtir:\n",
    "\n",
    "### âœ… Tamamlanan Ä°ÅŸlemler:\n",
    "1. **Boyut Azaltma**: 518 â†’ 225 Ã¶znitelik\n",
    "2. **Etiket Kodlama**: MÃ¼zik tÃ¼rleri sayÄ±sallaÅŸtÄ±rÄ±ldÄ±\n",
    "3. **Veri Temizleme**: Eksik ve sonsuz deÄŸerler giderildi\n",
    "4. **Optimizasyon**: Bellek ve hesaplama verimliliÄŸi artÄ±rÄ±ldÄ±\n",
    "\n",
    "### ðŸ”„ SÄ±radaki Ä°ÅŸlemler:\n",
    "1. **Veri Dengeleme**: RandomOverSampler + BorderlineSMOTE\n",
    "2. **EÄŸitim-Test BÃ¶lme**: Stratified split uygulamasÄ±\n",
    "3. **Model EÄŸitimi**: LSTM aÄŸ yapÄ±sÄ±\n",
    "4. **Performans DeÄŸerlendirme**: Accuracy, precision, recall metrikleri\n",
    "\n",
    "Veri seti artÄ±k **sÄ±nÄ±f dengesizliÄŸi problemi Ã§Ã¶zÃ¼lmek Ã¼zere** bir sonraki aÅŸamaya hazÄ±rdÄ±r."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c705ac60",
   "metadata": {},
   "source": [
    "## BaÅŸlangÄ±Ã§ Veri Analizi\n",
    "\n",
    "# Veriyi yÃ¼kle ve Ã¶niÅŸle - This cell now relies on variables from cell fc93bb40\n",
    "# X_original, y_original_labels, label_encoder_global, original_feature_columns are already loaded\n",
    "\n",
    "print(\"Plotting initial class distribution using data from 'load_and_preprocess_data'...\")\n",
    "# y_original_labels is numeric here, label_encoder_global.classes_ provides the names\n",
    "plot_class_distribution(y_original_labels, label_encoder_global.classes_, 'BaÅŸlangÄ±Ã§ SÄ±nÄ±f DaÄŸÄ±lÄ±mÄ± (FiltrelenmemiÅŸ)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ef4617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veriyi bÃ¶l ve eÄŸitim daÄŸÄ±lÄ±mÄ±nÄ± gÃ¶ster\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_selected, y_original_labels, test_size=0.2, stratify=y_original_labels, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "plot_class_distribution(y_train, label_encoder_global.classes_, 'EÄŸitim Seti DaÄŸÄ±lÄ±mÄ±')\n",
    "print(f'EÄŸitim/test bÃ¶lÃ¼nmesi tamamlandÄ±: X_train {X_train.shape}, X_test {X_test.shape}')\n",
    "\n",
    "# DetaylÄ± daÄŸÄ±lÄ±mÄ± yazdÄ±r\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "print(\"\\nEÄŸitim Seti DaÄŸÄ±lÄ±mÄ± (ham sayÄ±lar):\")\n",
    "for i, (u, c) in enumerate(zip(unique, counts)):\n",
    "    print(f\"SÄ±nÄ±f {u} ({label_encoder_global.classes_[i]}): {c} Ã¶rnek\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48a9420",
   "metadata": {},
   "source": [
    "## Veri Dengeleme - AÅŸama 1\n",
    "\n",
    "Ä°lk aÅŸamada, Ã§ok az Ã¶rneÄŸe sahip sÄ±nÄ±flar iÃ§in RandomOverSampler kullanÄ±lÄ±yor. Bu aÅŸama, BorderlineSMOTE iÃ§in yeterli Ã¶rnek sayÄ±sÄ±na ulaÅŸmamÄ±zÄ± saÄŸlar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d82e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AdÄ±m 1: En az temsil edilen sÄ±nÄ±flar iÃ§in RandomOverSampler\n",
    "print('\\nAdÄ±m 1: AÅŸÄ±rÄ± az temsil edilen sÄ±nÄ±flar iÃ§in RandomOverSampler uygulanÄ±yor...')\n",
    "\n",
    "# Mevcut sÄ±nÄ±f daÄŸÄ±lÄ±mÄ±nÄ± kontrol et\n",
    "unique_train, counts_train = np.unique(y_train, return_counts=True)\n",
    "print(\"\\nEÄŸitim setindeki mevcut daÄŸÄ±lÄ±m:\")\n",
    "for i, (u, c) in enumerate(zip(unique_train, counts_train)):\n",
    "    class_name = label_encoder_global.classes_[u]\n",
    "    print(f\"SÄ±nÄ±f {u} ({class_name}): {c} Ã¶rnek\")\n",
    "\n",
    "# MIN_SAMPLES_THRESHOLD'dan az Ã¶rneÄŸe sahip sÄ±nÄ±flarÄ± belirle\n",
    "classes_to_oversample = {}\n",
    "for i, (u, c) in enumerate(zip(unique_train, counts_train)):\n",
    "    if c < MIN_SAMPLES_THRESHOLD:\n",
    "        classes_to_oversample[u] = MIN_SAMPLES_THRESHOLD\n",
    "        class_name = label_encoder_global.classes_[u]\n",
    "        print(f\"SÄ±nÄ±f {u} ({class_name}) Ã¶rneklem sayÄ±sÄ± {c} â†’ {MIN_SAMPLES_THRESHOLD} artÄ±rÄ±lacak\")\n",
    "\n",
    "# EÄŸer Ã¶rneklem artÄ±rÄ±lacak sÄ±nÄ±f varsa RandomOverSampler uygula\n",
    "if classes_to_oversample:\n",
    "    ros = RandomOverSampler(sampling_strategy=classes_to_oversample, random_state=RANDOM_STATE)\n",
    "    X_partial, y_partial = ros.fit_resample(X_train, y_train)\n",
    "    print(f\"\\nRandomOverSampler uygulandÄ±: {len(classes_to_oversample)} sÄ±nÄ±f iÃ§in Ã¶rneklem artÄ±rÄ±ldÄ±\")\n",
    "else:\n",
    "    X_partial, y_partial = X_train, y_train\n",
    "    print(f\"\\nTÃ¼m sÄ±nÄ±flar zaten {MIN_SAMPLES_THRESHOLD} eÅŸiÄŸinin Ã¼zerinde, RandomOverSampler uygulanmadÄ±\")\n",
    "\n",
    "# Ara sonuÃ§larÄ± gÃ¶ster\n",
    "unique_partial, counts_partial = np.unique(y_partial, return_counts=True)\n",
    "print(\"\\nRandomOverSampler sonrasÄ± daÄŸÄ±lÄ±m (ham sayÄ±lar):\")\n",
    "for i, (u, c) in enumerate(zip(unique_partial, counts_partial)):\n",
    "    # Use label_encoder_global from cell 20 for class names\n",
    "    print(f\"SÄ±nÄ±f {u} ({label_encoder_global.classes_[i]}): {c} Ã¶rnek\")\n",
    "\n",
    "# Use label_encoder_global from cell 20 for class names\n",
    "plot_class_distribution(y_partial, label_encoder_global.classes_, 'RandomOverSampler SonrasÄ±')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0017c8",
   "metadata": {},
   "source": [
    "## Veri Dengeleme - AÅŸama 2\n",
    "\n",
    "Ä°kinci aÅŸamada, daha sofistike bir yaklaÅŸÄ±m olan BorderlineSMOTE kullanÄ±larak kalan sÄ±nÄ±flar dengeleniyor. Bu yÃ¶ntem, sadece rastgele kopyalama yerine sentetik Ã¶rnekler oluÅŸturur.\n",
    "\n",
    "Not: Bu aÅŸama, veri setinin yapÄ±sÄ±na baÄŸlÄ± olarak baÅŸarÄ±sÄ±z olabilir. Bu durumda, ilk aÅŸamadaki sonuÃ§lar kullanÄ±lacaktÄ±r."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08db968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AdÄ±m 2: Kalan sÄ±nÄ±flar iÃ§in BorderlineSMOTE\n",
    "print('\\nAdÄ±m 2: Kalan sÄ±nÄ±flar iÃ§in BorderlineSMOTE uygulanÄ±yor...')\n",
    "borderline_smote = BorderlineSMOTE(random_state=RANDOM_STATE) # Use RANDOM_STATE from config\n",
    "\n",
    "try:\n",
    "    X_res, y_res = borderline_smote.fit_resample(X_partial, y_partial)\n",
    "    print(f'Kombine Ã¶rnekleme tamamlandÄ±: X_res {X_res.shape}, y_res {y_res.shape}')\n",
    "    \n",
    "    # Son daÄŸÄ±lÄ±mÄ± yazdÄ±r ve gÃ¶ster\n",
    "    unique_res, counts_res = np.unique(y_res, return_counts=True)\n",
    "    print(\"\\nSon DaÄŸÄ±lÄ±m (ham sayÄ±lar):\")\n",
    "    for i, (u, c) in enumerate(zip(unique_res, counts_res)):\n",
    "        # Use label_encoder_global from cell 20 for class names\n",
    "        print(f\"SÄ±nÄ±f {u} ({label_encoder_global.classes_[i]}): {c} Ã¶rnek\")\n",
    "    \n",
    "    # Use label_encoder_global from cell 20 for class names\n",
    "    plot_class_distribution(y_res, label_encoder_global.classes_, 'Son DengelenmiÅŸ DaÄŸÄ±lÄ±m')\n",
    "\n",
    "except Exception as e:\n",
    "    print(f'BorderlineSMOTE Ã¶rnekleme baÅŸarÄ±sÄ±z oldu: {e} - kÄ±smi Ã¶rneklenmiÅŸ veri kullanÄ±lÄ±yor')\n",
    "    X_res, y_res = X_partial, y_partial\n",
    "    # Use label_encoder_global from cell 20 for class names\n",
    "    plot_class_distribution(y_res, label_encoder_global.classes_, 'KÄ±smi Ã–rnekleme (BorderlineSMOTE baÅŸarÄ±sÄ±z)')\n",
    "\n",
    "print(\"\\nÄ°ÅŸlem hattÄ± tamamlandÄ±. Yeniden Ã¶rneklenmiÅŸ eÄŸitim verisi (X_res, y_res) ve test verisi (X_test, y_test) hazÄ±r.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36224907",
   "metadata": {},
   "source": [
    "## PyTorch LSTM MODEL EÄžÄ°TÄ°MÄ°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a149b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nPyTorch LSTM Model EÄŸitimi BaÅŸlÄ±yor...\")\n",
    "\n",
    "# Veri yÃ¼kleme, Ã¶niÅŸleme, bÃ¶lme ve dengeleme adÄ±mlarÄ±nÄ±n tamamlandÄ±ÄŸÄ± varsayÄ±lÄ±r.\n",
    "# Bu noktada aÅŸaÄŸÄ±daki deÄŸiÅŸkenlerin mevcut olmasÄ± beklenir:\n",
    "# X_res, y_res (DengelenmiÅŸ eÄŸitim verisi)\n",
    "# X_test, y_test (Test verisi)\n",
    "# label_encoder_global (LabelEncoder nesnesi)\n",
    "# X_val, y_val (DoÄŸrulama verisi - bir sonraki hÃ¼crede oluÅŸturulacak)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2538a571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DengelenmiÅŸ veri setinden doÄŸrulama seti ayÄ±r\n",
    "X_train_bal, X_val, y_train_bal, y_val = train_test_split(\n",
    "    X_res, y_res, test_size=0.1, stratify=y_res, random_state=RANDOM_STATE # Use RANDOM_STATE from config\n",
    ")\n",
    "\n",
    "# Veri Ã–lÃ§eklendirme (StandardScaler)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_bal)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test) # Ensure X_test from the initial split is used here\n",
    "\n",
    "print(\"Veri Ã¶lÃ§eklendirme tamamlandÄ±.\")\n",
    "print(f\"Ã–lÃ§eklenmiÅŸ eÄŸitim verisi boyutu: {X_train_scaled.shape}\")\n",
    "print(f\"Ã–lÃ§eklenmiÅŸ doÄŸrulama verisi boyutu: {X_val_scaled.shape}\")\n",
    "print(f\"Ã–lÃ§eklenmiÅŸ test verisi boyutu: {X_test_scaled.shape}\")\n",
    "\n",
    "# Save the scaler for later use (using global config path)\n",
    "save_scaler_and_encoder(scaler, label_encoder_global, SCALER_SAVE_PATH, ENCODER_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ad9976",
   "metadata": {},
   "source": [
    "## LSTM Modeli iÃ§in Veri HazÄ±rlÄ±ÄŸÄ±\n",
    "\n",
    "PyTorch LSTM modeli iÃ§in, veriyi uygun formata dÃ¶nÃ¼ÅŸtÃ¼rmemiz gerekir. LSTM modeller sÄ±ralÄ± veri bekler, bu nedenle Ã¶znitelik vektÃ¶rÃ¼nÃ¼ zamansal bir diziye dÃ¶nÃ¼ÅŸtÃ¼receÄŸiz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57400533",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch tensÃ¶rlerine dÃ¶nÃ¼ÅŸtÃ¼rme ve veri setlerini hazÄ±rlama\n",
    "def create_sequence_data(X, y, sequence_length=SEQUENCE_LENGTH): # Use SEQUENCE_LENGTH from config\n",
    "    \"\"\"\n",
    "    Ã–znitelik vektÃ¶rÃ¼nÃ¼ sÄ±ralÄ± verilere dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r.\n",
    "    FMA veri seti sÄ±ralÄ± yapÄ±da deÄŸil, bu nedenle yapay bir sÄ±ra oluÅŸturuyoruz.\n",
    "    \"\"\"\n",
    "    # Veri boyutlarÄ±nÄ± kontrol et\n",
    "    n_samples, n_features = X.shape\n",
    "    \n",
    "    # Veriyi yeniden ÅŸekillendirme\n",
    "    features_per_timestep = n_features // sequence_length\n",
    "    \n",
    "    if features_per_timestep == 0:\n",
    "        print(f\"Warning: features_per_timestep is 0. n_features={n_features}, sequence_length={sequence_length}. Adjusting sequence_length.\")\n",
    "        features_per_timestep = 1 # Ensure at least one feature per timestep\n",
    "        sequence_length = min(sequence_length, n_features) # Adjust sequence length if too large\n",
    "        print(f\"Adjusted: features_per_timestep={features_per_timestep}, sequence_length={sequence_length}\")\n",
    "\n",
    "    # Son timestep'e sÄ±ÄŸmayan Ã¶zellikleri ele alma (padding or truncation implicitly handled by slicing)\n",
    "    # X_padded = np.pad(X, ((0,0), (0, sequence_length * features_per_timestep - n_features)), 'constant') if n_features < sequence_length * features_per_timestep else X[:, :sequence_length * features_per_timestep]\n",
    "    # X_reshaped = X_padded.reshape(n_samples, sequence_length, features_per_timestep)\n",
    "    \n",
    "    # Yeniden ÅŸekillendirilmiÅŸ veri iÃ§in array oluÅŸturma\n",
    "    # Ensure the last dimension matches features_per_timestep\n",
    "    X_seq = np.zeros((n_samples, sequence_length, features_per_timestep))\n",
    "    \n",
    "    # Veriyi yeniden ÅŸekillendirme\n",
    "    for i in range(n_samples):\n",
    "        for t in range(sequence_length):\n",
    "            start_idx = t * features_per_timestep\n",
    "            # Ensure end_idx does not exceed n_features and slice correctly for X_seq's last dimension\n",
    "            end_idx = start_idx + features_per_timestep \n",
    "            current_features_slice = X[i, start_idx:end_idx]\n",
    "            X_seq[i, t, :len(current_features_slice)] = current_features_slice\n",
    "\n",
    "    # PyTorch tensÃ¶rlerine dÃ¶nÃ¼ÅŸtÃ¼rme\n",
    "    X_tensor = torch.FloatTensor(X_seq).to(device) # Move to device\n",
    "    y_tensor = torch.LongTensor(y).to(device)   # Move to device\n",
    "    \n",
    "    return X_tensor, y_tensor\n",
    "\n",
    "# SÄ±ralÄ± veri iÃ§in hiperparametre (SEQUENCE_LENGTH is from global config)\n",
    "\n",
    "# Ã–lÃ§eklenmiÅŸ verileri sÄ±ralÄ± forma dÃ¶nÃ¼ÅŸtÃ¼rme\n",
    "X_train_seq, y_train_tensor = create_sequence_data(X_train_scaled, y_train_bal)\n",
    "X_val_seq, y_val_tensor = create_sequence_data(X_val_scaled, y_val)\n",
    "X_test_seq, y_test_tensor = create_sequence_data(X_test_scaled, y_test)\n",
    "\n",
    "print(f\"EÄŸitim veri boyutu (Tensor): {X_train_seq.shape}, Etiketler: {y_train_tensor.shape}\")\n",
    "print(f\"DoÄŸrulama veri boyutu (Tensor): {X_val_seq.shape}, Etiketler: {y_val_tensor.shape}\")\n",
    "print(f\"Test veri boyutu (Tensor): {X_test_seq.shape}, Etiketler: {y_test_tensor.shape}\")\n",
    "\n",
    "# PyTorch DataLoader oluÅŸturma (BATCH_SIZE from global config)\n",
    "train_dataset = TensorDataset(X_train_seq, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_seq, y_val_tensor)\n",
    "test_dataset = TensorDataset(X_test_seq, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"DataLoaders created with batch size: {BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d3a574",
   "metadata": {},
   "source": [
    "## ðŸ“Š LSTM Model TanÄ±mÄ± ve EÄŸitimi\n",
    "\n",
    "AÅŸaÄŸÄ±da mÃ¼zik tÃ¼rÃ¼ sÄ±nÄ±flandÄ±rmasÄ± iÃ§in bir LSTM (Long Short-Term Memory) aÄŸÄ± tanÄ±mlÄ±yoruz. LSTM'ler, mÃ¼zik gibi sÄ±ralÄ± verilerde baÅŸarÄ±lÄ± olan bir derin Ã¶ÄŸrenme mimarisidir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72764d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === LSTM Model Definition ===\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers, dropout_prob, bidirectional=True):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, \n",
    "                              batch_first=True, dropout=dropout_prob, \n",
    "                              bidirectional=bidirectional)\n",
    "        \n",
    "        # Adjust linear layer input if bidirectional\n",
    "        self.fc_input_dim = hidden_dim * 2 if bidirectional else hidden_dim\n",
    "        self.fc = nn.Linear(self.fc_input_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_length, input_dim)\n",
    "        # h0 and c0 are initialized to zero by default if not provided\n",
    "        out, (hn, cn) = self.lstm(x)\n",
    "        \n",
    "        # out shape: (batch_size, seq_length, hidden_dim * num_directions)\n",
    "        # We are interested in the output of the last time step\n",
    "        if self.lstm.bidirectional:\n",
    "            # Concatenate the last hidden state of the forward pass (from hn)\n",
    "            # and the last hidden state of the backward pass (from hn)\n",
    "            # hn shape: (num_layers * num_directions, batch, hidden_dim)\n",
    "            # Forward direction: hn[-2,:,:] (last layer, forward)\n",
    "            # Backward direction: hn[-1,:,:] (last layer, backward)\n",
    "            out_forward = hn[-2,:,:] \n",
    "            out_backward = hn[-1,:,:]\n",
    "            out_concat = torch.cat((out_forward, out_backward), dim=1)\n",
    "        else:\n",
    "            # If not bidirectional, just take the last hidden state of the last layer\n",
    "            # hn shape: (num_layers, batch, hidden_dim)\n",
    "            out_concat = hn[-1,:,:] # (batch, hidden_dim)\n",
    "            \n",
    "        out_concat = self.dropout(out_concat)\n",
    "        out = self.fc(out_concat) # (batch, output_dim)\n",
    "        return out\n",
    "\n",
    "# Model Instantiation using global config\n",
    "input_dim = X_train_seq.shape[2]  # Number of features per timestep\n",
    "output_dim = len(label_encoder_global.classes_)\n",
    "\n",
    "model = LSTMClassifier(input_dim, HIDDEN_SIZE, output_dim, NUM_LAYERS, DROPOUT_PROB, BIDIRECTIONAL)\n",
    "model = model.to(device)\n",
    "\n",
    "# Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "print(f\"LSTM Model Instantiated on {device}:\")\n",
    "print(model)\n",
    "print(f\"Input Dim: {input_dim}, Hidden Dim: {HIDDEN_SIZE}, Output Dim: {output_dim}, Layers: {NUM_LAYERS}, Dropout: {DROPOUT_PROB}, Bidirectional: {BIDIRECTIONAL}\")\n",
    "print(f\"Optimizer: Adam, LR: {LEARNING_RATE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95425cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Model Training Loop ===\n",
    "print(\"\\nStarting LSTM Model Training...\")\n",
    "\n",
    "# Fix for NumPy 2.0 compatibility - replace np.Inf with np.inf in any imported modules\n",
    "import numpy as np\n",
    "\n",
    "# Define EarlyStopping class for NumPy 2.0 compatibility\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt'):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.inf\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        score = -val_loss\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}). Saving model...')\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_loss_min = val_loss\n",
    "\n",
    "# Define load_model helper function\n",
    "def load_model(model, path_param):\n",
    "    model.load_state_dict(torch.load(path_param))\n",
    "    return model\n",
    "\n",
    "# Initialize EarlyStopping with patience and model save path from config\n",
    "early_stopping = EarlyStopping(patience=PATIENCE_EARLY_STOPPING, verbose=True, path=MODEL_SAVE_PATH)\n",
    "\n",
    "training_history = {'loss': [], 'acc': [], 'val_loss': [], 'val_acc': []}\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "\n",
    "    for i, (sequences, labels) in enumerate(train_loader):\n",
    "        sequences, labels = sequences.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(sequences)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * sequences.size(0)\n",
    "        _, predicted_train = torch.max(outputs.data, 1)\n",
    "        total_train += labels.size(0)\n",
    "        correct_train += (predicted_train == labels).sum().item()\n",
    "\n",
    "    epoch_loss = running_loss / total_train\n",
    "    epoch_acc = correct_train / total_train\n",
    "    training_history['loss'].append(epoch_loss)\n",
    "    training_history['acc'].append(epoch_acc)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_running_loss = 0.0\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "    with torch.no_grad():\n",
    "        for sequences_val, labels_val in val_loader:\n",
    "            sequences_val, labels_val = sequences_val.to(device), labels_val.to(device)\n",
    "            outputs_val = model(sequences_val)\n",
    "            loss_val = criterion(outputs_val, labels_val)\n",
    "            val_running_loss += loss_val.item() * sequences_val.size(0)\n",
    "            _, predicted_val = torch.max(outputs_val.data, 1)\n",
    "            total_val += labels_val.size(0)\n",
    "            correct_val += (predicted_val == labels_val).sum().item()\n",
    "\n",
    "    epoch_val_loss = val_running_loss / total_val\n",
    "    epoch_val_acc = correct_val / total_val\n",
    "    training_history['val_loss'].append(epoch_val_loss)\n",
    "    training_history['val_acc'].append(epoch_val_acc)\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{EPOCHS} - Loss: {epoch_loss:.4f}, Acc: {epoch_acc:.4f}, Val Loss: {epoch_val_loss:.4f}, Val Acc: {epoch_val_acc:.4f}')\n",
    "\n",
    "    # Early stopping call (path is now handled within the EarlyStopping class)\n",
    "    early_stopping(epoch_val_loss, model)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping triggered.\")\n",
    "        break\n",
    "\n",
    "print(\"Training finished.\")\n",
    "# Load best model\n",
    "# Ensure model is an instance of LSTMClassifier before calling load_model\n",
    "if isinstance(model, LSTMClassifier):\n",
    "    model = load_model(model, path_param=MODEL_SAVE_PATH) # Use the specific load_model helper\n",
    "    print(f\"Best model re-loaded from {MODEL_SAVE_PATH} using helper function.\")\n",
    "else:\n",
    "    # Fallback to direct loading if model object type is unexpected, though it should be LSTMClassifier\n",
    "    model.load_state_dict(torch.load(MODEL_SAVE_PATH))\n",
    "    print(f\"Best model loaded from {MODEL_SAVE_PATH} directly.\")\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(training_history['loss'], label='Training Loss')\n",
    "plt.plot(training_history['val_loss'], label='Validation Loss')\n",
    "plt.title('Loss Over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(training_history['acc'], label='Training Accuracy')\n",
    "plt.plot(training_history['val_acc'], label='Validation Accuracy')\n",
    "plt.title('Accuracy Over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b3bc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Model Evaluation on Test Set ===\n",
    "print(\"\\nEvaluating model on the test set...\")\n",
    "model.eval()\n",
    "y_pred_list = []\n",
    "y_true_list = []\n",
    "y_prob_list = [] # For ROC-AUC\n",
    "\n",
    "with torch.no_grad():\n",
    "    for sequences_test, labels_test in test_loader:\n",
    "        sequences_test, labels_test = sequences_test.to(device), labels_test.to(device)\n",
    "        outputs_test = model(sequences_test)\n",
    "        \n",
    "        # Probabilities for ROC-AUC\n",
    "        probs = torch.softmax(outputs_test, dim=1)\n",
    "        y_prob_list.extend(probs.cpu().numpy())\n",
    "        \n",
    "        _, predicted_test = torch.max(outputs_test.data, 1)\n",
    "        y_pred_list.extend(predicted_test.cpu().numpy())\n",
    "        y_true_list.extend(labels_test.cpu().numpy())\n",
    "\n",
    "y_pred_np = np.array(y_pred_list)\n",
    "y_true_np = np.array(y_true_list)\n",
    "y_prob_np = np.array(y_prob_list)\n",
    "\n",
    "# Classification Report and Confusion Matrix\n",
    "print(\"\\nTest Set Evaluation:\")\n",
    "plot_confusion_matrix_and_report(y_true_np, y_pred_np, label_encoder_global.classes_, title='Test Set Confusion Matrix')\n",
    "\n",
    "# Advanced Metrics\n",
    "print(\"\\nAdvanced Test Set Metrics:\")\n",
    "# Pass the number of classes for num_classes_for_roc\n",
    "advanced_metrics(y_true_np, y_pred_np, y_prob_np, num_classes_for_roc=len(label_encoder_global.classes_))\n",
    "\n",
    "# Log Experiment\n",
    "experiment_params = {\n",
    "    \"K_BEST\": K_BEST,\n",
    "    \"SEQUENCE_LENGTH\": SEQUENCE_LENGTH,\n",
    "    \"HIDDEN_SIZE\": HIDDEN_SIZE,\n",
    "    \"NUM_LAYERS\": NUM_LAYERS,\n",
    "    \"DROPOUT_PROB\": DROPOUT_PROB,\n",
    "    \"BIDIRECTIONAL\": BIDIRECTIONAL,\n",
    "    \"LEARNING_RATE\": LEARNING_RATE,\n",
    "    \"EPOCHS_RUN\": len(training_history['loss']), # Actual epochs run\n",
    "    \"BATCH_SIZE\": BATCH_SIZE,\n",
    "    \"RANDOM_STATE\": RANDOM_STATE,\n",
    "    \"MIN_SAMPLES_THRESHOLD\": MIN_SAMPLES_THRESHOLD,\n",
    "    \"DATA_USED\": \"BorderlineSMOTE_balanced\"\n",
    "}\n",
    "\n",
    "# Collect metrics for logging\n",
    "# Ensure these are calculated correctly based on your evaluation\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "accuracy = accuracy_score(y_true_np, y_pred_np)\n",
    "test_f1_weighted = f1_score(y_true_np, y_pred_np, average='weighted')\n",
    "\n",
    "# Handle ROC-AUC for multi-class\n",
    "try:\n",
    "    test_roc_auc_ovr = roc_auc_score(y_true_np, y_prob_np, multi_class='ovr', average='weighted')\n",
    "except ValueError as e:\n",
    "    print(f\"Could not calculate ROC AUC: {e}\")\n",
    "    test_roc_auc_ovr = None # Or some placeholder like 0.0 or np.nan\n",
    "\n",
    "experiment_metrics = {\n",
    "    \"test_accuracy\": accuracy,\n",
    "    \"test_f1_weighted\": test_f1_weighted,\n",
    "    \"test_roc_auc_ovr\": test_roc_auc_ovr,\n",
    "    \"final_train_loss\": training_history['loss'][-1] if training_history['loss'] else None,\n",
    "    \"final_val_loss\": training_history['val_loss'][-1] if training_history['val_loss'] else None,\n",
    "    \"final_train_acc\": training_history['acc'][-1] if training_history['acc'] else None,\n",
    "    \"final_val_acc\": training_history['val_acc'][-1] if training_history['val_acc'] else None\n",
    "}\n",
    "\n",
    "log_experiment(experiment_params, experiment_metrics)\n",
    "\n",
    "print(\"\\nModel evaluation and logging complete.\")\n",
    "\n",
    "# Save scaler and encoder (already done after scaling, but good to ensure it's here if flow changes)\n",
    "# save_scaler_and_encoder(scaler, label_encoder_global, SCALER_SAVE_PATH, ENCODER_SAVE_PATH)\n",
    "# print(f\"Scaler and LabelEncoder saved to {SCALER_SAVE_PATH} and {ENCODER_SAVE_PATH} respectively.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f908ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SIMPLE DASHBOARD: Quick Experiment Summary ===\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def dashboard(history, metrics):\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    ax[0].plot(history['loss'], label='Loss')\n",
    "    ax[0].set_title('Training Loss')\n",
    "    ax[0].set_xlabel('Epoch')\n",
    "    ax[0].set_ylabel('Loss')\n",
    "    ax[0].legend()\n",
    "    ax[1].plot(history['acc'], label='Accuracy')\n",
    "    ax[1].set_title('Training Accuracy')\n",
    "    ax[1].set_xlabel('Epoch')\n",
    "    ax[1].set_ylabel('Accuracy')\n",
    "    ax[1].legend()\n",
    "    plt.suptitle('Experiment Dashboard')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    print('Final Metrics:', metrics)\n",
    "\n",
    "# Example usage:\n",
    "# dashboard(history, {'F1': 0.88, 'ROC-AUC': 0.91})\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pydebian (3.11.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
