{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bfa6a60",
   "metadata": {},
   "source": [
    "# Müzik Türü Sınıflandırma Projesi\n",
    "\n",
    "Bu notebook, FMA (Free Music Archive) veri setini kullanarak müzik türü sınıflandırma modeli geliştirmek için veri hazırlama ve dengeleme işlemlerini içermektedir.\n",
    "\n",
    "## Gerekli Kütüphanelerin İçe Aktarılması\n",
    "Aşağıdaki hücrede, projede kullanılacak temel Python kütüphaneleri import edilmektedir:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2a41ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from imblearn.over_sampling import RandomOverSampler, BorderlineSMOTE\n",
    "from collections import Counter\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set(style='whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4206449",
   "metadata": {},
   "source": [
    "## Yardımcı Fonksiyonlar\n",
    "\n",
    "### Sınıf Dağılımı Görselleştirme Fonksiyonu\n",
    "Aşağıdaki fonksiyon, veri setindeki sınıf dağılımlarını görselleştirmek için kullanılacaktır. Bu görselleştirme, veri dengesizliğini anlamamıza yardımcı olur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ae2a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_class_distribution(y, labels, title):\n",
    "    counts = pd.Series(y).value_counts().sort_index()\n",
    "    valid_indices = counts.index[counts.index < len(labels)]\n",
    "    counts = counts.loc[valid_indices]\n",
    "    names = labels[counts.index]\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    ax = sns.barplot(x=names, y=counts.values, hue=names, palette='viridis', legend=False)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('Sınıf')\n",
    "    ax.set_ylabel('Sayı')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471baca3",
   "metadata": {},
   "source": [
    "## Veri Yükleme ve Ön İşleme\n",
    "\n",
    "Bu bölümdeki fonksiyon:\n",
    "- FMA metadata dosyalarını yükler\n",
    "- Gerekli sütunları seçer\n",
    "- Eksik verileri temizler\n",
    "- Etiketleri kodlar\n",
    "- Veriyi sayısal formata dönüştürür"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e25b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    tracks_path = 'fma_metadata/tracks.csv'\n",
    "    features_path = 'fma_metadata/features.csv'\n",
    "\n",
    "    if not os.path.exists(tracks_path) or not os.path.exists(features_path):\n",
    "        raise FileNotFoundError(f\"Gerekli veri dosyaları bulunamadı. '{tracks_path}' ve '{features_path}' dosyalarının mevcut olduğundan emin olun.\")\n",
    "\n",
    "    tracks = pd.read_csv(tracks_path, index_col=0, header=[0,1])\n",
    "    \n",
    "    features = pd.read_csv(features_path, index_col=0, header=[0,1])  # Çok seviyeli başlıkla oku\n",
    "    features = features.loc[:, features.columns.get_level_values(0) != 'statistics']  # 'statistics' sütunlarını kaldır\n",
    "    features = features.astype(np.float32)  # Sayısal olmayan sütunları kaldırdıktan sonra float'a dönüştür\n",
    "\n",
    "    features.index = features.index.astype(str)\n",
    "    tracks.index = tracks.index.astype(str)\n",
    "\n",
    "    genre_series = tracks[('track', 'genre_top')].dropna()\n",
    "    common_index = features.index.intersection(genre_series.index)\n",
    "\n",
    "    X = features.loc[common_index]\n",
    "    y_labels = genre_series.loc[common_index]\n",
    "\n",
    "    X = X.fillna(0).replace([np.inf, -np.inf], 0).astype(np.float32)\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "    y = label_encoder.fit_transform(y_labels)\n",
    "\n",
    "    print('Veriler yüklendi ve önişlendi.')\n",
    "    return X, y, label_encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c705ac60",
   "metadata": {},
   "source": [
    "## Başlangıç Veri Analizi\n",
    "\n",
    "Verinin ilk yüklemesini yapıp, başlangıçtaki sınıf dağılımını inceleyelim. Bu analiz, veri dengesizliği problemini görselleştirmemize yardımcı olacak."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97f61d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veriyi yükle ve önişle\n",
    "X, y, le = load_data()\n",
    "\n",
    "# Başlangıç dağılımını göster\n",
    "plot_class_distribution(y, le.classes_, 'Başlangıç Sınıf Dağılımı')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b6b7b9",
   "metadata": {},
   "source": [
    "## Veri Bölme ve Eğitim Seti Analizi\n",
    "\n",
    "Veriyi eğitim ve test setlerine ayırıp, eğitim setindeki sınıf dağılımını inceliyoruz. Stratified split kullanarak orijinal dağılımı koruyoruz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ef4617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veriyi böl ve eğitim dağılımını göster\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "plot_class_distribution(y_train, le.classes_, 'Eğitim Seti Dağılımı')\n",
    "print(f'Eğitim/test bölünmesi tamamlandı: X_train {X_train.shape}, X_test {X_test.shape}')\n",
    "\n",
    "# Detaylı dağılımı yazdır\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "print(\"\\nEğitim Seti Dağılımı (ham sayılar):\")\n",
    "for i, (u, c) in enumerate(zip(unique, counts)):\n",
    "    print(f\"Sınıf {u} ({le.classes_[i]}): {c} örnek\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48a9420",
   "metadata": {},
   "source": [
    "## Veri Dengeleme - Aşama 1\n",
    "\n",
    "İlk aşamada, çok az örneğe sahip sınıflar için RandomOverSampler kullanılıyor. Bu aşama, BorderlineSMOTE için yeterli örnek sayısına ulaşmamızı sağlar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d82e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adım 1: En az temsil edilen sınıflar için RandomOverSampler\n",
    "print('\\nAdım 1: Aşırı az temsil edilen sınıflar için RandomOverSampler uygulanıyor...')\n",
    "min_samples_threshold = 60  # BorderlineSMOTE için gereken minimum örnek sayısı\n",
    "ros = RandomOverSampler(sampling_strategy={3: min_samples_threshold}, random_state=42)\n",
    "X_partial, y_partial = ros.fit_resample(X_train, y_train)\n",
    "\n",
    "# Ara sonuçları göster\n",
    "unique_partial, counts_partial = np.unique(y_partial, return_counts=True)\n",
    "print(\"\\nRandomOverSampler sonrası dağılım (ham sayılar):\")\n",
    "for i, (u, c) in enumerate(zip(unique_partial, counts_partial)):\n",
    "    print(f\"Sınıf {u} ({le.classes_[i]}): {c} örnek\")\n",
    "\n",
    "plot_class_distribution(y_partial, le.classes_, 'RandomOverSampler Sonrası')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b9d611",
   "metadata": {},
   "source": [
    "## Veri Dengeleme - Aşama 2\n",
    "\n",
    "İkinci aşamada, daha sofistike bir yaklaşım olan BorderlineSMOTE kullanılarak kalan sınıflar dengeleniyor. Bu yöntem, sadece rastgele kopyalama yerine sentetik örnekler oluşturur.\n",
    "\n",
    "Not: Bu aşama, veri setinin yapısına bağlı olarak başarısız olabilir. Bu durumda, ilk aşamadaki sonuçlar kullanılacaktır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0e95e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adım 2: Kalan sınıflar için BorderlineSMOTE\n",
    "print('\\nAdım 2: Kalan sınıflar için BorderlineSMOTE uygulanıyor...')\n",
    "borderline_smote = BorderlineSMOTE(random_state=42)\n",
    "\n",
    "try:\n",
    "    X_res, y_res = borderline_smote.fit_resample(X_partial, y_partial)\n",
    "    print(f'Kombine örnekleme tamamlandı: X_res {X_res.shape}, y_res {y_res.shape}')\n",
    "    \n",
    "    # Son dağılımı yazdır ve göster\n",
    "    unique_res, counts_res = np.unique(y_res, return_counts=True)\n",
    "    print(\"\\nSon Dağılım (ham sayılar):\")\n",
    "    for i, (u, c) in enumerate(zip(unique_res, counts_res)):\n",
    "        print(f\"Sınıf {u} ({le.classes_[i]}): {c} örnek\")\n",
    "    \n",
    "    plot_class_distribution(y_res, le.classes_, 'Son Dengelenmiş Dağılım')\n",
    "\n",
    "except Exception as e:\n",
    "    print(f'BorderlineSMOTE örnekleme başarısız oldu: {e} - kısmi örneklenmiş veri kullanılıyor')\n",
    "    X_res, y_res = X_partial, y_partial\n",
    "    plot_class_distribution(y_res, le.classes_, 'Kısmi Örnekleme (BorderlineSMOTE başarısız)')\n",
    "    \n",
    "print(\"\\nİşlem hattı tamamlandı. Yeniden örneklenmiş eğitim verisi (X_res, y_res) ve test verisi (X_test, y_test) hazır.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291b507b",
   "metadata": {},
   "source": [
    "## Özellik Seçimi (K-Best Feature Selection)\n",
    "\n",
    "Model performansını artırmak ve aşırı öğrenmeyi (overfitting) azaltmak için K-Best özellik seçimi algoritmasını uygulayacağız. Bu algoritma, her özelliğin hedef değişkenle olan istatistiksel ilişkisini ölçer ve en anlamlı K özelliği seçer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c6196b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Best özellik seçimi uygulaması\n",
    "print('\\nK-Best özellik seçimi uygulanıyor...')\n",
    "\n",
    "k = 100  # Seçilecek özellik sayısı\n",
    "print(f\"Toplam özellik sayısı: {X_res.shape[1]}, Seçilecek özellik sayısı: {k}\")\n",
    "\n",
    "# SelectKBest ile özellik seçimi\n",
    "selector = SelectKBest(score_func=f_classif, k=k)\n",
    "X_res_selected = selector.fit_transform(X_res, y_res)\n",
    "X_test_selected = selector.transform(X_test)\n",
    "\n",
    "# Hangi özelliklerin seçildiğini gösteren görselleştirme\n",
    "selected_mask = selector.get_support()\n",
    "scores = selector.scores_\n",
    "feature_indices = np.arange(len(selected_mask))\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(feature_indices, scores, alpha=0.3, color='g')\n",
    "plt.bar(feature_indices[selected_mask], scores[selected_mask], color='g')\n",
    "plt.title('Özellik Skorları ve Seçilen Özellikler')\n",
    "plt.xlabel('Özellik İndeksi')\n",
    "plt.ylabel('F-değeri (F-value)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Özellik seçimi tamamlandı. Seçilen özelliklerin boyutu: {X_res_selected.shape}\")\n",
    "\n",
    "# Orijinal veriyi güncellenmiş veri ile değiştirelim\n",
    "X_res = X_res_selected\n",
    "X_test = X_test_selected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69962f4f",
   "metadata": {},
   "source": [
    "*-----------------------------------------------------------------------------------*\n",
    "# PyTorch LSTM MODEL EĞİTİMİ\n",
    "*-----------------------------------------------------------------------------------*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340b3123",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nPyTorch LSTM Model Eğitimi Başlıyor...\")\n",
    "\n",
    "# Veri yükleme, önişleme, bölme ve dengeleme adımlarının tamamlandığı varsayılır.\n",
    "# Bu noktada aşağıdaki değişkenlerin mevcut olması beklenir:\n",
    "# X_res, y_res (Dengelenmiş eğitim verisi)\n",
    "# X_val, y_val (Doğrulama verisi)\n",
    "# X_test, y_test (Test verisi)\n",
    "# le (LabelEncoder nesnesi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4bc7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dengelenmiş veri setinden doğrulama seti ayır\n",
    "X_train_bal, X_val, y_train_bal, y_val = train_test_split(\n",
    "    X_res, y_res, test_size=0.1, stratify=y_res, random_state=42\n",
    ")\n",
    "\n",
    "# Veri Ölçeklendirme (StandardScaler)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_bal)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"Veri ölçeklendirme tamamlandı.\")\n",
    "print(f\"Ölçeklenmiş eğitim verisi boyutu: {X_train_scaled.shape}\")\n",
    "print(f\"Ölçeklenmiş doğrulama verisi boyutu: {X_val_scaled.shape}\")\n",
    "print(f\"Ölçeklenmiş test verisi boyutu: {X_test_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f746e0b",
   "metadata": {},
   "source": [
    "## LSTM Modeli için Veri Hazırlığı\n",
    "\n",
    "PyTorch LSTM modeli için, veriyi uygun formata dönüştürmemiz gerekir. LSTM modeller sıralı veri bekler, bu nedenle öznitelik vektörünü zamansal bir diziye dönüştüreceğiz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa7d165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch tensörlerine dönüştürme ve veri setlerini hazırlama\n",
    "def create_sequence_data(X, y, sequence_length=10):\n",
    "    \"\"\"\n",
    "    Öznitelik vektörünü sıralı verilere dönüştürür.\n",
    "    FMA veri seti sıralı yapıda değil, bu nedenle yapay bir sıra oluşturuyoruz.\n",
    "    \"\"\"\n",
    "    # Veri boyutlarını kontrol et\n",
    "    n_samples, n_features = X.shape\n",
    "    \n",
    "    # Veriyi yeniden şekillendirme\n",
    "    features_per_timestep = n_features // sequence_length\n",
    "    \n",
    "    if features_per_timestep == 0:\n",
    "        features_per_timestep = 1\n",
    "        sequence_length = min(sequence_length, n_features)\n",
    "    \n",
    "    # Son timestep'e sığmayan özellikleri ele alma\n",
    "    remainder = n_features - (sequence_length * features_per_timestep)\n",
    "    \n",
    "    # Yeniden şekillendirilmiş veri için array oluşturma\n",
    "    X_seq = np.zeros((n_samples, sequence_length, features_per_timestep))\n",
    "    \n",
    "    # Veriyi yeniden şekillendirme\n",
    "    for i in range(n_samples):\n",
    "        for t in range(sequence_length):\n",
    "            start_idx = t * features_per_timestep\n",
    "            end_idx = min(start_idx + features_per_timestep, n_features)\n",
    "            \n",
    "            if start_idx < n_features:\n",
    "                X_seq[i, t, :end_idx-start_idx] = X[i, start_idx:end_idx]\n",
    "    \n",
    "    # PyTorch tensörlerine dönüştürme\n",
    "    X_tensor = torch.FloatTensor(X_seq)\n",
    "    y_tensor = torch.LongTensor(y)\n",
    "    \n",
    "    return X_tensor, y_tensor\n",
    "\n",
    "# Sıralı veri için hiperparametreler\n",
    "sequence_length = 10  # Her örnek için kullanılacak zaman adımı sayısı\n",
    "\n",
    "# Ölçeklenmiş verileri sıralı forma dönüştürme\n",
    "X_train_seq, y_train_tensor = create_sequence_data(X_train_scaled, y_train_bal, sequence_length)\n",
    "X_val_seq, y_val_tensor = create_sequence_data(X_val_scaled, y_val, sequence_length)\n",
    "X_test_seq, y_test_tensor = create_sequence_data(X_test_scaled, y_test, sequence_length)\n",
    "\n",
    "print(f\"Eğitim veri boyutu: {X_train_seq.shape}\")\n",
    "print(f\"Doğrulama veri boyutu: {X_val_seq.shape}\")\n",
    "print(f\"Test veri boyutu: {X_test_seq.shape}\")\n",
    "\n",
    "# PyTorch DataLoader oluşturma\n",
    "batch_size = 512\n",
    "train_dataset = TensorDataset(X_train_seq, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_seq, y_val_tensor)\n",
    "test_dataset = TensorDataset(X_test_seq, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719d6bcd",
   "metadata": {},
   "source": [
    "## LSTM Model Tanımı ve Eğitimi\n",
    "\n",
    "Aşağıda müzik türü sınıflandırması için bir LSTM (Long Short-Term Memory) ağı tanımlıyoruz. LSTM'ler, müzik gibi sıralı verilerde başarılı olan bir derin öğrenme mimarisidir.\n",
    "\n",
    "Model, LSTM katmanlarından sonra bir **attention mekanizması** içerir. Bu mekanizma, modelin yapay olarak oluşturulan zaman adımlarının hangilerinin daha bilgilendirici olduğunu öğrenmesine yardımcı olur ve her zaman adımına farklı ağırlıklar vererek daha etkili bir özellik kombinasyonu oluşturur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d73d18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention mekanizması sınıfı\n",
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Attention ağırlıklarını hesaplamak için linear katmanlar\n",
    "        self.attention_linear = nn.Linear(hidden_size, hidden_size)\n",
    "        self.context_vector = nn.Linear(hidden_size, 1, bias=False)\n",
    "        \n",
    "    def forward(self, lstm_outputs):\n",
    "        # lstm_outputs şekli: (batch_size, sequence_length, hidden_size)\n",
    "        \n",
    "        # Her zaman adımı için attention skorları hesapla\n",
    "        attention_weights = self.attention_linear(lstm_outputs)  # (batch_size, seq_len, hidden_size)\n",
    "        attention_weights = torch.tanh(attention_weights)\n",
    "        attention_scores = self.context_vector(attention_weights)  # (batch_size, seq_len, 1)\n",
    "        attention_scores = attention_scores.squeeze(2)  # (batch_size, seq_len)\n",
    "        \n",
    "        # Softmax ile normalize et\n",
    "        attention_weights = torch.softmax(attention_scores, dim=1)  # (batch_size, seq_len)\n",
    "        \n",
    "        # Weighted sum hesapla\n",
    "        # attention_weights: (batch_size, seq_len) -> (batch_size, seq_len, 1)\n",
    "        attention_weights = attention_weights.unsqueeze(2)\n",
    "        \n",
    "        # Weighted combination of LSTM outputs\n",
    "        attended_output = torch.sum(lstm_outputs * attention_weights, dim=1)  # (batch_size, hidden_size)\n",
    "        \n",
    "        return attended_output, attention_weights.squeeze(2)\n",
    "\n",
    "# Bidirectional LSTM model sınıfını tanımlama (Attention ile)\n",
    "class MusicGenreLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout=0.3, bidirectional=True):\n",
    "        super(MusicGenreLSTM, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional = bidirectional\n",
    "        \n",
    "        # Bidirectional LSTM katmanları\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size, \n",
    "            hidden_size=hidden_size, \n",
    "            num_layers=num_layers, \n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=bidirectional\n",
    "        )\n",
    "        \n",
    "        # Bidirectional LSTM çıkış boyutunu hesapla\n",
    "        lstm_output_size = hidden_size * 2 if bidirectional else hidden_size\n",
    "        \n",
    "        # Attention katmanı (bidirectional output için)\n",
    "        self.attention = AttentionLayer(lstm_output_size)\n",
    "        \n",
    "        # Batch normalization\n",
    "        self.batch_norm = nn.BatchNorm1d(lstm_output_size)\n",
    "        \n",
    "        # Dropout katmanı\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Tam bağlantılı katmanlar\n",
    "        self.fc1 = nn.Linear(lstm_output_size, 128)  # İlk tam bağlantılı katman\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "        \n",
    "        # Aktivasyon fonksiyonları\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Bidirectional LSTM katmanından geçirme\n",
    "        # x şekli: (batch_size, sequence_length, input_size)\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        \n",
    "        # lstm_out şekli: (batch_size, sequence_length, hidden_size * 2) for bidirectional\n",
    "        # veya (batch_size, sequence_length, hidden_size) for unidirectional\n",
    "        \n",
    "        # Attention mekanizması uygula\n",
    "        attended_output, attention_weights = self.attention(lstm_out)\n",
    "        \n",
    "        # Batch normalization\n",
    "        batch_norm_out = self.batch_norm(attended_output)\n",
    "        \n",
    "        # İlk tam bağlantılı katman\n",
    "        fc1_out = self.fc1(batch_norm_out)\n",
    "        fc1_out = self.relu(fc1_out)\n",
    "        fc1_out = self.dropout(fc1_out)\n",
    "        \n",
    "        # İkinci tam bağlantılı katman (çıkış katmanı)\n",
    "        out = self.fc2(fc1_out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "# Learning Rate Warmup Scheduler\n",
    "class WarmupScheduler:\n",
    "    def __init__(self, optimizer, warmup_epochs, max_lr, min_lr=1e-7):\n",
    "        self.optimizer = optimizer\n",
    "        self.warmup_epochs = warmup_epochs\n",
    "        self.max_lr = max_lr\n",
    "        self.min_lr = min_lr\n",
    "        self.current_epoch = 0\n",
    "        \n",
    "    def step(self):\n",
    "        if self.current_epoch < self.warmup_epochs:\n",
    "            # Linear warmup\n",
    "            lr = self.min_lr + (self.max_lr - self.min_lr) * (self.current_epoch / self.warmup_epochs)\n",
    "            for param_group in self.optimizer.param_groups:\n",
    "                param_group['lr'] = lr\n",
    "        self.current_epoch += 1\n",
    "        \n",
    "    def get_lr(self):\n",
    "        return self.optimizer.param_groups[0]['lr']\n",
    "\n",
    "# Model parametreleri\n",
    "input_size = X_train_seq.shape[2]  # Bir zaman adımındaki özellik sayısı\n",
    "hidden_size = 128  # LSTM gizli katman boyutu\n",
    "num_layers = 2  # LSTM katman sayısı\n",
    "num_classes = len(le.classes_)  # Sınıf sayısı\n",
    "dropout = 0.3\n",
    "bidirectional = True  # Bidirectional LSTM kullan\n",
    "\n",
    "# GPU kullanılabilir mi kontrol et\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Kullanılan cihaz: {device}\")\n",
    "\n",
    "# Model oluşturma\n",
    "model = MusicGenreLSTM(input_size, hidden_size, num_layers, num_classes, dropout, bidirectional).to(device)\n",
    "print(f\"\\nModel yapısı (Bidirectional: {bidirectional}):\")\n",
    "print(model)\n",
    "\n",
    "# Model parametrelerinin sayısını hesapla\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"\\nToplam parametre sayısı: {total_params:,}\")\n",
    "print(f\"Eğitilebilir parametre sayısı: {trainable_params:,}\")\n",
    "\n",
    "# Gelişmiş Learning Rate Ayarları\n",
    "initial_lr = 0.001\n",
    "max_lr = 0.01  # Maksimum öğrenme oranı\n",
    "min_lr = 1e-6  # Minimum öğrenme oranı\n",
    "warmup_epochs = 5  # İlk 5 epoch'ta yavaşça arttır\n",
    "\n",
    "# Kayıp fonksiyonu ve optimize edici tanımlama\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=min_lr)  # Düşük lr ile başla\n",
    "\n",
    "# Çoklu scheduler sistemi\n",
    "# 1. Warmup scheduler - İlk birkaç epoch'ta learning rate'i yavaşça arttırır\n",
    "warmup_scheduler = WarmupScheduler(optimizer, warmup_epochs=warmup_epochs, max_lr=max_lr, min_lr=min_lr)\n",
    "\n",
    "# 2. ReduceLROnPlateau - Validation loss plateau'ya ulaştığında LR'yi azaltır\n",
    "reduce_scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=3, min_lr=min_lr\n",
    ")\n",
    "\n",
    "# 3. CosineAnnealing - Cosine fonksiyonu ile LR'yi düzenli olarak değiştirir\n",
    "cosine_scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer, T_max=20, eta_min=min_lr\n",
    ")\n",
    "\n",
    "print(f\"\\nLearning Rate Ayarları:\")\n",
    "print(f\"Başlangıç LR: {min_lr}\")\n",
    "print(f\"Maksimum LR: {max_lr}\")\n",
    "print(f\"Minimum LR: {min_lr}\")\n",
    "print(f\"Warmup Epochs: {warmup_epochs}\")\n",
    "\n",
    "# Gelişmiş eğitim fonksiyonu\n",
    "def train_model_with_advanced_lr(model, train_loader, val_loader, criterion, optimizer, \n",
    "                                warmup_scheduler, reduce_scheduler, cosine_scheduler,\n",
    "                                num_epochs=50, early_stopping_patience=7, \n",
    "                                min_improvement_threshold=0.001, use_cosine_after_warmup=True):\n",
    "    \"\"\"\n",
    "    Gelişmiş learning rate scheduling ile model eğitimi\n",
    "    \"\"\"\n",
    "    # Ölçüm değerlerini saklayacak listeler\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accs = []\n",
    "    val_accs = []\n",
    "    learning_rates = []  # LR geçmişini takip et\n",
    "    \n",
    "    # En iyi doğrulama kaybını ve modeli saklama\n",
    "    best_val_loss = float('inf')\n",
    "    best_model = None\n",
    "    \n",
    "    # Erken durdurma için sayaç\n",
    "    early_stopping_counter = 0\n",
    "    \n",
    "    print(\"\\n=== Gelişmiş Learning Rate Scheduling ile Eğitim Başlıyor ===\")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Learning Rate Scheduling\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        if epoch < warmup_scheduler.warmup_epochs:\n",
    "            # Warmup phase\n",
    "            warmup_scheduler.step()\n",
    "            schedule_info = f\"Warmup Phase (Epoch {epoch+1}/{warmup_scheduler.warmup_epochs})\"\n",
    "        elif use_cosine_after_warmup and epoch >= warmup_scheduler.warmup_epochs:\n",
    "            # Cosine annealing after warmup\n",
    "            cosine_scheduler.step()\n",
    "            schedule_info = \"Cosine Annealing\"\n",
    "        else:\n",
    "            schedule_info = \"ReduceLROnPlateau (will be applied after validation)\"\n",
    "        \n",
    "        new_lr = optimizer.param_groups[0]['lr']\n",
    "        learning_rates.append(new_lr)\n",
    "        \n",
    "        # Eğitim modu\n",
    "        model.train()\n",
    "        \n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            # Gradyanları sıfırla\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # İleri geçiş\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Geri yayılım ve optimize etme\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping (exploding gradient problemini önler)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            # İstatistikleri güncelle\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            train_total += labels.size(0)\n",
    "            train_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        # Doğrulama modu\n",
    "        model.eval()\n",
    "        \n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                \n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        # Epoch sonuçlarını hesapla\n",
    "        epoch_train_loss = train_loss / len(train_loader.dataset)\n",
    "        epoch_val_loss = val_loss / len(val_loader.dataset)\n",
    "        epoch_train_acc = train_correct / train_total\n",
    "        epoch_val_acc = val_correct / val_total\n",
    "        \n",
    "        # ReduceLROnPlateau scheduler'ı warmup sonrası uygula\n",
    "        if epoch >= warmup_scheduler.warmup_epochs and not use_cosine_after_warmup:\n",
    "            reduce_scheduler.step(epoch_val_loss)\n",
    "        \n",
    "        # Sonuçları sakla\n",
    "        train_losses.append(epoch_train_loss)\n",
    "        val_losses.append(epoch_val_loss)\n",
    "        train_accs.append(epoch_train_acc)\n",
    "        val_accs.append(epoch_val_acc)\n",
    "        \n",
    "        # Eğitim durumunu yazdır\n",
    "        print(f'Epoch {epoch+1:2d}/{num_epochs} | '\n",
    "              f'Train Loss: {epoch_train_loss:.4f} | Train Acc: {epoch_train_acc:.4f} | '\n",
    "              f'Val Loss: {epoch_val_loss:.4f} | Val Acc: {epoch_val_acc:.4f} | '\n",
    "              f'LR: {new_lr:.2e} | {schedule_info}')\n",
    "        \n",
    "        # En iyi modeli sakla ve erken durdurma kontrolü\n",
    "        improvement = best_val_loss - epoch_val_loss\n",
    "        \n",
    "        if epoch_val_loss < best_val_loss:\n",
    "            if improvement > min_improvement_threshold:\n",
    "                early_stopping_counter = 0\n",
    "                improvement_msg = f\"✓ Significant improvement: {improvement:.6f}\"\n",
    "            else:\n",
    "                early_stopping_counter += 1\n",
    "                improvement_msg = f\"⚠ Minor improvement: {improvement:.6f}\"\n",
    "            \n",
    "            best_val_loss = epoch_val_loss\n",
    "            best_model = model.state_dict()\n",
    "            print(f\"  {improvement_msg}\")\n",
    "        else:\n",
    "            early_stopping_counter += 1\n",
    "            print(f\"  ✗ No improvement (counter: {early_stopping_counter}/{early_stopping_patience})\")\n",
    "            \n",
    "        # Erken durdurma kontrolü\n",
    "        if early_stopping_counter >= early_stopping_patience:\n",
    "            print(f'\\n🛑 Erken durdurma: {early_stopping_patience} epoch boyunca yeterli iyileşme yok.')\n",
    "            break\n",
    "    \n",
    "    # En iyi model ağırlıklarını yükle\n",
    "    if best_model is not None:\n",
    "        model.load_state_dict(best_model)\n",
    "        print(f\"\\n✅ En iyi model yüklendi (Val Loss: {best_val_loss:.6f})\")\n",
    "    \n",
    "    return model, train_losses, val_losses, train_accs, val_accs, learning_rates\n",
    "\n",
    "# Modeli gelişmiş LR scheduling ile eğit\n",
    "print(\"\\n🚀 Bidirectional LSTM model eğitimi (Gelişmiş LR Scheduling) başlıyor...\")\n",
    "num_epochs = 50\n",
    "early_stopping_patience = 3  # Erken durdurma için sabır sayısı\n",
    "min_improvement_threshold = 0.01  # İyileşme eşiği\n",
    "\n",
    "try:\n",
    "    model, train_losses, val_losses, train_accs, val_accs, learning_rates = train_model_with_advanced_lr(\n",
    "        model, train_loader, val_loader, criterion, optimizer, \n",
    "        warmup_scheduler, reduce_scheduler, cosine_scheduler,\n",
    "        num_epochs=num_epochs, \n",
    "        early_stopping_patience=early_stopping_patience,\n",
    "        min_improvement_threshold=min_improvement_threshold,\n",
    "        use_cosine_after_warmup=True  # Warmup sonrası cosine annealing kullan\n",
    "    )\n",
    "    print(\"\\n🎉 Model eğitimi başarıyla tamamlandı!\")\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n⏹️ Eğitim kullanıcı tarafından durduruldu.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f720d377",
   "metadata": {},
   "source": [
    "## Model Değerlendirmesi ve Görselleştirme\n",
    "\n",
    "Bu bölümde eğitilmiş modeli test veri seti üzerinde değerlendirip, sonuçları görselleştireceğiz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5361a1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eğitim sonuçlarını görselleştirme\n",
    "def plot_training_history(train_losses, val_losses, train_accs, val_accs, learning_rates=None):\n",
    "    if learning_rates is not None:\n",
    "        plt.figure(figsize=(18, 6))\n",
    "        \n",
    "        # Kayıp grafiği\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.plot(train_losses, label='Eğitim', marker='o', alpha=0.7)\n",
    "        plt.plot(val_losses, label='Doğrulama', marker='*', alpha=0.7)\n",
    "        plt.title('Model Kaybı')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Kayıp (Cross-Entropy)')\n",
    "        plt.legend()\n",
    "        plt.grid(True, linestyle='--', alpha=0.6)\n",
    "        \n",
    "        # Doğruluk grafiği\n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.plot(train_accs, label='Eğitim', marker='o', alpha=0.7)\n",
    "        plt.plot(val_accs, label='Doğrulama', marker='*', alpha=0.7)\n",
    "        plt.title('Model Doğruluğu')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Doğruluk')\n",
    "        plt.legend()\n",
    "        plt.grid(True, linestyle='--', alpha=0.6)\n",
    "        \n",
    "        # Learning Rate grafiği\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.plot(learning_rates, marker='s', alpha=0.7, color='red')\n",
    "        plt.title('Learning Rate Değişimi')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Learning Rate')\n",
    "        plt.yscale('log')  # Log scale for better visualization\n",
    "        plt.grid(True, linestyle='--', alpha=0.6)\n",
    "    else:\n",
    "        plt.figure(figsize=(14, 5))\n",
    "        \n",
    "        # Kayıp grafiği\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(train_losses, label='Eğitim', marker='o')\n",
    "        plt.plot(val_losses, label='Doğrulama', marker='*')\n",
    "        plt.title('Model Kaybı')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Kayıp (Cross-Entropy)')\n",
    "        plt.legend()\n",
    "        plt.grid(True, linestyle='--', alpha=0.6)\n",
    "        \n",
    "        # Doğruluk grafiği\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(train_accs, label='Eğitim', marker='o')\n",
    "        plt.plot(val_accs, label='Doğrulama', marker='*')\n",
    "        plt.title('Model Doğruluğu')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Doğruluk')\n",
    "        plt.legend()\n",
    "        plt.grid(True, linestyle='--', alpha=0.6)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Eğitim sonuçlarını görselleştir\n",
    "try:\n",
    "    if 'learning_rates' in locals():\n",
    "        plot_training_history(train_losses, val_losses, train_accs, val_accs, learning_rates)\n",
    "        \n",
    "        # Detaylı learning rate analizi\n",
    "        print(f\"\\n📊 Learning Rate İstatistikleri:\")\n",
    "        print(f\"Başlangıç LR: {learning_rates[0]:.2e}\")\n",
    "        print(f\"Maksimum LR: {max(learning_rates):.2e}\")\n",
    "        print(f\"Son LR: {learning_rates[-1]:.2e}\")\n",
    "        print(f\"Ortalama LR: {np.mean(learning_rates):.2e}\")\n",
    "    else:\n",
    "        plot_training_history(train_losses, val_losses, train_accs, val_accs)\n",
    "except NameError:\n",
    "    print(\"Eğitim geçmişi bulunamadı. Önce modeli eğitin.\")\n",
    "\n",
    "# Test veri seti üzerinde değerlendirme\n",
    "def evaluate_model(model, test_loader, device):\n",
    "    model.eval()\n",
    "    \n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            \n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(predicted.cpu().numpy())\n",
    "    \n",
    "    # Doğruluk hesapla\n",
    "    accuracy = np.mean(np.array(y_true) == np.array(y_pred))\n",
    "    \n",
    "    # Sonuçları yazdır\n",
    "    print(f\"\\n🎯 Test Doğruluğu: {accuracy:.4f}\")\n",
    "    \n",
    "    # Sınıflandırma raporu\n",
    "    print(\"\\n📋 Sınıflandırma Raporu:\")\n",
    "    print(classification_report(y_true, y_pred, target_names=le.classes_))\n",
    "    \n",
    "    # Karmaşıklık matrisi\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=le.classes_, yticklabels=le.classes_)\n",
    "    plt.title('Karmaşıklık Matrisi')\n",
    "    plt.xlabel('Tahmin Edilen Etiketler')\n",
    "    plt.ylabel('Gerçek Etiketler')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return y_true, y_pred\n",
    "\n",
    "# Test veri seti üzerinde değerlendir\n",
    "try:\n",
    "    y_true, y_pred = evaluate_model(model, test_loader, device)\n",
    "except NameError:\n",
    "    print(\"Model bulunamadı. Önce modeli eğitin.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pydebian (3.11.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
