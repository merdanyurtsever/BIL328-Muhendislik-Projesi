{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bfa6a60",
   "metadata": {},
   "source": [
    "# MÃ¼zik TÃ¼rÃ¼ SÄ±nÄ±flandÄ±rma Projesi\n",
    "\n",
    "Bu notebook, FMA (Free Music Archive) veri setini kullanarak mÃ¼zik tÃ¼rÃ¼ sÄ±nÄ±flandÄ±rma modeli geliÅŸtirmek iÃ§in veri hazÄ±rlama ve dengeleme iÅŸlemlerini iÃ§ermektedir.\n",
    "\n",
    "## Gerekli KÃ¼tÃ¼phanelerin Ä°Ã§e AktarÄ±lmasÄ±\n",
    "AÅŸaÄŸÄ±daki hÃ¼crede, projede kullanÄ±lacak temel Python kÃ¼tÃ¼phaneleri import edilmektedir:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2a41ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from imblearn.over_sampling import RandomOverSampler, BorderlineSMOTE\n",
    "from collections import Counter\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set(style='whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4206449",
   "metadata": {},
   "source": [
    "## YardÄ±mcÄ± Fonksiyonlar\n",
    "\n",
    "### SÄ±nÄ±f DaÄŸÄ±lÄ±mÄ± GÃ¶rselleÅŸtirme Fonksiyonu\n",
    "AÅŸaÄŸÄ±daki fonksiyon, veri setindeki sÄ±nÄ±f daÄŸÄ±lÄ±mlarÄ±nÄ± gÃ¶rselleÅŸtirmek iÃ§in kullanÄ±lacaktÄ±r. Bu gÃ¶rselleÅŸtirme, veri dengesizliÄŸini anlamamÄ±za yardÄ±mcÄ± olur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ae2a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_class_distribution(y, labels, title):\n",
    "    counts = pd.Series(y).value_counts().sort_index()\n",
    "    valid_indices = counts.index[counts.index < len(labels)]\n",
    "    counts = counts.loc[valid_indices]\n",
    "    names = labels[counts.index]\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    ax = sns.barplot(x=names, y=counts.values, hue=names, palette='viridis', legend=False)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('SÄ±nÄ±f')\n",
    "    ax.set_ylabel('SayÄ±')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471baca3",
   "metadata": {},
   "source": [
    "## Veri YÃ¼kleme ve Ã–n Ä°ÅŸleme\n",
    "\n",
    "Bu bÃ¶lÃ¼mdeki fonksiyon:\n",
    "- FMA metadata dosyalarÄ±nÄ± yÃ¼kler\n",
    "- Gerekli sÃ¼tunlarÄ± seÃ§er\n",
    "- Eksik verileri temizler\n",
    "- Etiketleri kodlar\n",
    "- Veriyi sayÄ±sal formata dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e25b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    tracks_path = 'fma_metadata/tracks.csv'\n",
    "    features_path = 'fma_metadata/features.csv'\n",
    "\n",
    "    if not os.path.exists(tracks_path) or not os.path.exists(features_path):\n",
    "        raise FileNotFoundError(f\"Gerekli veri dosyalarÄ± bulunamadÄ±. '{tracks_path}' ve '{features_path}' dosyalarÄ±nÄ±n mevcut olduÄŸundan emin olun.\")\n",
    "\n",
    "    tracks = pd.read_csv(tracks_path, index_col=0, header=[0,1])\n",
    "    \n",
    "    features = pd.read_csv(features_path, index_col=0, header=[0,1])  # Ã‡ok seviyeli baÅŸlÄ±kla oku\n",
    "    features = features.loc[:, features.columns.get_level_values(0) != 'statistics']  # 'statistics' sÃ¼tunlarÄ±nÄ± kaldÄ±r\n",
    "    features = features.astype(np.float32)  # SayÄ±sal olmayan sÃ¼tunlarÄ± kaldÄ±rdÄ±ktan sonra float'a dÃ¶nÃ¼ÅŸtÃ¼r\n",
    "\n",
    "    features.index = features.index.astype(str)\n",
    "    tracks.index = tracks.index.astype(str)\n",
    "\n",
    "    genre_series = tracks[('track', 'genre_top')].dropna()\n",
    "    common_index = features.index.intersection(genre_series.index)\n",
    "\n",
    "    X = features.loc[common_index]\n",
    "    y_labels = genre_series.loc[common_index]\n",
    "\n",
    "    X = X.fillna(0).replace([np.inf, -np.inf], 0).astype(np.float32)\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "    y = label_encoder.fit_transform(y_labels)\n",
    "\n",
    "    print('Veriler yÃ¼klendi ve Ã¶niÅŸlendi.')\n",
    "    return X, y, label_encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c705ac60",
   "metadata": {},
   "source": [
    "## BaÅŸlangÄ±Ã§ Veri Analizi\n",
    "\n",
    "Verinin ilk yÃ¼klemesini yapÄ±p, baÅŸlangÄ±Ã§taki sÄ±nÄ±f daÄŸÄ±lÄ±mÄ±nÄ± inceleyelim. Bu analiz, veri dengesizliÄŸi problemini gÃ¶rselleÅŸtirmemize yardÄ±mcÄ± olacak."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97f61d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veriyi yÃ¼kle ve Ã¶niÅŸle\n",
    "X, y, le = load_data()\n",
    "\n",
    "# BaÅŸlangÄ±Ã§ daÄŸÄ±lÄ±mÄ±nÄ± gÃ¶ster\n",
    "plot_class_distribution(y, le.classes_, 'BaÅŸlangÄ±Ã§ SÄ±nÄ±f DaÄŸÄ±lÄ±mÄ±')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b6b7b9",
   "metadata": {},
   "source": [
    "## Veri BÃ¶lme ve EÄŸitim Seti Analizi\n",
    "\n",
    "Veriyi eÄŸitim ve test setlerine ayÄ±rÄ±p, eÄŸitim setindeki sÄ±nÄ±f daÄŸÄ±lÄ±mÄ±nÄ± inceliyoruz. Stratified split kullanarak orijinal daÄŸÄ±lÄ±mÄ± koruyoruz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ef4617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veriyi bÃ¶l ve eÄŸitim daÄŸÄ±lÄ±mÄ±nÄ± gÃ¶ster\n",
    "# Ä°lk bÃ¶lme: Ana eÄŸitim ve test setleri\n",
    "X_train_orig, X_test, y_train_orig, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# Ä°kinci bÃ¶lme: Ana eÄŸitim setini resampling ve temiz doÄŸrulama setlerine ayÄ±r\n",
    "X_train_for_resample, X_val_clean, y_train_for_resample, y_val_clean = train_test_split(\n",
    "    X_train_orig, y_train_orig, test_size=0.15, stratify=y_train_orig, random_state=42\n",
    ")\n",
    "\n",
    "print(f'Ä°lk bÃ¶lÃ¼nme tamamlandÄ±: X_train_orig {X_train_orig.shape}, X_test {X_test.shape}')\n",
    "print(f'Ä°kinci bÃ¶lÃ¼nme tamamlandÄ±: X_train_for_resample {X_train_for_resample.shape}, X_val_clean {X_val_clean.shape}')\n",
    "\n",
    "plot_class_distribution(y_train_for_resample, le.classes_, 'Resampling Ä°Ã§in EÄŸitim Seti DaÄŸÄ±lÄ±mÄ±')\n",
    "\n",
    "# DetaylÄ± daÄŸÄ±lÄ±mÄ± yazdÄ±r\n",
    "unique, counts = np.unique(y_train_for_resample, return_counts=True)\n",
    "print(\"\\nResampling Ä°Ã§in EÄŸitim Seti DaÄŸÄ±lÄ±mÄ± (ham sayÄ±lar):\")\n",
    "for i, (u, c) in enumerate(zip(unique, counts)):\n",
    "    print(f\"SÄ±nÄ±f {u} ({le.classes_[i]}): {c} Ã¶rnek\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48a9420",
   "metadata": {},
   "source": [
    "## Veri Dengeleme - AÅŸama 1\n",
    "\n",
    "Ä°lk aÅŸamada, Ã§ok az Ã¶rneÄŸe sahip sÄ±nÄ±flar iÃ§in RandomOverSampler kullanÄ±lÄ±yor. Bu aÅŸama, BorderlineSMOTE iÃ§in yeterli Ã¶rnek sayÄ±sÄ±na ulaÅŸmamÄ±zÄ± saÄŸlar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d82e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced Oversampling Strategy - MORE AGGRESSIVE FOR BETTER BALANCE\n",
    "print('\\nEnhanced oversampling strategy for better class balance...')\n",
    "\n",
    "# Get current distribution\n",
    "unique, counts = np.unique(y_train_for_resample, return_counts=True)\n",
    "print(f\"Original distribution - Max: {max(counts)}, Min: {min(counts)}, Classes: {len(unique)}\")\n",
    "\n",
    "# More aggressive strategy: bring all classes to at least 25% of the max class\n",
    "max_samples = max(counts)\n",
    "target_min = max(15, int(max_samples * 0.25))  # At least 15 samples or 25% of max class\n",
    "\n",
    "print(f\"Target minimum samples per class: {target_min}\")\n",
    "\n",
    "# Create sampling strategy\n",
    "sampling_strategy = {}\n",
    "total_added = 0\n",
    "\n",
    "for class_idx, count in zip(unique, counts):\n",
    "    if count < target_min:\n",
    "        sampling_strategy[class_idx] = target_min\n",
    "        total_added += (target_min - count)\n",
    "        print(f\"  Boosting {le.classes_[class_idx]}: {count} â†’ {target_min} (+{target_min - count})\")\n",
    "\n",
    "if sampling_strategy:\n",
    "    print(f\"\\nApplying oversampling to {len(sampling_strategy)} classes...\")\n",
    "    print(f\"Total samples to be added: {total_added}\")\n",
    "    \n",
    "    ros = RandomOverSampler(sampling_strategy=sampling_strategy, random_state=42)\n",
    "    X_res, y_res = ros.fit_resample(X_train_for_resample, y_train_for_resample)\n",
    "    \n",
    "    print(f\"âœ… Enhanced oversampling completed!\")\n",
    "    print(f\"Dataset size: {len(y_train_for_resample)} â†’ {len(y_res)} (+{len(y_res) - len(y_train_for_resample)})\")\n",
    "else:\n",
    "    print(\"No oversampling needed - all classes already well represented\")\n",
    "    X_res, y_res = X_train_for_resample, y_train_for_resample\n",
    "\n",
    "# Show final distribution\n",
    "unique_final, counts_final = np.unique(y_res, return_counts=True)\n",
    "print(f\"\\nFinal distribution after enhanced oversampling:\")\n",
    "print(f\"Max: {max(counts_final)}, Min: {min(counts_final)}, Ratio: {max(counts_final)/min(counts_final):.2f}\")\n",
    "\n",
    "for i, (u, c) in enumerate(zip(unique_final, counts_final)):\n",
    "    if i < len(le.classes_):\n",
    "        print(f\"  {le.classes_[u]}: {c} samples\")\n",
    "\n",
    "plot_class_distribution(y_res, le.classes_, 'Enhanced Oversampling Results')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b9d611",
   "metadata": {},
   "source": [
    "## Veri Dengeleme - AÅŸama 2\n",
    "\n",
    "Ä°kinci aÅŸamada, daha sofistike bir yaklaÅŸÄ±m olan BorderlineSMOTE kullanÄ±larak kalan sÄ±nÄ±flar dengeleniyor. Bu yÃ¶ntem, sadece rastgele kopyalama yerine sentetik Ã¶rnekler oluÅŸturur.\n",
    "\n",
    "Not: Bu aÅŸama, veri setinin yapÄ±sÄ±na baÄŸlÄ± olarak baÅŸarÄ±sÄ±z olabilir. Bu durumda, ilk aÅŸamadaki sonuÃ§lar kullanÄ±lacaktÄ±r."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0e95e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# REMOVE BorderlineSMOTE - Major overfitting culprit\n",
    "print('\\nSkipping BorderlineSMOTE to prevent overfitting...')\n",
    "print('Using only minimal RandomOverSampler results for better generalization.')\n",
    "\n",
    "# No BorderlineSMOTE - use minimal oversampling results directly\n",
    "print(\"\\nFinal distribution after minimal oversampling:\")\n",
    "unique_final, counts_final = np.unique(y_res, return_counts=True)\n",
    "for i, (u, c) in enumerate(zip(unique_final, counts_final)):\n",
    "    print(f\"SÄ±nÄ±f {u} ({le.classes_[i]}): {c} Ã¶rnek\")\n",
    "\n",
    "print(\"\\nOversampling pipeline completed. Using conservative approach for better generalization.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2090aaf6",
   "metadata": {},
   "source": [
    "## SÄ±nÄ±f AÄŸÄ±rlÄ±klandÄ±rma (Class Weighting)\n",
    "\n",
    "Veri dengelemeye ek olarak, model eÄŸitimi sÄ±rasÄ±nda sÄ±nÄ±f aÄŸÄ±rlÄ±klandÄ±rma uygulayacaÄŸÄ±z. Bu yaklaÅŸÄ±m, az temsil edilen sÄ±nÄ±flara daha fazla Ã¶nem vererek modelin bu sÄ±nÄ±flarÄ± daha iyi Ã¶ÄŸrenmesini saÄŸlar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f61f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Simplified class weighting - focus on original imbalance\n",
    "print(\"\\nCalculating class weights for loss function...\")\n",
    "\n",
    "# Calculate weights from original unbalanced data\n",
    "class_weights = compute_class_weight(\n",
    "    'balanced',\n",
    "    classes=np.unique(y_train_for_resample),\n",
    "    y=y_train_for_resample\n",
    ")\n",
    "\n",
    "# Convert to PyTorch tensor\n",
    "class_weights_tensor = torch.FloatTensor(class_weights)\n",
    "\n",
    "print(\"Class weights calculated:\")\n",
    "for i, weight in enumerate(class_weights):\n",
    "    if i < len(le.classes_):\n",
    "        print(f\"{le.classes_[i]}: {weight:.3f}\")\n",
    "\n",
    "print(f\"\\nClass weights tensor shape: {class_weights_tensor.shape}\")\n",
    "print(f\"Max weight: {class_weights_tensor.max():.3f}, Min weight: {class_weights_tensor.min():.3f}\")\n",
    "\n",
    "# Check if weights are too extreme\n",
    "weight_ratio = class_weights_tensor.max() / class_weights_tensor.min()\n",
    "print(f\"Weight ratio (max/min): {weight_ratio:.2f}\")\n",
    "if weight_ratio > 10:\n",
    "    print(\"âš ï¸  WARNING: Very extreme class weights detected!\")\n",
    "    print(\"This may cause training instability. Consider using no weights or softer weights.\")\n",
    "    \n",
    "    # Option to use no class weights\n",
    "    USE_CLASS_WEIGHTS = False  # Set to False if weights are too extreme\n",
    "    if not USE_CLASS_WEIGHTS:\n",
    "        print(\"Disabling class weights for stability...\")\n",
    "        class_weights_tensor = torch.ones(len(le.classes_))\n",
    "else:\n",
    "    print(\"âœ… Class weights look reasonable.\")\n",
    "    USE_CLASS_WEIGHTS = True\n",
    "\n",
    "print(\"These weights will be used in CrossEntropyLoss.\")\n",
    "print(f\"Using class weights: {USE_CLASS_WEIGHTS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8700f560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DETAILED DATA FLOW VERIFICATION\n",
    "print(\"\\n=== DETAILED DATA PREPARATION SUMMARY ===\")\n",
    "print(f\"Original training samples: {len(y_train_for_resample)}\")\n",
    "print(f\"After minimal oversampling: {len(y_res)}\")\n",
    "print(f\"Features: {X_res.shape[1]}\")\n",
    "print(f\"Classes: {len(le.classes_)}\")\n",
    "\n",
    "# Show detailed class distribution comparison\n",
    "print(\"\\nğŸ” OVERSAMPLING VERIFICATION:\")\n",
    "print(\"BEFORE oversampling:\")\n",
    "unique_before, counts_before = np.unique(y_train_for_resample, return_counts=True)\n",
    "for i, (u, c) in enumerate(zip(unique_before, counts_before)):\n",
    "    if i < len(le.classes_):\n",
    "        print(f\"  {le.classes_[u]}: {c} samples\")\n",
    "\n",
    "print(\"\\nAFTER oversampling:\")\n",
    "unique_after, counts_after = np.unique(y_res, return_counts=True)\n",
    "for i, (u, c) in enumerate(zip(unique_after, counts_after)):\n",
    "    if i < len(le.classes_):\n",
    "        print(f\"  {le.classes_[u]}: {c} samples\")\n",
    "\n",
    "# Calculate oversampling effect\n",
    "total_before = len(y_train_for_resample)\n",
    "total_after = len(y_res)\n",
    "oversampling_factor = total_after / total_before\n",
    "print(f\"\\nOversampling effect: {total_before} â†’ {total_after} samples\")\n",
    "print(f\"Oversampling factor: {oversampling_factor:.2f}x\")\n",
    "\n",
    "if oversampling_factor <= 1.05:\n",
    "    print(\"âš ï¸  WARNING: Very little or no oversampling applied!\")\n",
    "    print(\"This might explain why accuracy hasn't improved significantly.\")\n",
    "else:\n",
    "    print(\"âœ… Oversampling successfully applied!\")\n",
    "\n",
    "print(\"\\nProceeding to feature selection and model training...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65e0072",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e4cdda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "291b507b",
   "metadata": {},
   "source": [
    "## Ã–zellik SeÃ§imi (K-Best Feature Selection)\n",
    "\n",
    "Model performansÄ±nÄ± artÄ±rmak ve aÅŸÄ±rÄ± Ã¶ÄŸrenmeyi (overfitting) azaltmak iÃ§in K-Best Ã¶zellik seÃ§imi algoritmasÄ±nÄ± uygulayacaÄŸÄ±z. Bu algoritma, her Ã¶zelliÄŸin hedef deÄŸiÅŸkenle olan istatistiksel iliÅŸkisini Ã¶lÃ§er ve en anlamlÄ± K Ã¶zelliÄŸi seÃ§er."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c6196b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Best Ã¶zellik seÃ§imi uygulamasÄ± - OPTIMIZED FOR BETTER PERFORMANCE\n",
    "print('\\nK-Best Ã¶zellik seÃ§imi uygulanÄ±yor...')\n",
    "\n",
    "k = 150  # INCREASED from 100 - more features for better performance\n",
    "print(f\"Toplam Ã¶zellik sayÄ±sÄ±: {X_res.shape[1]}, SeÃ§ilecek Ã¶zellik sayÄ±sÄ±: {k}\")\n",
    "\n",
    "# SelectKBest ile Ã¶zellik seÃ§imi - Sadece resampled data Ã¼zerinde fit et\n",
    "selector = SelectKBest(score_func=f_classif, k=k)\n",
    "X_res_selected = selector.fit_transform(X_res, y_res)\n",
    "\n",
    "# Fitted selector ile validation ve test setlerini transform et\n",
    "X_val_clean_selected = selector.transform(X_val_clean)\n",
    "X_test_selected = selector.transform(X_test)\n",
    "\n",
    "print(f\"Ã–zellik seÃ§imi tamamlandÄ±. SeÃ§ilen Ã¶zelliklerin boyutu: {X_res_selected.shape}\")\n",
    "print(f\"Validation set boyutu: {X_val_clean_selected.shape}\")\n",
    "print(f\"Test set boyutu: {X_test_selected.shape}\")\n",
    "\n",
    "# Orijinal veriyi gÃ¼ncellenmiÅŸ veri ile deÄŸiÅŸtirelim\n",
    "X_res = X_res_selected\n",
    "X_val_clean = X_val_clean_selected\n",
    "X_test = X_test_selected\n",
    "\n",
    "# Veri Ã–lÃ§eklendirme (RobustScaler) - Better for outliers\n",
    "print('\\nVeri Ã¶lÃ§eklendirme uygulanÄ±yor (RobustScaler)...')\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "scaler = RobustScaler()  # More robust to outliers than StandardScaler\n",
    "X_res_scaled = scaler.fit_transform(X_res)\n",
    "X_val_clean_scaled = scaler.transform(X_val_clean)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"Veri Ã¶lÃ§eklendirme tamamlandÄ±.\")\n",
    "print(f\"Ã–lÃ§eklenmiÅŸ resampled eÄŸitim verisi boyutu: {X_res_scaled.shape}\")\n",
    "print(f\"Ã–lÃ§eklenmiÅŸ validation verisi boyutu: {X_val_clean_scaled.shape}\")\n",
    "print(f\"Ã–lÃ§eklenmiÅŸ test verisi boyutu: {X_test_scaled.shape}\")\n",
    "\n",
    "# Veriyi gÃ¼ncellenmiÅŸ Ã¶lÃ§eklenmiÅŸ veriler ile deÄŸiÅŸtir\n",
    "X_res = X_res_scaled\n",
    "X_val_clean = X_val_clean_scaled\n",
    "X_test = X_test_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69962f4f",
   "metadata": {},
   "source": [
    "*-----------------------------------------------------------------------------------*\n",
    "# PyTorch LSTM MODEL EÄÄ°TÄ°MÄ°\n",
    "*-----------------------------------------------------------------------------------*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340b3123",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nPyTorch LSTM Model EÄŸitimi BaÅŸlÄ±yor...\")\n",
    "\n",
    "# Veri yÃ¼kleme, Ã¶niÅŸleme, bÃ¶lme ve dengeleme adÄ±mlarÄ±nÄ±n tamamlandÄ±ÄŸÄ± varsayÄ±lÄ±r.\n",
    "# Bu noktada aÅŸaÄŸÄ±daki deÄŸiÅŸkenlerin mevcut olmasÄ± beklenir:\n",
    "# X_res, y_res (DengelenmiÅŸ eÄŸitim verisi)\n",
    "# X_val, y_val (DoÄŸrulama verisi)\n",
    "# X_test, y_test (Test verisi)\n",
    "# le (LabelEncoder nesnesi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9a8a07",
   "metadata": {},
   "source": [
    "## GeliÅŸmiÅŸ Ã–zellik MÃ¼hendisliÄŸi ve Model Optimizasyonu (Ä°steÄŸe BaÄŸlÄ±)\n",
    "\n",
    "Bu bÃ¶lÃ¼mde, model performansÄ±nÄ± artÄ±rmak iÃ§in geliÅŸmiÅŸ Ã¶zellik mÃ¼hendisliÄŸi tekniklerini ve model optimizasyonlarÄ±nÄ± gÃ¼venli bir ÅŸekilde uygulayabiliriz. Bu teknikler veri sÄ±zÄ±ntÄ±sÄ±nÄ± Ã¶nlemek iÃ§in dikkatli bir ÅŸekilde tasarlanmÄ±ÅŸtÄ±r."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2a356a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced feature engineering - DISABLED to prevent overfitting\n",
    "print(\"Enhanced feature engineering is DISABLED to prevent overfitting.\")\n",
    "print(\"Using only basic features with minimal oversampling approach.\")\n",
    "\n",
    "print(f\"\\nFinal data dimensions:\")\n",
    "print(f\"Training (X_res): {X_res.shape}\")\n",
    "print(f\"Validation (X_val_clean): {X_val_clean.shape}\")\n",
    "print(f\"Test (X_test): {X_test.shape}\")\n",
    "\n",
    "print(\"\\nSimple and conservative approach for better generalization.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f746e0b",
   "metadata": {},
   "source": [
    "## LSTM Modeli iÃ§in Veri HazÄ±rlÄ±ÄŸÄ±\n",
    "\n",
    "PyTorch LSTM modeli iÃ§in, veriyi uygun formata dÃ¶nÃ¼ÅŸtÃ¼rmemiz gerekir. LSTM modeller sÄ±ralÄ± veri bekler, bu nedenle Ã¶znitelik vektÃ¶rÃ¼nÃ¼ zamansal bir diziye dÃ¶nÃ¼ÅŸtÃ¼receÄŸiz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa7d165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch tensÃ¶rlerine dÃ¶nÃ¼ÅŸtÃ¼rme ve veri setlerini hazÄ±rlama\n",
    "def create_sequence_data(X, y, sequence_length=10):\n",
    "    \"\"\"\n",
    "    Ã–znitelik vektÃ¶rÃ¼nÃ¼ sÄ±ralÄ± verilere dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r.\n",
    "    FMA veri seti sÄ±ralÄ± yapÄ±da deÄŸil, bu nedenle yapay bir sÄ±ra oluÅŸturuyoruz.\n",
    "    \"\"\"\n",
    "    print(f\"Creating sequences from {X.shape[0]} samples with {X.shape[1]} features...\")\n",
    "    \n",
    "    # Veri boyutlarÄ±nÄ± kontrol et\n",
    "    n_samples, n_features = X.shape\n",
    "    \n",
    "    # Veriyi yeniden ÅŸekillendirme\n",
    "    features_per_timestep = n_features // sequence_length\n",
    "    \n",
    "    if features_per_timestep == 0:\n",
    "        features_per_timestep = 1\n",
    "        sequence_length = min(sequence_length, n_features)\n",
    "    \n",
    "    print(f\"Sequence config: {sequence_length} timesteps, {features_per_timestep} features per timestep\")\n",
    "    \n",
    "    # Son timestep'e sÄ±ÄŸmayan Ã¶zellikleri ele alma\n",
    "    remainder = n_features - (sequence_length * features_per_timestep)\n",
    "    if remainder > 0:\n",
    "        print(f\"Note: {remainder} features will be unused due to sequence reshaping\")\n",
    "    \n",
    "    # Yeniden ÅŸekillendirilmiÅŸ veri iÃ§in array oluÅŸturma\n",
    "    X_seq = np.zeros((n_samples, sequence_length, features_per_timestep))\n",
    "    \n",
    "    # Veriyi yeniden ÅŸekillendirme\n",
    "    for i in range(n_samples):\n",
    "        for t in range(sequence_length):\n",
    "            start_idx = t * features_per_timestep\n",
    "            end_idx = min(start_idx + features_per_timestep, n_features)\n",
    "            \n",
    "            if start_idx < n_features:\n",
    "                X_seq[i, t, :end_idx-start_idx] = X[i, start_idx:end_idx]\n",
    "    \n",
    "    # PyTorch tensÃ¶rlerine dÃ¶nÃ¼ÅŸtÃ¼rme\n",
    "    X_tensor = torch.FloatTensor(X_seq)\n",
    "    y_tensor = torch.LongTensor(y)\n",
    "    \n",
    "    print(f\"âœ… Sequence creation completed: {X_tensor.shape} -> {y_tensor.shape}\")\n",
    "    return X_tensor, y_tensor\n",
    "\n",
    "# Optimized sequence parameters\n",
    "sequence_length = 10  # INCREASED from 5 for better temporal modeling\n",
    "\n",
    "print(\"\\n=== CREATING SEQUENCE DATA ===\")\n",
    "print(\"Converting scaled data to sequence format...\")\n",
    "\n",
    "# Ã–lÃ§eklenmiÅŸ verileri sÄ±ralÄ± forma dÃ¶nÃ¼ÅŸtÃ¼rme\n",
    "X_train_seq, y_train_tensor = create_sequence_data(X_res, y_res, sequence_length)\n",
    "X_val_seq, y_val_tensor = create_sequence_data(X_val_clean, y_val_clean, sequence_length)\n",
    "X_test_seq, y_test_tensor = create_sequence_data(X_test, y_test, sequence_length)\n",
    "\n",
    "print(f\"\\nFinal data shapes:\")\n",
    "print(f\"  Training: {X_train_seq.shape} samples\")\n",
    "print(f\"  Validation: {X_val_seq.shape} samples\")\n",
    "print(f\"  Test: {X_test_seq.shape} samples\")\n",
    "\n",
    "# Optimized DataLoader settings\n",
    "batch_size = 256  # REDUCED from 512 for better gradient updates\n",
    "train_dataset = TensorDataset(X_train_seq, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_seq, y_val_tensor)\n",
    "test_dataset = TensorDataset(X_test_seq, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "print(f\"\\nDataLoaders created:\")\n",
    "print(f\"  Training batches: {len(train_loader)} (batch_size={batch_size})\")\n",
    "print(f\"  Validation batches: {len(val_loader)}\")\n",
    "print(f\"  Test batches: {len(test_loader)}\")\n",
    "print(f\"  Total training samples: {len(train_loader.dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719d6bcd",
   "metadata": {},
   "source": [
    "## LSTM Model TanÄ±mÄ± ve EÄŸitimi\n",
    "\n",
    "AÅŸaÄŸÄ±da mÃ¼zik tÃ¼rÃ¼ sÄ±nÄ±flandÄ±rmasÄ± iÃ§in bir LSTM (Long Short-Term Memory) aÄŸÄ± tanÄ±mlÄ±yoruz. LSTM'ler, mÃ¼zik gibi sÄ±ralÄ± verilerde baÅŸarÄ±lÄ± olan bir derin Ã¶ÄŸrenme mimarisidir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6ce84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced LSTM model - FOCUSED IMPROVEMENTS FOR BETTER ACCURACY\n",
    "class MusicGenreLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout=0.3):\n",
    "        super(MusicGenreLSTM, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Bidirectional LSTM for better feature capture\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size, \n",
    "            hidden_size=hidden_size, \n",
    "            num_layers=num_layers, \n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=True  # Key improvement: bidirectional\n",
    "        )\n",
    "        \n",
    "        # Batch normalization for training stability\n",
    "        self.batch_norm = nn.BatchNorm1d(hidden_size * 2)  # *2 for bidirectional\n",
    "        \n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Two-layer classifier for better decision boundary\n",
    "        self.fc1 = nn.Linear(hidden_size * 2, hidden_size)  # *2 for bidirectional\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # LSTM forward pass\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        \n",
    "        # Use last time step output (both directions)\n",
    "        lstm_out = lstm_out[:, -1, :]\n",
    "        \n",
    "        # Batch normalization\n",
    "        lstm_out = self.batch_norm(lstm_out)\n",
    "        \n",
    "        # First FC layer with activation and dropout\n",
    "        out = self.fc1(lstm_out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        # Final classification layer\n",
    "        out = self.fc2(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "# Improved model parameters - balanced for better performance\n",
    "input_size = X_train_seq.shape[2]\n",
    "hidden_size = 64  # Keep balanced\n",
    "num_layers = 2  # Increased for more capacity\n",
    "num_classes = len(le.classes_)\n",
    "dropout = 0.3  # Optimal dropout\n",
    "\n",
    "# GPU kontrolÃ¼\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"KullanÄ±lan cihaz: {device}\")\n",
    "\n",
    "# Enhanced model creation\n",
    "model = MusicGenreLSTM(input_size, hidden_size, num_layers, num_classes, dropout).to(device)\n",
    "print(model)\n",
    "\n",
    "# Model parameter count\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total trainable parameters: {total_params:,}\")\n",
    "\n",
    "# Enhanced loss function and optimizer\n",
    "print(f\"Class weights being used: {class_weights_tensor}\")\n",
    "class_weights_device = class_weights_tensor.to(device)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights_device, label_smoothing=0.1)  # Label smoothing\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)  # AdamW for better generalization\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=5, eta_min=1e-6)  # Better LR schedule\n",
    "\n",
    "# Enhanced Training function - FOCUSED IMPROVEMENTS\n",
    "def train_model_enhanced(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accs = []\n",
    "    val_accs = []\n",
    "    \n",
    "    best_val_acc = 0.0  # Track best validation accuracy instead of loss\n",
    "    best_model = None\n",
    "    patience_counter = 0\n",
    "    PATIENCE = 7  # Increased patience for better convergence\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping for stability\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            train_total += labels.size(0)\n",
    "            train_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        # Calculate metrics\n",
    "        epoch_train_loss = train_loss / len(train_loader.dataset)\n",
    "        epoch_val_loss = val_loss / len(val_loader.dataset)\n",
    "        epoch_train_acc = train_correct / train_total\n",
    "        epoch_val_acc = val_correct / val_total\n",
    "        \n",
    "        # Learning rate scheduling\n",
    "        scheduler.step()\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        # Store metrics\n",
    "        train_losses.append(epoch_train_loss)\n",
    "        val_losses.append(epoch_val_loss)\n",
    "        train_accs.append(epoch_train_acc)\n",
    "        val_accs.append(epoch_val_acc)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{num_epochs} - '\n",
    "              f'Train Loss: {epoch_train_loss:.4f}, Train Acc: {epoch_train_acc:.4f}, '\n",
    "              f'Val Loss: {epoch_val_loss:.4f}, Val Acc: {epoch_val_acc:.4f}, LR: {current_lr:.6f}')\n",
    "        \n",
    "        # Save best model based on validation accuracy\n",
    "        if epoch_val_acc > best_val_acc:\n",
    "            best_val_acc = epoch_val_acc\n",
    "            best_model = model.state_dict().copy()\n",
    "            patience_counter = 0\n",
    "            print(f'*** New best validation accuracy: {best_val_acc:.4f} ***')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            \n",
    "        if patience_counter >= PATIENCE:\n",
    "            print(f'Early stopping at epoch {epoch+1}')\n",
    "            break\n",
    "    \n",
    "    # Load best model\n",
    "    model.load_state_dict(best_model)\n",
    "    print(f'Best validation accuracy achieved: {best_val_acc:.4f}')\n",
    "    return model, train_losses, val_losses, train_accs, val_accs\n",
    "\n",
    "# Train with enhanced settings\n",
    "print(\"Training with enhanced settings for better accuracy...\")\n",
    "model, train_losses, val_losses, train_accs, val_accs = train_model_enhanced(\n",
    "    model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=25\n",
    ")\n",
    "print(\"Enhanced training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d73d18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DIAGNOSTIC ANALYSIS - Check for potential issues\n",
    "print(\"=== DIAGNOSTIC ANALYSIS ===\")\n",
    "\n",
    "# 1. Check data shapes and ranges\n",
    "print(f\"Training data shape: {X_train_seq.shape}\")\n",
    "print(f\"Training labels shape: {y_train_tensor.shape}\")\n",
    "print(f\"Unique classes in training: {torch.unique(y_train_tensor)}\")\n",
    "print(f\"Data range - Min: {X_train_seq.min():.4f}, Max: {X_train_seq.max():.4f}\")\n",
    "print(f\"Data mean: {X_train_seq.mean():.4f}, std: {X_train_seq.std():.4f}\")\n",
    "\n",
    "# 2. Check class distribution\n",
    "class_counts = torch.bincount(y_train_tensor)\n",
    "print(f\"\\nClass distribution in training:\")\n",
    "for i, count in enumerate(class_counts):\n",
    "    if i < len(le.classes_):\n",
    "        print(f\"  {le.classes_[i]}: {count} samples\")\n",
    "\n",
    "# 3. Check for data leakage or issues\n",
    "print(f\"\\nValidation data shape: {X_val_seq.shape}\")\n",
    "print(f\"Test data shape: {X_test_seq.shape}\")\n",
    "print(f\"Val data range - Min: {X_val_seq.min():.4f}, Max: {X_val_seq.max():.4f}\")\n",
    "\n",
    "# 4. Check model complexity\n",
    "model_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "data_samples = len(y_train_tensor)\n",
    "params_per_sample = model_params / data_samples\n",
    "print(f\"\\nModel complexity analysis:\")\n",
    "print(f\"  Total parameters: {model_params:,}\")\n",
    "print(f\"  Training samples: {data_samples:,}\")\n",
    "print(f\"  Parameters per sample: {params_per_sample:.2f}\")\n",
    "if params_per_sample > 10:\n",
    "    print(\"  âš ï¸  WARNING: High parameter-to-sample ratio may cause overfitting\")\n",
    "elif params_per_sample < 0.1:\n",
    "    print(\"  âš ï¸  WARNING: Very low parameter-to-sample ratio may cause underfitting\")\n",
    "else:\n",
    "    print(\"  âœ… Parameter-to-sample ratio looks reasonable\")\n",
    "\n",
    "# 5. Quick baseline test\n",
    "print(f\"\\nBaseline accuracy (most frequent class): {class_counts.max().item() / data_samples:.4f}\")\n",
    "print(\"If model performs worse than this, there's a serious issue.\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6d764e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SIMPLE BASELINE COMPARISON\n",
    "print(\"=== BASELINE MODEL COMPARISON ===\")\n",
    "\n",
    "# Let's try a simple feedforward network first to establish baseline\n",
    "class SimpleFFN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes, dropout=0.3):\n",
    "        super(SimpleFFN, self).__init__()\n",
    "        # Flatten the sequence dimension\n",
    "        self.flatten_size = input_size * 5  # sequence_length = 5\n",
    "        \n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(self.flatten_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size // 2, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Flatten sequence data\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.layers(x)\n",
    "\n",
    "# Create baseline model\n",
    "baseline_model = SimpleFFN(input_size, 64, num_classes, 0.3).to(device)\n",
    "baseline_params = sum(p.numel() for p in baseline_model.parameters() if p.requires_grad)\n",
    "print(f\"Baseline FFN parameters: {baseline_params:,}\")\n",
    "print(f\"LSTM parameters: {model_params:,}\")\n",
    "\n",
    "# Quick test to see if data flows correctly\n",
    "with torch.no_grad():\n",
    "    sample_batch = next(iter(train_loader))\n",
    "    sample_input, sample_labels = sample_batch[0][:5].to(device), sample_batch[1][:5].to(device)\n",
    "    \n",
    "    # Test LSTM\n",
    "    lstm_output = model(sample_input)\n",
    "    print(f\"\\nLSTM output shape: {lstm_output.shape}\")\n",
    "    print(f\"LSTM output range: {lstm_output.min():.4f} to {lstm_output.max():.4f}\")\n",
    "    \n",
    "    # Test FFN\n",
    "    ffn_output = baseline_model(sample_input)\n",
    "    print(f\"FFN output shape: {ffn_output.shape}\")\n",
    "    print(f\"FFN output range: {ffn_output.min():.4f} to {ffn_output.max():.4f}\")\n",
    "    \n",
    "    print(f\"Sample labels: {sample_labels}\")\n",
    "    print(f\"Expected output classes: {num_classes}\")\n",
    "\n",
    "print(\"\\nModels initialized successfully. Proceeding with training...\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41781a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENSEMBLE APPROACH FOR BETTER ACCURACY\n",
    "print(\"=== ENSEMBLE TRAINING ===\")\n",
    "\n",
    "# Train multiple models with different initialization\n",
    "class EnsembleModel:\n",
    "    def __init__(self, num_models=3):\n",
    "        self.models = []\n",
    "        self.num_models = num_models\n",
    "        \n",
    "        for i in range(num_models):\n",
    "            model = MusicGenreLSTM(input_size, hidden_size, num_layers, num_classes, dropout).to(device)\n",
    "            self.models.append(model)\n",
    "            print(f\"Model {i+1} initialized\")\n",
    "    \n",
    "    def train_ensemble(self, train_loader, val_loader, criterion, num_epochs=20):\n",
    "        \"\"\"Train all models in ensemble\"\"\"\n",
    "        trained_models = []\n",
    "        \n",
    "        for i, model in enumerate(self.models):\n",
    "            print(f\"\\nTraining Model {i+1}/{self.num_models}...\")\n",
    "            \n",
    "            # Create separate optimizer for each model\n",
    "            optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "            scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=5)\n",
    "            \n",
    "            # Train model\n",
    "            trained_model, _, _, _, _ = train_model_enhanced(\n",
    "                model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs\n",
    "            )\n",
    "            trained_models.append(trained_model)\n",
    "        \n",
    "        self.models = trained_models\n",
    "        return self.models\n",
    "    \n",
    "    def predict(self, data_loader):\n",
    "        \"\"\"Make ensemble predictions\"\"\"\n",
    "        all_predictions = []\n",
    "        \n",
    "        # Get predictions from each model\n",
    "        for model in self.models:\n",
    "            model.eval()\n",
    "            predictions = []\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for inputs, _ in data_loader:\n",
    "                    inputs = inputs.to(device)\n",
    "                    outputs = model(inputs)\n",
    "                    probabilities = torch.softmax(outputs, dim=1)\n",
    "                    predictions.append(probabilities.cpu())\n",
    "            \n",
    "            all_predictions.append(torch.cat(predictions, dim=0))\n",
    "        \n",
    "        # Average predictions\n",
    "        ensemble_predictions = torch.stack(all_predictions).mean(dim=0)\n",
    "        return ensemble_predictions\n",
    "\n",
    "# Create and train ensemble (using 3 models for balance of performance and time)\n",
    "ensemble = EnsembleModel(num_models=3)\n",
    "print(f\"Training ensemble of {ensemble.num_models} models...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c3a4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA AUGMENTATION FOR BETTER GENERALIZATION\n",
    "print(\"\\n=== DATA AUGMENTATION ===\")\n",
    "\n",
    "def augment_data(X, y, augment_factor=0.2):\n",
    "    \"\"\"Simple data augmentation with noise injection\"\"\"\n",
    "    n_samples = X.shape[0]\n",
    "    n_augmented = int(n_samples * augment_factor)\n",
    "    \n",
    "    # Random indices for augmentation\n",
    "    aug_indices = np.random.choice(n_samples, n_augmented, replace=True)\n",
    "    \n",
    "    # Create augmented samples\n",
    "    X_aug = X[aug_indices].copy()\n",
    "    y_aug = y[aug_indices].copy()\n",
    "    \n",
    "    # Add small gaussian noise\n",
    "    noise_std = 0.01 * X_aug.std()\n",
    "    noise = np.random.normal(0, noise_std, X_aug.shape)\n",
    "    X_aug = X_aug + noise\n",
    "    \n",
    "    # Combine original and augmented data\n",
    "    X_combined = np.vstack([X, X_aug])\n",
    "    y_combined = np.hstack([y, y_aug])\n",
    "    \n",
    "    return X_combined, y_combined\n",
    "\n",
    "# Apply data augmentation to training data\n",
    "print(\"Applying data augmentation...\")\n",
    "X_res_aug, y_res_aug = augment_data(X_res, y_res, augment_factor=0.15)\n",
    "\n",
    "print(f\"Original training size: {X_res.shape[0]}\")\n",
    "print(f\"Augmented training size: {X_res_aug.shape[0]}\")\n",
    "print(f\"Augmentation factor: {(X_res_aug.shape[0] - X_res.shape[0]) / X_res.shape[0]:.2f}\")\n",
    "\n",
    "# CRITICAL: Verify class distribution is maintained after augmentation\n",
    "print(\"\\nğŸ” VERIFYING AUGMENTATION EFFECT:\")\n",
    "print(\"Before augmentation:\")\n",
    "unique_before_aug, counts_before_aug = np.unique(y_res, return_counts=True)\n",
    "for i, (u, c) in enumerate(zip(unique_before_aug, counts_before_aug)):\n",
    "    if i < len(le.classes_):\n",
    "        print(f\"  {le.classes_[u]}: {c} samples\")\n",
    "\n",
    "print(\"\\nAfter augmentation:\")\n",
    "unique_after_aug, counts_after_aug = np.unique(y_res_aug, return_counts=True)\n",
    "for i, (u, c) in enumerate(zip(unique_after_aug, counts_after_aug)):\n",
    "    if i < len(le.classes_):\n",
    "        print(f\"  {le.classes_[u]}: {c} samples\")\n",
    "\n",
    "# Update training data\n",
    "X_res = X_res_aug\n",
    "y_res = y_res_aug\n",
    "\n",
    "# Recreate sequence data with augmented dataset\n",
    "X_train_seq, y_train_tensor = create_sequence_data(X_res, y_res, sequence_length)\n",
    "print(f\"\\nNew training sequence shape: {X_train_seq.shape}\")\n",
    "print(f\"Final training labels: {len(y_train_tensor)} samples\")\n",
    "\n",
    "# Update train loader\n",
    "train_dataset = TensorDataset(X_train_seq, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
    "\n",
    "print(\"âœ… Data augmentation completed successfully!\")\n",
    "print(f\"Final DataLoader has {len(train_loader)} batches with batch_size={batch_size}\")\n",
    "print(f\"Total training samples in DataLoader: {len(train_loader.dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccc11c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CRITICAL: VERIFY DATA PIPELINE INTEGRITY\n",
    "print(\"\\n=== DATA PIPELINE VERIFICATION ===\")\n",
    "print(\"Checking if our oversampling and augmentation actually made it to the training loop...\")\n",
    "\n",
    "# Check actual class distribution in the final training tensor\n",
    "final_class_counts = torch.bincount(y_train_tensor)\n",
    "print(\"\\nğŸ“Š FINAL TRAINING DATA DISTRIBUTION:\")\n",
    "for i, count in enumerate(final_class_counts):\n",
    "    if i < len(le.classes_):\n",
    "        print(f\"  {le.classes_[i]}: {count} samples\")\n",
    "\n",
    "# Calculate balance metrics\n",
    "max_count = final_class_counts.max().item()\n",
    "min_count = final_class_counts.min().item()\n",
    "balance_ratio = max_count / min_count if min_count > 0 else float('inf')\n",
    "\n",
    "print(f\"\\nBalance Analysis:\")\n",
    "print(f\"  Most samples: {max_count}\")\n",
    "print(f\"  Least samples: {min_count}\")\n",
    "print(f\"  Imbalance ratio: {balance_ratio:.2f}\")\n",
    "\n",
    "if balance_ratio > 10:\n",
    "    print(\"  âš ï¸  WARNING: Still very imbalanced! Oversampling may not have worked.\")\n",
    "elif balance_ratio > 5:\n",
    "    print(\"  ğŸ“Š Moderate imbalance - this is acceptable\")\n",
    "else:\n",
    "    print(\"  âœ… Good balance achieved!\")\n",
    "\n",
    "# Verify data actually changed from original\n",
    "original_samples = len(y_train_for_resample)\n",
    "final_samples = len(y_train_tensor)\n",
    "data_increase = (final_samples - original_samples) / original_samples * 100\n",
    "\n",
    "print(f\"\\nData Growth Verification:\")\n",
    "print(f\"  Original samples: {original_samples}\")\n",
    "print(f\"  Final samples: {final_samples}\")\n",
    "print(f\"  Increase: {data_increase:.1f}%\")\n",
    "\n",
    "if data_increase < 5:\n",
    "    print(\"  âš ï¸  WARNING: Very little data augmentation applied!\")\n",
    "    print(\"  This suggests oversampling/augmentation isn't working properly.\")\n",
    "else:\n",
    "    print(f\"  âœ… Data successfully augmented by {data_increase:.1f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f720d377",
   "metadata": {},
   "source": [
    "## Model DeÄŸerlendirmesi ve GÃ¶rselleÅŸtirme\n",
    "\n",
    "Bu bÃ¶lÃ¼mde eÄŸitilmiÅŸ modeli test veri seti Ã¼zerinde deÄŸerlendirip, sonuÃ§larÄ± gÃ¶rselleÅŸtireceÄŸiz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5361a1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EÄŸitim sonuÃ§larÄ±nÄ± gÃ¶rselleÅŸtirme\n",
    "def plot_training_history(train_losses, val_losses, train_accs, val_accs):\n",
    "    plt.figure(figsize=(14, 5))\n",
    "    \n",
    "    # KayÄ±p grafiÄŸi\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses, label='EÄŸitim', marker='o')\n",
    "    plt.plot(val_losses, label='DoÄŸrulama', marker='*')\n",
    "    plt.title('Model KaybÄ±')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('KayÄ±p (Cross-Entropy)')\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "    \n",
    "    # DoÄŸruluk grafiÄŸi\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(train_accs, label='EÄŸitim', marker='o')\n",
    "    plt.plot(val_accs, label='DoÄŸrulama', marker='*')\n",
    "    plt.title('Model DoÄŸruluÄŸu')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('DoÄŸruluk')\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# EÄŸitim sonuÃ§larÄ±nÄ± gÃ¶rselleÅŸtir\n",
    "try:\n",
    "    plot_training_history(train_losses, val_losses, train_accs, val_accs)\n",
    "except NameError:\n",
    "    print(\"EÄŸitim geÃ§miÅŸi bulunamadÄ±. Ã–nce modeli eÄŸitin.\")\n",
    "\n",
    "# Test veri seti Ã¼zerinde deÄŸerlendirme\n",
    "def evaluate_model(model, test_loader, device):\n",
    "    model.eval()\n",
    "    \n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            \n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(predicted.cpu().numpy())\n",
    "    \n",
    "    # DoÄŸruluk hesapla\n",
    "    accuracy = np.mean(np.array(y_true) == np.array(y_pred))\n",
    "    \n",
    "    # SonuÃ§larÄ± yazdÄ±r\n",
    "    print(f\"Test DoÄŸruluÄŸu: {accuracy:.4f}\")\n",
    "    \n",
    "    # SÄ±nÄ±flandÄ±rma raporu\n",
    "    print(\"\\nSÄ±nÄ±flandÄ±rma Raporu:\")\n",
    "    print(classification_report(y_true, y_pred, target_names=le.classes_, \n",
    "                               zero_division=0, labels=np.unique(y_true)))\n",
    "    \n",
    "    # Class-wise performance analysis\n",
    "    print(\"\\nDetaylÄ± SÄ±nÄ±f BazÄ±nda Performans:\")\n",
    "    class_report = classification_report(y_true, y_pred, target_names=le.classes_, \n",
    "                                       output_dict=True, zero_division=0, labels=np.unique(y_true))\n",
    "    for class_name, metrics in class_report.items():\n",
    "        if isinstance(metrics, dict) and 'f1-score' in metrics:\n",
    "            print(f\"{class_name}: Precision={metrics['precision']:.3f}, Recall={metrics['recall']:.3f}, F1={metrics['f1-score']:.3f}, Support={metrics['support']}\")\n",
    "    \n",
    "    # KarmaÅŸÄ±klÄ±k matrisi\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=le.classes_, yticklabels=le.classes_)\n",
    "    plt.title('KarmaÅŸÄ±klÄ±k Matrisi')\n",
    "    plt.xlabel('Tahmin Edilen Etiketler')\n",
    "    plt.ylabel('GerÃ§ek Etiketler')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return y_true, y_pred\n",
    "\n",
    "# Test veri seti Ã¼zerinde deÄŸerlendir\n",
    "try:\n",
    "    y_true, y_pred = evaluate_model(model, test_loader, device)\n",
    "except NameError:\n",
    "    print(\"Model bulunamadÄ±. Ã–nce modeli eÄŸitin.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a7e87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score, accuracy_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def get_model_probabilities(model, test_loader, device):\n",
    "    \"\"\"\n",
    "    Get prediction probabilities from the trained model\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_proba = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            probabilities = F.softmax(outputs, dim=1)\n",
    "            \n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_proba.extend(probabilities.cpu().numpy())\n",
    "    \n",
    "    return np.array(y_true), np.array(y_proba)\n",
    "\n",
    "def comprehensive_evaluation(model, test_loader, device, class_names):\n",
    "    \"\"\"\n",
    "    Provide detailed evaluation metrics for the trained model\n",
    "    \"\"\"\n",
    "    # Get true labels and prediction probabilities\n",
    "    y_true, y_proba = get_model_probabilities(model, test_loader, device)\n",
    "    \n",
    "    # Get predictions\n",
    "    y_pred = np.argmax(y_proba, axis=1)\n",
    "    \n",
    "    # Basic metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    \n",
    "    # Per-class metrics\n",
    "    precision, recall, f1, support = precision_recall_fscore_support(\n",
    "        y_true, y_pred, average=None, labels=range(len(class_names))\n",
    "    )\n",
    "    \n",
    "    # Macro and weighted averages\n",
    "    precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(\n",
    "        y_true, y_pred, average='macro'\n",
    "    )\n",
    "    precision_weighted, recall_weighted, f1_weighted, _ = precision_recall_fscore_support(\n",
    "        y_true, y_pred, average='weighted'\n",
    "    )\n",
    "    \n",
    "    # AUC-ROC for multiclass\n",
    "    y_true_binarized = label_binarize(y_true, classes=range(len(class_names)))\n",
    "    auc_scores = []\n",
    "    for i in range(len(class_names)):\n",
    "        if len(np.unique(y_true_binarized[:, i])) > 1:  # Check if class exists\n",
    "            auc = roc_auc_score(y_true_binarized[:, i], y_proba[:, i])\n",
    "            auc_scores.append(auc)\n",
    "    \n",
    "    # Create detailed report\n",
    "    results = {\n",
    "        'accuracy': accuracy,\n",
    "        'macro_precision': precision_macro,\n",
    "        'macro_recall': recall_macro,\n",
    "        'macro_f1': f1_macro,\n",
    "        'weighted_precision': precision_weighted,\n",
    "        'weighted_recall': recall_weighted,\n",
    "        'weighted_f1': f1_weighted,\n",
    "        'mean_auc': np.mean(auc_scores) if auc_scores else 0,\n",
    "        'per_class_metrics': {\n",
    "            class_names[i]: {\n",
    "                'precision': precision[i],\n",
    "                'recall': recall[i],\n",
    "                'f1': f1[i],\n",
    "                'support': support[i]\n",
    "            } for i in range(len(class_names))\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Use the comprehensive evaluation function\n",
    "try:\n",
    "    if 'model' in locals() and 'test_loader' in locals():\n",
    "        print(\"\\nDetaylÄ± model deÄŸerlendirmesi...\")\n",
    "        detailed_results = comprehensive_evaluation(model, test_loader, device, le.classes_)\n",
    "        \n",
    "        print(f\"\\nDetaylÄ± SonuÃ§lar:\")\n",
    "        print(f\"Accuracy: {detailed_results['accuracy']:.4f}\")\n",
    "        print(f\"Macro F1: {detailed_results['macro_f1']:.4f}\")\n",
    "        print(f\"Weighted F1: {detailed_results['weighted_f1']:.4f}\")\n",
    "        print(f\"Mean AUC: {detailed_results['mean_auc']:.4f}\")\n",
    "        \n",
    "        print(\"\\nSÄ±nÄ±f bazÄ±nda detaylar:\")\n",
    "        for class_name, metrics in detailed_results['per_class_metrics'].items():\n",
    "            print(f\"{class_name}: F1={metrics['f1']:.3f}, Precision={metrics['precision']:.3f}, Recall={metrics['recall']:.3f}\")\n",
    "    else:\n",
    "        print(\"Model henÃ¼z eÄŸitilmemiÅŸ. Ã–nce modeli eÄŸitin.\")\n",
    "except NameError:\n",
    "    print(\"Model bulunamadÄ±. Ã–nce modeli eÄŸitin.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f88f77",
   "metadata": {},
   "source": [
    "## Model DeÄŸerlendirmesi ve Ä°leriye DÃ¶nÃ¼k Ã‡alÄ±ÅŸmalar\n",
    "\n",
    "MÃ¼zik tÃ¼rÃ¼ sÄ±nÄ±flandÄ±rma modelimiz veriyi dengeledikten sonra eÄŸitilmiÅŸtir. SonuÃ§lar deÄŸerlendirilirken ÅŸunlar gÃ¶z Ã¶nÃ¼nde bulundurulmalÄ±dÄ±r:\n",
    "\n",
    "1. **Veri Kalitesi**: FMA veri setindeki Ã¶zellikler, ses dosyalarÄ±ndan Ã§Ä±karÄ±lmÄ±ÅŸ Ã¶zelliklerdir. Daha iyi sonuÃ§lar iÃ§in ham ses verileri Ã¼zerinde spektrogram analizi yapÄ±labilir.\n",
    "\n",
    "2. **Model Mimarisi**: LSTM modeli, sÄ±ralÄ± verilerde baÅŸarÄ±lÄ± olmasÄ±na raÄŸmen, mÃ¼zik tÃ¼rÃ¼ tanÄ±ma iÃ§in CNN (Convolutional Neural Network) veya CNN-LSTM hibrit modeller de kullanÄ±labilir.\n",
    "\n",
    "3. **Hiperparametreler**: FarklÄ± hiperparametreler (Ã¶rn. Ã¶ÄŸrenme oranÄ±, katman sayÄ±sÄ±, nÃ¶ron sayÄ±sÄ±) ile model performansÄ± artÄ±rÄ±labilir.\n",
    "\n",
    "4. **Veri Dengeleme**: KullandÄ±ÄŸÄ±mÄ±z veri dengeleme yÃ¶ntemleri, eÄŸitim setindeki sÄ±nÄ±f daÄŸÄ±lÄ±mÄ±nÄ± eÅŸitlemeye yardÄ±mcÄ± olur, ancak sentetik veri oluÅŸturma riskleri de taÅŸÄ±r.\n",
    "\n",
    "5. **Ã–zellik SeÃ§imi**: K-Best algoritmasÄ± ile seÃ§ilen Ã¶zellikler, modelin daha iyi genelleme yapmasÄ±na ve aÅŸÄ±rÄ± Ã¶ÄŸrenmesinin azalmasÄ±na yardÄ±mcÄ± olabilir. FarklÄ± K deÄŸerleri denenerek optimum Ã¶zellik sayÄ±sÄ± bulunabilir.\n",
    "\n",
    "Ä°leriye dÃ¶nÃ¼k Ã§alÄ±ÅŸmalarda, daha karmaÅŸÄ±k modeller, farklÄ± Ã¶zellik Ã§Ä±karma teknikleri ve daha bÃ¼yÃ¼k veri setleri kullanÄ±larak performans artÄ±rÄ±labilir."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d18bee",
   "metadata": {},
   "source": [
    "## Model Optimizasyonu ve Sorun Giderme\n",
    "\n",
    "Bu bÃ¶lÃ¼m, model performansÄ±nÄ± artÄ±rmak iÃ§in Ã§eÅŸitli optimizasyon teknikleri ve sorun giderme yÃ¶ntemlerini iÃ§erir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c4d430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Performans Analizi ve Ä°yileÅŸtirme Ã–nerileri\n",
    "\n",
    "def analyze_model_performance():\n",
    "    \"\"\"\n",
    "    Model performansÄ±nÄ± analiz et ve iyileÅŸtirme Ã¶nerileri sun\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if 'detailed_results' in locals() or 'detailed_results' in globals():\n",
    "            results = detailed_results\n",
    "            \n",
    "            print(\"\\n=== MODEL PERFORMANS ANALÄ°ZÄ° ===\")\n",
    "            print(f\"Genel DoÄŸruluk: {results['accuracy']:.4f}\")\n",
    "            print(f\"Macro F1 Skoru: {results['macro_f1']:.4f}\")\n",
    "            print(f\"Weighted F1 Skoru: {results['weighted_f1']:.4f}\")\n",
    "            print(f\"Ortalama AUC: {results['mean_auc']:.4f}\")\n",
    "            \n",
    "            # Performans deÄŸerlendirmesi ve Ã¶neriler\n",
    "            if results['accuracy'] < 0.6:\n",
    "                print(\"\\nâš ï¸  DÃœÅÃœK PERFORMANS TESPÄ°T EDÄ°LDÄ°\")\n",
    "                print(\"Ã–neriler:\")\n",
    "                print(\"1. Daha fazla veri toplama\")\n",
    "                print(\"2. FarklÄ± model mimarisi deneme (CNN, Transformer)\")\n",
    "                print(\"3. Hiperparametre optimizasyonu\")\n",
    "                print(\"4. Veri Ã¶n iÅŸleme tekniklerini gÃ¶zden geÃ§irme\")\n",
    "                \n",
    "            elif results['accuracy'] < 0.75:\n",
    "                print(\"\\nğŸ“Š ORTA SEVÄ°YE PERFORMANS\")\n",
    "                print(\"Ä°yileÅŸtirme Ã¶nerileri:\")\n",
    "                print(\"1. Ã–zellik mÃ¼hendisliÄŸi uygulama\")\n",
    "                print(\"2. Model ensemble teknikleri\")\n",
    "                print(\"3. Daha sofistike veri dengeleme\")\n",
    "                print(\"4. Regularization teknikleri\")\n",
    "                \n",
    "            else:\n",
    "                print(\"\\nâœ… Ä°YÄ° PERFORMANS\")\n",
    "                print(\"Model baÅŸarÄ±lÄ± bir ÅŸekilde Ã§alÄ±ÅŸÄ±yor.\")\n",
    "                \n",
    "            # SÄ±nÄ±f bazÄ±nda performans analizi\n",
    "            print(\"\\n=== SINIF BAZINDA PERFORMANS ===\")\n",
    "            poor_classes = []\n",
    "            for class_name, metrics in results['per_class_metrics'].items():\n",
    "                if metrics['f1'] < 0.5:\n",
    "                    poor_classes.append(class_name)\n",
    "                    \n",
    "            if poor_classes:\n",
    "                print(f\"DÃ¼ÅŸÃ¼k performanslÄ± sÄ±nÄ±flar: {', '.join(poor_classes)}\")\n",
    "                print(\"Bu sÄ±nÄ±flar iÃ§in:\")\n",
    "                print(\"- Daha fazla veri toplama\")\n",
    "                print(\"- SÄ±nÄ±f aÄŸÄ±rlÄ±klandÄ±rma\")\n",
    "                print(\"- Focal loss kullanma\")\n",
    "                \n",
    "        else:\n",
    "            print(\"Model deÄŸerlendirmesi henÃ¼z yapÄ±lmamÄ±ÅŸ.\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Performans analizi sÄ±rasÄ±nda hata: {e}\")\n",
    "\n",
    "def get_improvement_suggestions():\n",
    "    \"\"\"\n",
    "    GeliÅŸmiÅŸ iyileÅŸtirme Ã¶nerileri\n",
    "    \"\"\"\n",
    "    suggestions = {\n",
    "        \"Veri Ä°yileÅŸtirmeleri\": [\n",
    "            \"Veri temizleme ve outlier detection\",\n",
    "            \"Feature scaling yÃ¶ntemlerini karÅŸÄ±laÅŸtÄ±rma (RobustScaler, MinMaxScaler)\",\n",
    "            \"Veri artÄ±rma teknikleri (audio augmentation)\"\n",
    "        ],\n",
    "        \"Model Ä°yileÅŸtirmeleri\": [\n",
    "            \"Bidirectional LSTM kullanma\",\n",
    "            \"Attention mechanism ekleme\",\n",
    "            \"Residual connections\",\n",
    "            \"Batch normalization optimizasyonu\"\n",
    "        ],\n",
    "        \"EÄŸitim Ä°yileÅŸtirmeleri\": [\n",
    "            \"Learning rate scheduling\",\n",
    "            \"Gradient clipping\",\n",
    "            \"Warm-up strategies\",\n",
    "            \"Cyclical learning rates\"\n",
    "        ],\n",
    "        \"Ensemble YÃ¶ntemleri\": [\n",
    "            \"FarklÄ± model mimarilerini birleÅŸtirme\",\n",
    "            \"Voting classifiers\",\n",
    "            \"Stacking\",\n",
    "            \"Bagging\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    print(\"\\n=== GELÄ°ÅMÄ°Å Ä°YÄ°LEÅTÄ°RME Ã–NERÄ°LERÄ° ===\")\n",
    "    for category, items in suggestions.items():\n",
    "        print(f\"\\n{category}:\")\n",
    "        for item in items:\n",
    "            print(f\"  â€¢ {item}\")\n",
    "\n",
    "# Performans analizini Ã§alÄ±ÅŸtÄ±r\n",
    "analyze_model_performance()\n",
    "get_improvement_suggestions()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL EÄÄ°TÄ°MÄ° VE DEÄERLENDÄ°RMESÄ° TAMAMLANDI\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f66fac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SIMPLIFICATION: Removed cross-validation and hyperparameter tuning\n",
    "# These add complexity and can lead to overfitting\n",
    "print(\"Cross-validation and hyperparameter tuning removed for simplicity.\")\n",
    "print(\"Focus on getting basic model to generalize well first.\")\n",
    "print(\"Once basic performance is good, then optimize hyperparameters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adafbaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FURTHER SIMPLIFICATION: Remove complex hyperparameter tuning\n",
    "print(\"Complex hyperparameter tuning removed to prevent overfitting.\")\n",
    "print(\"Current model uses conservative parameters proven to work well.\")\n",
    "print(\"Focus on data quality and model architecture before fine-tuning.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf044d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING DIAGNOSTICS - Check what's happening during training\n",
    "print(\"\\n=== TRAINING DIAGNOSTICS ===\")\n",
    "\n",
    "# Quick check of first batch performance\n",
    "with torch.no_grad():\n",
    "    # Get first batch\n",
    "    first_batch = next(iter(train_loader))\n",
    "    inputs, labels = first_batch[0].to(device), first_batch[1].to(device)\n",
    "    \n",
    "    # Check model output before training\n",
    "    model.eval()\n",
    "    outputs = model(inputs)\n",
    "    _, predictions = torch.max(outputs, 1)\n",
    "    \n",
    "    print(f\"First batch:\")\n",
    "    print(f\"  Input shape: {inputs.shape}\")\n",
    "    print(f\"  Labels shape: {labels.shape}\")\n",
    "    print(f\"  Output shape: {outputs.shape}\")\n",
    "    print(f\"  Unique labels in batch: {torch.unique(labels)}\")\n",
    "    print(f\"  Unique predictions: {torch.unique(predictions)}\")\n",
    "    \n",
    "    # Check if model is outputting reasonable probabilities\n",
    "    probs = torch.softmax(outputs, dim=1)\n",
    "    print(f\"  Output probabilities - Min: {probs.min():.4f}, Max: {probs.max():.4f}\")\n",
    "    print(f\"  Most confident prediction: {probs.max(dim=1)[0].mean():.4f}\")\n",
    "    \n",
    "    # Check accuracy on first batch\n",
    "    accuracy = (predictions == labels).float().mean()\n",
    "    print(f\"  Random accuracy on first batch: {accuracy:.4f}\")\n",
    "    print(f\"  Expected random accuracy: {1/num_classes:.4f}\")\n",
    "    \n",
    "    if accuracy < 0.05:  # Much worse than random\n",
    "        print(\"  âš ï¸  WARNING: Model performing much worse than random!\")\n",
    "        print(\"  This suggests a serious issue with model or data.\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Ready to start training. Watch for:\")\n",
    "print(\"1. Training accuracy should improve from random levels\")\n",
    "print(\"2. Validation accuracy should follow training (with some gap)\")\n",
    "print(\"3. Loss should decrease steadily\")\n",
    "print(\"4. If validation accuracy stays near random, there's an issue\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603bdf18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FINAL ACCURACY BOOST WITH BEST PRACTICES\n",
    "print(\"\\n=== FINAL PERFORMANCE OPTIMIZATION ===\")\n",
    "\n",
    "# Option 1: Train a single optimized model with all improvements\n",
    "print(\"Training optimized single model...\")\n",
    "optimized_model = MusicGenreLSTM(input_size, hidden_size, num_layers, num_classes, dropout).to(device)\n",
    "\n",
    "# Enhanced optimizer settings\n",
    "optimizer_opt = optim.AdamW(optimized_model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "scheduler_opt = optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer_opt, max_lr=0.003, epochs=25, steps_per_epoch=len(train_loader)\n",
    ")\n",
    "\n",
    "# Custom training function with OneCycleLR\n",
    "def train_optimized_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accs = []\n",
    "    val_accs = []\n",
    "    \n",
    "    best_val_acc = 0.0\n",
    "    best_model = None\n",
    "    patience_counter = 0\n",
    "    PATIENCE = 8\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            scheduler.step()  # Step after each batch for OneCycleLR\n",
    "            \n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            train_total += labels.size(0)\n",
    "            train_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        # Calculate metrics\n",
    "        epoch_train_loss = train_loss / len(train_loader.dataset)\n",
    "        epoch_val_loss = val_loss / len(val_loader.dataset)\n",
    "        epoch_train_acc = train_correct / train_total\n",
    "        epoch_val_acc = val_correct / val_total\n",
    "        \n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        # Store metrics\n",
    "        train_losses.append(epoch_train_loss)\n",
    "        val_losses.append(epoch_val_loss)\n",
    "        train_accs.append(epoch_train_acc)\n",
    "        val_accs.append(epoch_val_acc)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{num_epochs} - '\n",
    "              f'Train Loss: {epoch_train_loss:.4f}, Train Acc: {epoch_train_acc:.4f}, '\n",
    "              f'Val Loss: {epoch_val_loss:.4f}, Val Acc: {epoch_val_acc:.4f}, LR: {current_lr:.6f}')\n",
    "        \n",
    "        # Save best model\n",
    "        if epoch_val_acc > best_val_acc:\n",
    "            best_val_acc = epoch_val_acc\n",
    "            best_model = model.state_dict().copy()\n",
    "            patience_counter = 0\n",
    "            print(f'*** New best validation accuracy: {best_val_acc:.4f} ***')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            \n",
    "        if patience_counter >= PATIENCE:\n",
    "            print(f'Early stopping at epoch {epoch+1}')\n",
    "            break\n",
    "    \n",
    "    # Load best model\n",
    "    model.load_state_dict(best_model)\n",
    "    print(f'Final optimized model validation accuracy: {best_val_acc:.4f}')\n",
    "    return model, train_losses, val_losses, train_accs, val_accs\n",
    "\n",
    "# Train the optimized model\n",
    "print(\"\\nTraining final optimized model...\")\n",
    "final_model, final_train_losses, final_val_losses, final_train_accs, final_val_accs = train_optimized_model(\n",
    "    optimized_model, train_loader, val_loader, criterion, optimizer_opt, scheduler_opt, num_epochs=25\n",
    ")\n",
    "\n",
    "print(\"\\nğŸ¯ FINAL MODEL TRAINING COMPLETED!\")\n",
    "print(\"Ready for test evaluation...\")\n",
    "print(\"Expected improvement: 61% â†’ 65-70% test accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112de52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FINAL TEST EVALUATION\n",
    "print(\"\\n=== FINAL TEST EVALUATION ===\")\n",
    "\n",
    "# Evaluate the final optimized model\n",
    "def evaluate_final_model(model, test_loader, device):\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    y_proba = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            probabilities = torch.softmax(outputs, dim=1)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            \n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(predicted.cpu().numpy())\n",
    "            y_proba.extend(probabilities.cpu().numpy())\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = np.mean(np.array(y_true) == np.array(y_pred))\n",
    "    \n",
    "    return accuracy, y_true, y_pred, np.array(y_proba)\n",
    "\n",
    "# Test the final model\n",
    "final_accuracy, y_true_final, y_pred_final, y_proba_final = evaluate_final_model(final_model, test_loader, device)\n",
    "\n",
    "print(f\"\\nğŸ‰ FINAL TEST ACCURACY: {final_accuracy:.4f} ({final_accuracy*100:.1f}%)\")\n",
    "\n",
    "# Compare with baseline\n",
    "baseline_accuracy = 0.61  # Previous best\n",
    "improvement = final_accuracy - baseline_accuracy\n",
    "print(f\"Improvement over baseline: {improvement:.4f} ({improvement*100:.1f} percentage points)\")\n",
    "\n",
    "if final_accuracy > 0.65:\n",
    "    print(\"ğŸš€ EXCELLENT! Achieved target of >65% accuracy!\")\n",
    "elif final_accuracy > baseline_accuracy:\n",
    "    print(\"âœ… GOOD! Model improved over baseline.\")\n",
    "else:\n",
    "    print(\"ğŸ“Š Performance maintained at baseline level.\")\n",
    "\n",
    "# Quick class-wise performance\n",
    "from sklearn.metrics import classification_report\n",
    "print(\"\\nClass-wise Performance:\")\n",
    "report = classification_report(y_true_final, y_pred_final, target_names=le.classes_, output_dict=True, zero_division=0)\n",
    "for class_name, metrics in report.items():\n",
    "    if isinstance(metrics, dict) and 'f1-score' in metrics:\n",
    "        print(f\"{class_name}: F1={metrics['f1-score']:.3f}\")\n",
    "\n",
    "print(f\"\\nMacro F1: {report['macro avg']['f1-score']:.4f}\")\n",
    "print(f\"Weighted F1: {report['weighted avg']['f1-score']:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ¯ MUSIC GENRE CLASSIFICATION OPTIMIZATION COMPLETE!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c917ed",
   "metadata": {},
   "source": [
    "## ğŸš€ Performance Optimization Summary\n",
    "\n",
    "### Improvements Applied:\n",
    "\n",
    "1. **Enhanced Model Architecture:**\n",
    "   - Bidirectional LSTM for better feature capture\n",
    "   - Optimized batch normalization and dropout\n",
    "   - Two-layer classifier for better decision boundaries\n",
    "\n",
    "2. **Advanced Training Techniques:**\n",
    "   - OneCycleLR scheduler for better convergence\n",
    "   - Label smoothing to prevent overconfidence\n",
    "   - Gradient clipping for training stability\n",
    "   - AdamW optimizer with weight decay\n",
    "\n",
    "3. **Data Enhancement:**\n",
    "   - Increased K-best features to 150\n",
    "   - Simple data augmentation with noise injection\n",
    "   - Improved sequence length for temporal modeling\n",
    "\n",
    "4. **Training Optimization:**\n",
    "   - Balanced batch size (256)\n",
    "   - Appropriate early stopping patience\n",
    "   - Best model selection based on validation accuracy\n",
    "\n",
    "### Expected Results:\n",
    "- **Baseline:** 61% test accuracy\n",
    "- **Target:** 65-70% test accuracy\n",
    "- **Key Improvement:** Better generalization without overfitting\n",
    "\n",
    "### Next Steps if Needed:\n",
    "- Ensemble methods (3-5 models)\n",
    "- Advanced augmentation techniques\n",
    "- Hyperparameter fine-tuning\n",
    "- Architecture search (CNN, Transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45549268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMMEDIATE TRAINING VERIFICATION\n",
    "print(\"\\n=== QUICK TRAINING VERIFICATION ===\")\n",
    "print(\"Testing if the enhanced oversampling actually improves learning...\")\n",
    "\n",
    "# Quick check of first few batches to see class distribution\n",
    "print(\"\\nChecking class distribution in first few training batches:\")\n",
    "class_counter = Counter()\n",
    "for i, (inputs, labels) in enumerate(train_loader):\n",
    "    for label in labels:\n",
    "        class_counter[label.item()] += 1\n",
    "    if i >= 2:  # Check first 3 batches\n",
    "        break\n",
    "\n",
    "print(\"Classes seen in first 3 batches:\")\n",
    "for class_idx, count in sorted(class_counter.items()):\n",
    "    if class_idx < len(le.classes_):\n",
    "        print(f\"  {le.classes_[class_idx]}: {count} samples\")\n",
    "\n",
    "total_seen = sum(class_counter.values())\n",
    "if len(class_counter) < len(le.classes_) * 0.7:  # Less than 70% of classes\n",
    "    print(f\"\\nâš ï¸  WARNING: Only {len(class_counter)} out of {len(le.classes_)} classes seen\")\n",
    "    print(\"This suggests severe class imbalance still exists!\")\n",
    "else:\n",
    "    print(f\"\\nâœ… Good diversity: {len(class_counter)} out of {len(le.classes_)} classes seen\")\n",
    "\n",
    "print(f\"\\nReady to train with {len(y_train_tensor)} total training samples\")\n",
    "print(f\"Expected accuracy improvement from better balance: 63% â†’ 66-68%\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pydebian (3.11.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
