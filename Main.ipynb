{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bfa6a60",
   "metadata": {},
   "source": [
    "# MÃ¼zik TÃ¼rÃ¼ SÄ±nÄ±flandÄ±rma Projesi\n",
    "\n",
    "Bu notebook, FMA (Free Music Archive) veri setini kullanarak mÃ¼zik tÃ¼rÃ¼ sÄ±nÄ±flandÄ±rma modeli geliÅŸtirmek iÃ§in veri hazÄ±rlama ve dengeleme iÅŸlemlerini iÃ§ermektedir.\n",
    "\n",
    "## Gerekli KÃ¼tÃ¼phanelerin Ä°Ã§e AktarÄ±lmasÄ±\n",
    "AÅŸaÄŸÄ±daki hÃ¼crede, projede kullanÄ±lacak temel Python kÃ¼tÃ¼phaneleri import edilmektedir:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2a41ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from imblearn.over_sampling import RandomOverSampler, BorderlineSMOTE\n",
    "from collections import Counter\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set(style='whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4206449",
   "metadata": {},
   "source": [
    "## YardÄ±mcÄ± Fonksiyonlar\n",
    "\n",
    "### SÄ±nÄ±f DaÄŸÄ±lÄ±mÄ± GÃ¶rselleÅŸtirme Fonksiyonu\n",
    "AÅŸaÄŸÄ±daki fonksiyon, veri setindeki sÄ±nÄ±f daÄŸÄ±lÄ±mlarÄ±nÄ± gÃ¶rselleÅŸtirmek iÃ§in kullanÄ±lacaktÄ±r. Bu gÃ¶rselleÅŸtirme, veri dengesizliÄŸini anlamamÄ±za yardÄ±mcÄ± olur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ae2a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_class_distribution(y, labels, title):\n",
    "    counts = pd.Series(y).value_counts().sort_index()\n",
    "    valid_indices = counts.index[counts.index < len(labels)]\n",
    "    counts = counts.loc[valid_indices]\n",
    "    names = labels[counts.index]\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    ax = sns.barplot(x=names, y=counts.values, hue=names, palette='viridis', legend=False)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('SÄ±nÄ±f')\n",
    "    ax.set_ylabel('SayÄ±')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471baca3",
   "metadata": {},
   "source": [
    "## Veri YÃ¼kleme ve Ã–n Ä°ÅŸleme\n",
    "\n",
    "Bu bÃ¶lÃ¼mdeki fonksiyon:\n",
    "- FMA metadata dosyalarÄ±nÄ± yÃ¼kler\n",
    "- Gerekli sÃ¼tunlarÄ± seÃ§er\n",
    "- Eksik verileri temizler\n",
    "- Etiketleri kodlar\n",
    "- Veriyi sayÄ±sal formata dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e25b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    tracks_path = 'fma_metadata/tracks.csv'\n",
    "    features_path = 'fma_metadata/features.csv'\n",
    "\n",
    "    if not os.path.exists(tracks_path) or not os.path.exists(features_path):\n",
    "        raise FileNotFoundError(f\"Gerekli veri dosyalarÄ± bulunamadÄ±. '{tracks_path}' ve '{features_path}' dosyalarÄ±nÄ±n mevcut olduÄŸundan emin olun.\")\n",
    "\n",
    "    tracks = pd.read_csv(tracks_path, index_col=0, header=[0,1])\n",
    "    \n",
    "    features = pd.read_csv(features_path, index_col=0, header=[0,1])  # Ã‡ok seviyeli baÅŸlÄ±kla oku\n",
    "    features = features.loc[:, features.columns.get_level_values(0) != 'statistics']  # 'statistics' sÃ¼tunlarÄ±nÄ± kaldÄ±r\n",
    "    features = features.astype(np.float32)  # SayÄ±sal olmayan sÃ¼tunlarÄ± kaldÄ±rdÄ±ktan sonra float'a dÃ¶nÃ¼ÅŸtÃ¼r\n",
    "\n",
    "    features.index = features.index.astype(str)\n",
    "    tracks.index = tracks.index.astype(str)\n",
    "\n",
    "    genre_series = tracks[('track', 'genre_top')].dropna()\n",
    "    common_index = features.index.intersection(genre_series.index)\n",
    "\n",
    "    X = features.loc[common_index]\n",
    "    y_labels = genre_series.loc[common_index]\n",
    "\n",
    "    X = X.fillna(0).replace([np.inf, -np.inf], 0).astype(np.float32)\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "    y = label_encoder.fit_transform(y_labels)\n",
    "\n",
    "    print('Veriler yÃ¼klendi ve Ã¶niÅŸlendi.')\n",
    "    return X, y, label_encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c705ac60",
   "metadata": {},
   "source": [
    "## BaÅŸlangÄ±Ã§ Veri Analizi\n",
    "\n",
    "Verinin ilk yÃ¼klemesini yapÄ±p, baÅŸlangÄ±Ã§taki sÄ±nÄ±f daÄŸÄ±lÄ±mÄ±nÄ± inceleyelim. Bu analiz, veri dengesizliÄŸi problemini gÃ¶rselleÅŸtirmemize yardÄ±mcÄ± olacak."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97f61d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veriyi yÃ¼kle ve Ã¶niÅŸle\n",
    "X, y, le = load_data()\n",
    "\n",
    "# BaÅŸlangÄ±Ã§ daÄŸÄ±lÄ±mÄ±nÄ± gÃ¶ster\n",
    "plot_class_distribution(y, le.classes_, 'BaÅŸlangÄ±Ã§ SÄ±nÄ±f DaÄŸÄ±lÄ±mÄ±')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b6b7b9",
   "metadata": {},
   "source": [
    "## Veri BÃ¶lme ve EÄŸitim Seti Analizi\n",
    "\n",
    "Veriyi eÄŸitim ve test setlerine ayÄ±rÄ±p, eÄŸitim setindeki sÄ±nÄ±f daÄŸÄ±lÄ±mÄ±nÄ± inceliyoruz. Stratified split kullanarak orijinal daÄŸÄ±lÄ±mÄ± koruyoruz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ef4617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veriyi bÃ¶l ve eÄŸitim daÄŸÄ±lÄ±mÄ±nÄ± gÃ¶ster\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "plot_class_distribution(y_train, le.classes_, 'EÄŸitim Seti DaÄŸÄ±lÄ±mÄ±')\n",
    "print(f'EÄŸitim/test bÃ¶lÃ¼nmesi tamamlandÄ±: X_train {X_train.shape}, X_test {X_test.shape}')\n",
    "\n",
    "# DetaylÄ± daÄŸÄ±lÄ±mÄ± yazdÄ±r\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "print(\"\\nEÄŸitim Seti DaÄŸÄ±lÄ±mÄ± (ham sayÄ±lar):\")\n",
    "for i, (u, c) in enumerate(zip(unique, counts)):\n",
    "    print(f\"SÄ±nÄ±f {u} ({le.classes_[i]}): {c} Ã¶rnek\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48a9420",
   "metadata": {},
   "source": [
    "## Veri Dengeleme - AÅŸama 1\n",
    "\n",
    "Ä°lk aÅŸamada, Ã§ok az Ã¶rneÄŸe sahip sÄ±nÄ±flar iÃ§in RandomOverSampler kullanÄ±lÄ±yor. Bu aÅŸama, BorderlineSMOTE iÃ§in yeterli Ã¶rnek sayÄ±sÄ±na ulaÅŸmamÄ±zÄ± saÄŸlar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d82e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AdÄ±m 1: En az temsil edilen sÄ±nÄ±flar iÃ§in RandomOverSampler\n",
    "print('\\nAdÄ±m 1: AÅŸÄ±rÄ± az temsil edilen sÄ±nÄ±flar iÃ§in RandomOverSampler uygulanÄ±yor...')\n",
    "min_samples_threshold = 60  # BorderlineSMOTE iÃ§in gereken minimum Ã¶rnek sayÄ±sÄ±\n",
    "ros = RandomOverSampler(sampling_strategy={3: min_samples_threshold}, random_state=42)\n",
    "X_partial, y_partial = ros.fit_resample(X_train, y_train)\n",
    "\n",
    "# Ara sonuÃ§larÄ± gÃ¶ster\n",
    "unique_partial, counts_partial = np.unique(y_partial, return_counts=True)\n",
    "print(\"\\nRandomOverSampler sonrasÄ± daÄŸÄ±lÄ±m (ham sayÄ±lar):\")\n",
    "for i, (u, c) in enumerate(zip(unique_partial, counts_partial)):\n",
    "    print(f\"SÄ±nÄ±f {u} ({le.classes_[i]}): {c} Ã¶rnek\")\n",
    "\n",
    "plot_class_distribution(y_partial, le.classes_, 'RandomOverSampler SonrasÄ±')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b9d611",
   "metadata": {},
   "source": [
    "## Veri Dengeleme - AÅŸama 2\n",
    "\n",
    "Ä°kinci aÅŸamada, daha sofistike bir yaklaÅŸÄ±m olan BorderlineSMOTE kullanÄ±larak kalan sÄ±nÄ±flar dengeleniyor. Bu yÃ¶ntem, sadece rastgele kopyalama yerine sentetik Ã¶rnekler oluÅŸturur.\n",
    "\n",
    "Not: Bu aÅŸama, veri setinin yapÄ±sÄ±na baÄŸlÄ± olarak baÅŸarÄ±sÄ±z olabilir. Bu durumda, ilk aÅŸamadaki sonuÃ§lar kullanÄ±lacaktÄ±r."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0e95e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AdÄ±m 2: Kalan sÄ±nÄ±flar iÃ§in BorderlineSMOTE\n",
    "print('\\nAdÄ±m 2: Kalan sÄ±nÄ±flar iÃ§in BorderlineSMOTE uygulanÄ±yor...')\n",
    "borderline_smote = BorderlineSMOTE(random_state=42)\n",
    "\n",
    "try:\n",
    "    X_res, y_res = borderline_smote.fit_resample(X_partial, y_partial)\n",
    "    print(f'Kombine Ã¶rnekleme tamamlandÄ±: X_res {X_res.shape}, y_res {y_res.shape}')\n",
    "    \n",
    "    # Son daÄŸÄ±lÄ±mÄ± yazdÄ±r ve gÃ¶ster\n",
    "    unique_res, counts_res = np.unique(y_res, return_counts=True)\n",
    "    print(\"\\nSon DaÄŸÄ±lÄ±m (ham sayÄ±lar):\")\n",
    "    for i, (u, c) in enumerate(zip(unique_res, counts_res)):\n",
    "        print(f\"SÄ±nÄ±f {u} ({le.classes_[i]}): {c} Ã¶rnek\")\n",
    "    \n",
    "    plot_class_distribution(y_res, le.classes_, 'Son DengelenmiÅŸ DaÄŸÄ±lÄ±m')\n",
    "\n",
    "except Exception as e:\n",
    "    print(f'BorderlineSMOTE Ã¶rnekleme baÅŸarÄ±sÄ±z oldu: {e} - kÄ±smi Ã¶rneklenmiÅŸ veri kullanÄ±lÄ±yor')\n",
    "    X_res, y_res = X_partial, y_partial\n",
    "    plot_class_distribution(y_res, le.classes_, 'KÄ±smi Ã–rnekleme (BorderlineSMOTE baÅŸarÄ±sÄ±z)')\n",
    "    \n",
    "print(\"\\nÄ°ÅŸlem hattÄ± tamamlandÄ±. Yeniden Ã¶rneklenmiÅŸ eÄŸitim verisi (X_res, y_res) ve test verisi (X_test, y_test) hazÄ±r.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291b507b",
   "metadata": {},
   "source": [
    "## Ã–zellik SeÃ§imi (K-Best Feature Selection)\n",
    "\n",
    "Model performansÄ±nÄ± artÄ±rmak ve aÅŸÄ±rÄ± Ã¶ÄŸrenmeyi (overfitting) azaltmak iÃ§in K-Best Ã¶zellik seÃ§imi algoritmasÄ±nÄ± uygulayacaÄŸÄ±z. Bu algoritma, her Ã¶zelliÄŸin hedef deÄŸiÅŸkenle olan istatistiksel iliÅŸkisini Ã¶lÃ§er ve en anlamlÄ± K Ã¶zelliÄŸi seÃ§er."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c6196b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Best Ã¶zellik seÃ§imi uygulamasÄ±\n",
    "print('\\nK-Best Ã¶zellik seÃ§imi uygulanÄ±yor...')\n",
    "\n",
    "k = 100  # SeÃ§ilecek Ã¶zellik sayÄ±sÄ±\n",
    "print(f\"Toplam Ã¶zellik sayÄ±sÄ±: {X_res.shape[1]}, SeÃ§ilecek Ã¶zellik sayÄ±sÄ±: {k}\")\n",
    "\n",
    "# SelectKBest ile Ã¶zellik seÃ§imi\n",
    "selector = SelectKBest(score_func=f_classif, k=k)\n",
    "X_res_selected = selector.fit_transform(X_res, y_res)\n",
    "X_test_selected = selector.transform(X_test)\n",
    "\n",
    "# Hangi Ã¶zelliklerin seÃ§ildiÄŸini gÃ¶steren gÃ¶rselleÅŸtirme\n",
    "selected_mask = selector.get_support()\n",
    "scores = selector.scores_\n",
    "feature_indices = np.arange(len(selected_mask))\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(feature_indices, scores, alpha=0.3, color='g')\n",
    "plt.bar(feature_indices[selected_mask], scores[selected_mask], color='g')\n",
    "plt.title('Ã–zellik SkorlarÄ± ve SeÃ§ilen Ã–zellikler')\n",
    "plt.xlabel('Ã–zellik Ä°ndeksi')\n",
    "plt.ylabel('F-deÄŸeri (F-value)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Ã–zellik seÃ§imi tamamlandÄ±. SeÃ§ilen Ã¶zelliklerin boyutu: {X_res_selected.shape}\")\n",
    "\n",
    "# Orijinal veriyi gÃ¼ncellenmiÅŸ veri ile deÄŸiÅŸtirelim\n",
    "X_res = X_res_selected\n",
    "X_test = X_test_selected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69962f4f",
   "metadata": {},
   "source": [
    "*-----------------------------------------------------------------------------------*\n",
    "# PyTorch LSTM MODEL EÄÄ°TÄ°MÄ°\n",
    "*-----------------------------------------------------------------------------------*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340b3123",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nPyTorch LSTM Model EÄŸitimi BaÅŸlÄ±yor...\")\n",
    "\n",
    "# Veri yÃ¼kleme, Ã¶niÅŸleme, bÃ¶lme ve dengeleme adÄ±mlarÄ±nÄ±n tamamlandÄ±ÄŸÄ± varsayÄ±lÄ±r.\n",
    "# Bu noktada aÅŸaÄŸÄ±daki deÄŸiÅŸkenlerin mevcut olmasÄ± beklenir:\n",
    "# X_res, y_res (DengelenmiÅŸ eÄŸitim verisi)\n",
    "# X_val, y_val (DoÄŸrulama verisi)\n",
    "# X_test, y_test (Test verisi)\n",
    "# le (LabelEncoder nesnesi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4bc7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DengelenmiÅŸ veri setinden doÄŸrulama seti ayÄ±r\n",
    "X_train_bal, X_val, y_train_bal, y_val = train_test_split(\n",
    "    X_res, y_res, test_size=0.1, stratify=y_res, random_state=42\n",
    ")\n",
    "\n",
    "# Veri Ã–lÃ§eklendirme (StandardScaler)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_bal)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"Veri Ã¶lÃ§eklendirme tamamlandÄ±.\")\n",
    "print(f\"Ã–lÃ§eklenmiÅŸ eÄŸitim verisi boyutu: {X_train_scaled.shape}\")\n",
    "print(f\"Ã–lÃ§eklenmiÅŸ doÄŸrulama verisi boyutu: {X_val_scaled.shape}\")\n",
    "print(f\"Ã–lÃ§eklenmiÅŸ test verisi boyutu: {X_test_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f746e0b",
   "metadata": {},
   "source": [
    "## LSTM Modeli iÃ§in Veri HazÄ±rlÄ±ÄŸÄ±\n",
    "\n",
    "PyTorch LSTM modeli iÃ§in, veriyi uygun formata dÃ¶nÃ¼ÅŸtÃ¼rmemiz gerekir. LSTM modeller sÄ±ralÄ± veri bekler, bu nedenle Ã¶znitelik vektÃ¶rÃ¼nÃ¼ zamansal bir diziye dÃ¶nÃ¼ÅŸtÃ¼receÄŸiz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa7d165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch tensÃ¶rlerine dÃ¶nÃ¼ÅŸtÃ¼rme ve veri setlerini hazÄ±rlama\n",
    "def create_sequence_data(X, y, sequence_length=10):\n",
    "    \"\"\"\n",
    "    Ã–znitelik vektÃ¶rÃ¼nÃ¼ sÄ±ralÄ± verilere dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r.\n",
    "    FMA veri seti sÄ±ralÄ± yapÄ±da deÄŸil, bu nedenle yapay bir sÄ±ra oluÅŸturuyoruz.\n",
    "    \"\"\"\n",
    "    # Veri boyutlarÄ±nÄ± kontrol et\n",
    "    n_samples, n_features = X.shape\n",
    "    \n",
    "    # Veriyi yeniden ÅŸekillendirme\n",
    "    features_per_timestep = n_features // sequence_length\n",
    "    \n",
    "    if features_per_timestep == 0:\n",
    "        features_per_timestep = 1\n",
    "        sequence_length = min(sequence_length, n_features)\n",
    "    \n",
    "    # Son timestep'e sÄ±ÄŸmayan Ã¶zellikleri ele alma\n",
    "    remainder = n_features - (sequence_length * features_per_timestep)\n",
    "    \n",
    "    # Yeniden ÅŸekillendirilmiÅŸ veri iÃ§in array oluÅŸturma\n",
    "    X_seq = np.zeros((n_samples, sequence_length, features_per_timestep))\n",
    "    \n",
    "    # Veriyi yeniden ÅŸekillendirme\n",
    "    for i in range(n_samples):\n",
    "        for t in range(sequence_length):\n",
    "            start_idx = t * features_per_timestep\n",
    "            end_idx = min(start_idx + features_per_timestep, n_features)\n",
    "            \n",
    "            if start_idx < n_features:\n",
    "                X_seq[i, t, :end_idx-start_idx] = X[i, start_idx:end_idx]\n",
    "    \n",
    "    # PyTorch tensÃ¶rlerine dÃ¶nÃ¼ÅŸtÃ¼rme\n",
    "    X_tensor = torch.FloatTensor(X_seq)\n",
    "    y_tensor = torch.LongTensor(y)\n",
    "    \n",
    "    return X_tensor, y_tensor\n",
    "\n",
    "# SÄ±ralÄ± veri iÃ§in hiperparametreler\n",
    "sequence_length = 10  # Her Ã¶rnek iÃ§in kullanÄ±lacak zaman adÄ±mÄ± sayÄ±sÄ±\n",
    "\n",
    "# Ã–lÃ§eklenmiÅŸ verileri sÄ±ralÄ± forma dÃ¶nÃ¼ÅŸtÃ¼rme\n",
    "X_train_seq, y_train_tensor = create_sequence_data(X_train_scaled, y_train_bal, sequence_length)\n",
    "X_val_seq, y_val_tensor = create_sequence_data(X_val_scaled, y_val, sequence_length)\n",
    "X_test_seq, y_test_tensor = create_sequence_data(X_test_scaled, y_test, sequence_length)\n",
    "\n",
    "print(f\"EÄŸitim veri boyutu: {X_train_seq.shape}\")\n",
    "print(f\"DoÄŸrulama veri boyutu: {X_val_seq.shape}\")\n",
    "print(f\"Test veri boyutu: {X_test_seq.shape}\")\n",
    "\n",
    "# PyTorch DataLoader oluÅŸturma\n",
    "batch_size = 512\n",
    "train_dataset = TensorDataset(X_train_seq, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_seq, y_val_tensor)\n",
    "test_dataset = TensorDataset(X_test_seq, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719d6bcd",
   "metadata": {},
   "source": [
    "## LSTM Model TanÄ±mÄ± ve EÄŸitimi\n",
    "\n",
    "AÅŸaÄŸÄ±da mÃ¼zik tÃ¼rÃ¼ sÄ±nÄ±flandÄ±rmasÄ± iÃ§in bir LSTM (Long Short-Term Memory) aÄŸÄ± tanÄ±mlÄ±yoruz. LSTM'ler, mÃ¼zik gibi sÄ±ralÄ± verilerde baÅŸarÄ±lÄ± olan bir derin Ã¶ÄŸrenme mimarisidir.\n",
    "\n",
    "Model, LSTM katmanlarÄ±ndan sonra bir **attention mekanizmasÄ±** iÃ§erir. Bu mekanizma, modelin yapay olarak oluÅŸturulan zaman adÄ±mlarÄ±nÄ±n hangilerinin daha bilgilendirici olduÄŸunu Ã¶ÄŸrenmesine yardÄ±mcÄ± olur ve her zaman adÄ±mÄ±na farklÄ± aÄŸÄ±rlÄ±klar vererek daha etkili bir Ã¶zellik kombinasyonu oluÅŸturur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d73d18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention mekanizmasÄ± sÄ±nÄ±fÄ±\n",
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Attention aÄŸÄ±rlÄ±klarÄ±nÄ± hesaplamak iÃ§in linear katmanlar\n",
    "        self.attention_linear = nn.Linear(hidden_size, hidden_size)\n",
    "        self.context_vector = nn.Linear(hidden_size, 1, bias=False)\n",
    "        \n",
    "    def forward(self, lstm_outputs):\n",
    "        # lstm_outputs ÅŸekli: (batch_size, sequence_length, hidden_size)\n",
    "        \n",
    "        # Her zaman adÄ±mÄ± iÃ§in attention skorlarÄ± hesapla\n",
    "        attention_weights = self.attention_linear(lstm_outputs)  # (batch_size, seq_len, hidden_size)\n",
    "        attention_weights = torch.tanh(attention_weights)\n",
    "        attention_scores = self.context_vector(attention_weights)  # (batch_size, seq_len, 1)\n",
    "        attention_scores = attention_scores.squeeze(2)  # (batch_size, seq_len)\n",
    "        \n",
    "        # Softmax ile normalize et\n",
    "        attention_weights = torch.softmax(attention_scores, dim=1)  # (batch_size, seq_len)\n",
    "        \n",
    "        # Weighted sum hesapla\n",
    "        # attention_weights: (batch_size, seq_len) -> (batch_size, seq_len, 1)\n",
    "        attention_weights = attention_weights.unsqueeze(2)\n",
    "        \n",
    "        # Weighted combination of LSTM outputs\n",
    "        attended_output = torch.sum(lstm_outputs * attention_weights, dim=1)  # (batch_size, hidden_size)\n",
    "        \n",
    "        return attended_output, attention_weights.squeeze(2)\n",
    "\n",
    "# Bidirectional LSTM model sÄ±nÄ±fÄ±nÄ± tanÄ±mlama (Attention ile)\n",
    "class MusicGenreLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout=0.3, bidirectional=True):\n",
    "        super(MusicGenreLSTM, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional = bidirectional\n",
    "        \n",
    "        # Bidirectional LSTM katmanlarÄ±\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size, \n",
    "            hidden_size=hidden_size, \n",
    "            num_layers=num_layers, \n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=bidirectional\n",
    "        )\n",
    "        \n",
    "        # Bidirectional LSTM Ã§Ä±kÄ±ÅŸ boyutunu hesapla\n",
    "        lstm_output_size = hidden_size * 2 if bidirectional else hidden_size\n",
    "        \n",
    "        # Attention katmanÄ± (bidirectional output iÃ§in)\n",
    "        self.attention = AttentionLayer(lstm_output_size)\n",
    "        \n",
    "        # Batch normalization\n",
    "        self.batch_norm = nn.BatchNorm1d(lstm_output_size)\n",
    "        \n",
    "        # Dropout katmanÄ±\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Tam baÄŸlantÄ±lÄ± katmanlar\n",
    "        self.fc1 = nn.Linear(lstm_output_size, 128)  # Ä°lk tam baÄŸlantÄ±lÄ± katman\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "        \n",
    "        # Aktivasyon fonksiyonlarÄ±\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Bidirectional LSTM katmanÄ±ndan geÃ§irme\n",
    "        # x ÅŸekli: (batch_size, sequence_length, input_size)\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        \n",
    "        # lstm_out ÅŸekli: (batch_size, sequence_length, hidden_size * 2) for bidirectional\n",
    "        # veya (batch_size, sequence_length, hidden_size) for unidirectional\n",
    "        \n",
    "        # Attention mekanizmasÄ± uygula\n",
    "        attended_output, attention_weights = self.attention(lstm_out)\n",
    "        \n",
    "        # Batch normalization\n",
    "        batch_norm_out = self.batch_norm(attended_output)\n",
    "        \n",
    "        # Ä°lk tam baÄŸlantÄ±lÄ± katman\n",
    "        fc1_out = self.fc1(batch_norm_out)\n",
    "        fc1_out = self.relu(fc1_out)\n",
    "        fc1_out = self.dropout(fc1_out)\n",
    "        \n",
    "        # Ä°kinci tam baÄŸlantÄ±lÄ± katman (Ã§Ä±kÄ±ÅŸ katmanÄ±)\n",
    "        out = self.fc2(fc1_out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "# Learning Rate Warmup Scheduler\n",
    "class WarmupScheduler:\n",
    "    def __init__(self, optimizer, warmup_epochs, max_lr, min_lr=1e-7):\n",
    "        self.optimizer = optimizer\n",
    "        self.warmup_epochs = warmup_epochs\n",
    "        self.max_lr = max_lr\n",
    "        self.min_lr = min_lr\n",
    "        self.current_epoch = 0\n",
    "        \n",
    "    def step(self):\n",
    "        if self.current_epoch < self.warmup_epochs:\n",
    "            # Linear warmup\n",
    "            lr = self.min_lr + (self.max_lr - self.min_lr) * (self.current_epoch / self.warmup_epochs)\n",
    "            for param_group in self.optimizer.param_groups:\n",
    "                param_group['lr'] = lr\n",
    "        self.current_epoch += 1\n",
    "        \n",
    "    def get_lr(self):\n",
    "        return self.optimizer.param_groups[0]['lr']\n",
    "\n",
    "# Model parametreleri\n",
    "input_size = X_train_seq.shape[2]  # Bir zaman adÄ±mÄ±ndaki Ã¶zellik sayÄ±sÄ±\n",
    "hidden_size = 128  # LSTM gizli katman boyutu\n",
    "num_layers = 2  # LSTM katman sayÄ±sÄ±\n",
    "num_classes = len(le.classes_)  # SÄ±nÄ±f sayÄ±sÄ±\n",
    "dropout = 0.3\n",
    "bidirectional = True  # Bidirectional LSTM kullan\n",
    "\n",
    "# GPU kullanÄ±labilir mi kontrol et\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"KullanÄ±lan cihaz: {device}\")\n",
    "\n",
    "# Model oluÅŸturma\n",
    "model = MusicGenreLSTM(input_size, hidden_size, num_layers, num_classes, dropout, bidirectional).to(device)\n",
    "print(f\"\\nModel yapÄ±sÄ± (Bidirectional: {bidirectional}):\")\n",
    "print(model)\n",
    "\n",
    "# Model parametrelerinin sayÄ±sÄ±nÄ± hesapla\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"\\nToplam parametre sayÄ±sÄ±: {total_params:,}\")\n",
    "print(f\"EÄŸitilebilir parametre sayÄ±sÄ±: {trainable_params:,}\")\n",
    "\n",
    "# GeliÅŸmiÅŸ Learning Rate AyarlarÄ±\n",
    "initial_lr = 0.001\n",
    "max_lr = 0.01  # Maksimum Ã¶ÄŸrenme oranÄ±\n",
    "min_lr = 1e-6  # Minimum Ã¶ÄŸrenme oranÄ±\n",
    "warmup_epochs = 5  # Ä°lk 5 epoch'ta yavaÅŸÃ§a arttÄ±r\n",
    "\n",
    "# KayÄ±p fonksiyonu ve optimize edici tanÄ±mlama\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=min_lr)  # DÃ¼ÅŸÃ¼k lr ile baÅŸla\n",
    "\n",
    "# Ã‡oklu scheduler sistemi\n",
    "# 1. Warmup scheduler - Ä°lk birkaÃ§ epoch'ta learning rate'i yavaÅŸÃ§a arttÄ±rÄ±r\n",
    "warmup_scheduler = WarmupScheduler(optimizer, warmup_epochs=warmup_epochs, max_lr=max_lr, min_lr=min_lr)\n",
    "\n",
    "# 2. ReduceLROnPlateau - Validation loss plateau'ya ulaÅŸtÄ±ÄŸÄ±nda LR'yi azaltÄ±r\n",
    "reduce_scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=3, min_lr=min_lr\n",
    ")\n",
    "\n",
    "# 3. CosineAnnealing - Cosine fonksiyonu ile LR'yi dÃ¼zenli olarak deÄŸiÅŸtirir\n",
    "cosine_scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer, T_max=20, eta_min=min_lr\n",
    ")\n",
    "\n",
    "print(f\"\\nLearning Rate AyarlarÄ±:\")\n",
    "print(f\"BaÅŸlangÄ±Ã§ LR: {min_lr}\")\n",
    "print(f\"Maksimum LR: {max_lr}\")\n",
    "print(f\"Minimum LR: {min_lr}\")\n",
    "print(f\"Warmup Epochs: {warmup_epochs}\")\n",
    "\n",
    "# GeliÅŸmiÅŸ eÄŸitim fonksiyonu\n",
    "def train_model_with_advanced_lr(model, train_loader, val_loader, criterion, optimizer, \n",
    "                                warmup_scheduler, reduce_scheduler, cosine_scheduler,\n",
    "                                num_epochs=50, early_stopping_patience=7, \n",
    "                                min_improvement_threshold=0.001, use_cosine_after_warmup=True):\n",
    "    \"\"\"\n",
    "    GeliÅŸmiÅŸ learning rate scheduling ile model eÄŸitimi\n",
    "    \"\"\"\n",
    "    # Ã–lÃ§Ã¼m deÄŸerlerini saklayacak listeler\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accs = []\n",
    "    val_accs = []\n",
    "    learning_rates = []  # LR geÃ§miÅŸini takip et\n",
    "    \n",
    "    # En iyi doÄŸrulama kaybÄ±nÄ± ve modeli saklama\n",
    "    best_val_loss = float('inf')\n",
    "    best_model = None\n",
    "    \n",
    "    # Erken durdurma iÃ§in sayaÃ§\n",
    "    early_stopping_counter = 0\n",
    "    \n",
    "    print(\"\\n=== GeliÅŸmiÅŸ Learning Rate Scheduling ile EÄŸitim BaÅŸlÄ±yor ===\")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Learning Rate Scheduling\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        if epoch < warmup_scheduler.warmup_epochs:\n",
    "            # Warmup phase\n",
    "            warmup_scheduler.step()\n",
    "            schedule_info = f\"Warmup Phase (Epoch {epoch+1}/{warmup_scheduler.warmup_epochs})\"\n",
    "        elif use_cosine_after_warmup and epoch >= warmup_scheduler.warmup_epochs:\n",
    "            # Cosine annealing after warmup\n",
    "            cosine_scheduler.step()\n",
    "            schedule_info = \"Cosine Annealing\"\n",
    "        else:\n",
    "            schedule_info = \"ReduceLROnPlateau (will be applied after validation)\"\n",
    "        \n",
    "        new_lr = optimizer.param_groups[0]['lr']\n",
    "        learning_rates.append(new_lr)\n",
    "        \n",
    "        # EÄŸitim modu\n",
    "        model.train()\n",
    "        \n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            # GradyanlarÄ± sÄ±fÄ±rla\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Ä°leri geÃ§iÅŸ\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Geri yayÄ±lÄ±m ve optimize etme\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping (exploding gradient problemini Ã¶nler)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            # Ä°statistikleri gÃ¼ncelle\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            train_total += labels.size(0)\n",
    "            train_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        # DoÄŸrulama modu\n",
    "        model.eval()\n",
    "        \n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                \n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        # Epoch sonuÃ§larÄ±nÄ± hesapla\n",
    "        epoch_train_loss = train_loss / len(train_loader.dataset)\n",
    "        epoch_val_loss = val_loss / len(val_loader.dataset)\n",
    "        epoch_train_acc = train_correct / train_total\n",
    "        epoch_val_acc = val_correct / val_total\n",
    "        \n",
    "        # ReduceLROnPlateau scheduler'Ä± warmup sonrasÄ± uygula\n",
    "        if epoch >= warmup_scheduler.warmup_epochs and not use_cosine_after_warmup:\n",
    "            reduce_scheduler.step(epoch_val_loss)\n",
    "        \n",
    "        # SonuÃ§larÄ± sakla\n",
    "        train_losses.append(epoch_train_loss)\n",
    "        val_losses.append(epoch_val_loss)\n",
    "        train_accs.append(epoch_train_acc)\n",
    "        val_accs.append(epoch_val_acc)\n",
    "        \n",
    "        # EÄŸitim durumunu yazdÄ±r\n",
    "        print(f'Epoch {epoch+1:2d}/{num_epochs} | '\n",
    "              f'Train Loss: {epoch_train_loss:.4f} | Train Acc: {epoch_train_acc:.4f} | '\n",
    "              f'Val Loss: {epoch_val_loss:.4f} | Val Acc: {epoch_val_acc:.4f} | '\n",
    "              f'LR: {new_lr:.2e} | {schedule_info}')\n",
    "        \n",
    "        # En iyi modeli sakla ve erken durdurma kontrolÃ¼\n",
    "        improvement = best_val_loss - epoch_val_loss\n",
    "        \n",
    "        if epoch_val_loss < best_val_loss:\n",
    "            if improvement > min_improvement_threshold:\n",
    "                early_stopping_counter = 0\n",
    "                improvement_msg = f\"âœ“ Significant improvement: {improvement:.6f}\"\n",
    "            else:\n",
    "                early_stopping_counter += 1\n",
    "                improvement_msg = f\"âš  Minor improvement: {improvement:.6f}\"\n",
    "            \n",
    "            best_val_loss = epoch_val_loss\n",
    "            best_model = model.state_dict()\n",
    "            print(f\"  {improvement_msg}\")\n",
    "        else:\n",
    "            early_stopping_counter += 1\n",
    "            print(f\"  âœ— No improvement (counter: {early_stopping_counter}/{early_stopping_patience})\")\n",
    "            \n",
    "        # Erken durdurma kontrolÃ¼\n",
    "        if early_stopping_counter >= early_stopping_patience:\n",
    "            print(f'\\nğŸ›‘ Erken durdurma: {early_stopping_patience} epoch boyunca yeterli iyileÅŸme yok.')\n",
    "            break\n",
    "    \n",
    "    # En iyi model aÄŸÄ±rlÄ±klarÄ±nÄ± yÃ¼kle\n",
    "    if best_model is not None:\n",
    "        model.load_state_dict(best_model)\n",
    "        print(f\"\\nâœ… En iyi model yÃ¼klendi (Val Loss: {best_val_loss:.6f})\")\n",
    "    \n",
    "    return model, train_losses, val_losses, train_accs, val_accs, learning_rates\n",
    "\n",
    "# Modeli geliÅŸmiÅŸ LR scheduling ile eÄŸit\n",
    "print(\"\\nğŸš€ Bidirectional LSTM model eÄŸitimi (GeliÅŸmiÅŸ LR Scheduling) baÅŸlÄ±yor...\")\n",
    "num_epochs = 50\n",
    "early_stopping_patience = 3  # Erken durdurma iÃ§in sabÄ±r sayÄ±sÄ±\n",
    "min_improvement_threshold = 0.01  # Ä°yileÅŸme eÅŸiÄŸi\n",
    "\n",
    "try:\n",
    "    model, train_losses, val_losses, train_accs, val_accs, learning_rates = train_model_with_advanced_lr(\n",
    "        model, train_loader, val_loader, criterion, optimizer, \n",
    "        warmup_scheduler, reduce_scheduler, cosine_scheduler,\n",
    "        num_epochs=num_epochs, \n",
    "        early_stopping_patience=early_stopping_patience,\n",
    "        min_improvement_threshold=min_improvement_threshold,\n",
    "        use_cosine_after_warmup=True  # Warmup sonrasÄ± cosine annealing kullan\n",
    "    )\n",
    "    print(\"\\nğŸ‰ Model eÄŸitimi baÅŸarÄ±yla tamamlandÄ±!\")\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nâ¹ï¸ EÄŸitim kullanÄ±cÄ± tarafÄ±ndan durduruldu.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f720d377",
   "metadata": {},
   "source": [
    "## Model DeÄŸerlendirmesi ve GÃ¶rselleÅŸtirme\n",
    "\n",
    "Bu bÃ¶lÃ¼mde eÄŸitilmiÅŸ modeli test veri seti Ã¼zerinde deÄŸerlendirip, sonuÃ§larÄ± gÃ¶rselleÅŸtireceÄŸiz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5361a1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EÄŸitim sonuÃ§larÄ±nÄ± gÃ¶rselleÅŸtirme\n",
    "def plot_training_history(train_losses, val_losses, train_accs, val_accs, learning_rates=None):\n",
    "    if learning_rates is not None:\n",
    "        plt.figure(figsize=(18, 6))\n",
    "        \n",
    "        # KayÄ±p grafiÄŸi\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.plot(train_losses, label='EÄŸitim', marker='o', alpha=0.7)\n",
    "        plt.plot(val_losses, label='DoÄŸrulama', marker='*', alpha=0.7)\n",
    "        plt.title('Model KaybÄ±')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('KayÄ±p (Cross-Entropy)')\n",
    "        plt.legend()\n",
    "        plt.grid(True, linestyle='--', alpha=0.6)\n",
    "        \n",
    "        # DoÄŸruluk grafiÄŸi\n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.plot(train_accs, label='EÄŸitim', marker='o', alpha=0.7)\n",
    "        plt.plot(val_accs, label='DoÄŸrulama', marker='*', alpha=0.7)\n",
    "        plt.title('Model DoÄŸruluÄŸu')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('DoÄŸruluk')\n",
    "        plt.legend()\n",
    "        plt.grid(True, linestyle='--', alpha=0.6)\n",
    "        \n",
    "        # Learning Rate grafiÄŸi\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.plot(learning_rates, marker='s', alpha=0.7, color='red')\n",
    "        plt.title('Learning Rate DeÄŸiÅŸimi')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Learning Rate')\n",
    "        plt.yscale('log')  # Log scale for better visualization\n",
    "        plt.grid(True, linestyle='--', alpha=0.6)\n",
    "    else:\n",
    "        plt.figure(figsize=(14, 5))\n",
    "        \n",
    "        # KayÄ±p grafiÄŸi\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(train_losses, label='EÄŸitim', marker='o')\n",
    "        plt.plot(val_losses, label='DoÄŸrulama', marker='*')\n",
    "        plt.title('Model KaybÄ±')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('KayÄ±p (Cross-Entropy)')\n",
    "        plt.legend()\n",
    "        plt.grid(True, linestyle='--', alpha=0.6)\n",
    "        \n",
    "        # DoÄŸruluk grafiÄŸi\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(train_accs, label='EÄŸitim', marker='o')\n",
    "        plt.plot(val_accs, label='DoÄŸrulama', marker='*')\n",
    "        plt.title('Model DoÄŸruluÄŸu')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('DoÄŸruluk')\n",
    "        plt.legend()\n",
    "        plt.grid(True, linestyle='--', alpha=0.6)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# EÄŸitim sonuÃ§larÄ±nÄ± gÃ¶rselleÅŸtir\n",
    "try:\n",
    "    if 'learning_rates' in locals():\n",
    "        plot_training_history(train_losses, val_losses, train_accs, val_accs, learning_rates)\n",
    "        \n",
    "        # DetaylÄ± learning rate analizi\n",
    "        print(f\"\\nğŸ“Š Learning Rate Ä°statistikleri:\")\n",
    "        print(f\"BaÅŸlangÄ±Ã§ LR: {learning_rates[0]:.2e}\")\n",
    "        print(f\"Maksimum LR: {max(learning_rates):.2e}\")\n",
    "        print(f\"Son LR: {learning_rates[-1]:.2e}\")\n",
    "        print(f\"Ortalama LR: {np.mean(learning_rates):.2e}\")\n",
    "    else:\n",
    "        plot_training_history(train_losses, val_losses, train_accs, val_accs)\n",
    "except NameError:\n",
    "    print(\"EÄŸitim geÃ§miÅŸi bulunamadÄ±. Ã–nce modeli eÄŸitin.\")\n",
    "\n",
    "# Test veri seti Ã¼zerinde deÄŸerlendirme\n",
    "def evaluate_model(model, test_loader, device):\n",
    "    model.eval()\n",
    "    \n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            \n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(predicted.cpu().numpy())\n",
    "    \n",
    "    # DoÄŸruluk hesapla\n",
    "    accuracy = np.mean(np.array(y_true) == np.array(y_pred))\n",
    "    \n",
    "    # SonuÃ§larÄ± yazdÄ±r\n",
    "    print(f\"\\nğŸ¯ Test DoÄŸruluÄŸu: {accuracy:.4f}\")\n",
    "    \n",
    "    # SÄ±nÄ±flandÄ±rma raporu\n",
    "    print(\"\\nğŸ“‹ SÄ±nÄ±flandÄ±rma Raporu:\")\n",
    "    print(classification_report(y_true, y_pred, target_names=le.classes_))\n",
    "    \n",
    "    # KarmaÅŸÄ±klÄ±k matrisi\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=le.classes_, yticklabels=le.classes_)\n",
    "    plt.title('KarmaÅŸÄ±klÄ±k Matrisi')\n",
    "    plt.xlabel('Tahmin Edilen Etiketler')\n",
    "    plt.ylabel('GerÃ§ek Etiketler')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return y_true, y_pred\n",
    "\n",
    "# Test veri seti Ã¼zerinde deÄŸerlendir\n",
    "try:\n",
    "    y_true, y_pred = evaluate_model(model, test_loader, device)\n",
    "except NameError:\n",
    "    print(\"Model bulunamadÄ±. Ã–nce modeli eÄŸitin.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pydebian (3.11.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
