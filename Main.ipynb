{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bfa6a60",
   "metadata": {},
   "source": [
    "# MÃ¼zik TÃ¼rÃ¼ SÄ±nÄ±flandÄ±rma Projesi\n",
    "\n",
    "Bu notebook, FMA (Free Music Archive) veri setini kullanarak mÃ¼zik tÃ¼rÃ¼ sÄ±nÄ±flandÄ±rma modeli geliÅŸtirmek iÃ§in veri hazÄ±rlama ve dengeleme iÅŸlemlerini iÃ§ermektedir.\n",
    "\n",
    "## Gerekli KÃ¼tÃ¼phanelerin Ä°Ã§e AktarÄ±lmasÄ±\n",
    "AÅŸaÄŸÄ±daki hÃ¼crede, projede kullanÄ±lacak temel Python kÃ¼tÃ¼phaneleri import edilmektedir:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2a41ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from collections import Counter\n",
    "\n",
    "# Machine learning libraries\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from imblearn.over_sampling import RandomOverSampler, BorderlineSMOTE\n",
    "\n",
    "# PyTorch libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Configuration\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "sns.set(style='whitegrid')\n",
    "print(\"ğŸ“š All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4206449",
   "metadata": {},
   "source": [
    "## YardÄ±mcÄ± Fonksiyonlar\n",
    "\n",
    "### SÄ±nÄ±f DaÄŸÄ±lÄ±mÄ± GÃ¶rselleÅŸtirme Fonksiyonu\n",
    "AÅŸaÄŸÄ±daki fonksiyon, veri setindeki sÄ±nÄ±f daÄŸÄ±lÄ±mlarÄ±nÄ± gÃ¶rselleÅŸtirmek iÃ§in kullanÄ±lacaktÄ±r. Bu gÃ¶rselleÅŸtirme, veri dengesizliÄŸini anlamamÄ±za yardÄ±mcÄ± olur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ae2a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_class_distribution(y, class_names, title):\n",
    "    \"\"\"Plot class distribution with improved visualization\"\"\"\n",
    "    counts = pd.Series(y).value_counts().sort_index()\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    bars = plt.bar(range(len(counts)), counts.values, color='skyblue', alpha=0.7)\n",
    "    plt.title(title, fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Music Genre')\n",
    "    plt.ylabel('Number of Samples')\n",
    "    plt.xticks(range(len(counts)), [class_names[i] for i in counts.index], rotation=45, ha='right')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, count in zip(bars, counts.values):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, \n",
    "                str(count), ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"Total samples: {sum(counts.values):,}\")\n",
    "    print(f\"Number of classes: {len(counts)}\")\n",
    "    print(f\"Min samples per class: {min(counts.values)}\")\n",
    "    print(f\"Max samples per class: {max(counts.values)}\")\n",
    "    print(f\"Average samples per class: {np.mean(counts.values):.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471baca3",
   "metadata": {},
   "source": [
    "## Veri YÃ¼kleme ve Ã–n Ä°ÅŸleme\n",
    "\n",
    "Bu bÃ¶lÃ¼mdeki fonksiyon:\n",
    "- FMA metadata dosyalarÄ±nÄ± yÃ¼kler\n",
    "- Gerekli sÃ¼tunlarÄ± seÃ§er\n",
    "- Eksik verileri temizler\n",
    "- Etiketleri kodlar\n",
    "- Veriyi sayÄ±sal formata dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e25b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_fma_data(data_dir='fma_metadata'):\n",
    "    \"\"\"Load and preprocess FMA dataset\"\"\"\n",
    "    tracks_path = os.path.join(data_dir, 'tracks.csv')\n",
    "    features_path = os.path.join(data_dir, 'features.csv')\n",
    "    \n",
    "    # Check if files exist\n",
    "    if not os.path.exists(tracks_path) or not os.path.exists(features_path):\n",
    "        raise FileNotFoundError(f\"Required files not found in {data_dir}/\")\n",
    "    \n",
    "    print(\"ğŸ“‚ Loading FMA metadata files...\")\n",
    "    tracks = pd.read_csv(tracks_path, index_col=0, header=[0,1])\n",
    "    features = pd.read_csv(features_path, index_col=0, header=[0,1])\n",
    "    \n",
    "    print(f\"âœ… Loaded tracks: {tracks.shape}, features: {features.shape}\")\n",
    "    \n",
    "    # Remove statistics columns (non-audio features)\n",
    "    audio_features = features.loc[:, features.columns.get_level_values(0) != 'statistics']\n",
    "    print(f\"ğŸ“Š Audio features after removing statistics: {audio_features.shape[1]} features\")\n",
    "    \n",
    "    # Get genre labels\n",
    "    genre_series = tracks[('track', 'genre_top')].dropna()\n",
    "    common_index = audio_features.index.intersection(genre_series.index)\n",
    "    \n",
    "    # Extract final data\n",
    "    X = audio_features.loc[common_index]\n",
    "    y_labels = genre_series.loc[common_index]\n",
    "    \n",
    "    # Clean and preprocess\n",
    "    X = X.fillna(0).replace([np.inf, -np.inf], 0).astype(np.float32)\n",
    "    X.index = X.index.astype(str)\n",
    "    \n",
    "    # Encode labels\n",
    "    label_encoder = LabelEncoder()\n",
    "    y = label_encoder.fit_transform(y_labels)\n",
    "    \n",
    "    print(f\"\\nğŸ¯ Final dataset:\")\n",
    "    print(f\"   Features: {X.shape[1]}\")\n",
    "    print(f\"   Samples: {X.shape[0]:,}\")\n",
    "    print(f\"   Classes: {len(label_encoder.classes_)}\")\n",
    "    print(f\"   Genres: {', '.join(label_encoder.classes_)}\")\n",
    "    \n",
    "    return X, y, label_encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c705ac60",
   "metadata": {},
   "source": [
    "## BaÅŸlangÄ±Ã§ Veri Analizi\n",
    "\n",
    "Verinin ilk yÃ¼klemesini yapÄ±p, baÅŸlangÄ±Ã§taki sÄ±nÄ±f daÄŸÄ±lÄ±mÄ±nÄ± inceleyelim. Bu analiz, veri dengesizliÄŸi problemini gÃ¶rselleÅŸtirmemize yardÄ±mcÄ± olacak."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97f61d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and analyze the dataset\n",
    "X, y, label_encoder = load_fma_data()\n",
    "\n",
    "# Visualize initial class distribution\n",
    "plot_class_distribution(y, label_encoder.classes_, 'Initial Class Distribution')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b6b7b9",
   "metadata": {},
   "source": [
    "## Veri BÃ¶lme ve EÄŸitim Seti Analizi\n",
    "\n",
    "Veriyi eÄŸitim ve test setlerine ayÄ±rÄ±p, eÄŸitim setindeki sÄ±nÄ±f daÄŸÄ±lÄ±mÄ±nÄ± inceliyoruz. Stratified split kullanarak orijinal daÄŸÄ±lÄ±mÄ± koruyoruz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ef4617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"ğŸ“Š Data split completed:\")\n",
    "print(f\"   Training set: {X_train.shape}\")\n",
    "print(f\"   Test set: {X_test.shape}\")\n",
    "\n",
    "# Visualize training set distribution\n",
    "plot_class_distribution(y_train, label_encoder.classes_, 'Training Set Distribution')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48a9420",
   "metadata": {},
   "source": [
    "## Veri Dengeleme - AÅŸama 1\n",
    "\n",
    "Ä°lk aÅŸamada, Ã§ok az Ã¶rneÄŸe sahip sÄ±nÄ±flar iÃ§in RandomOverSampler kullanÄ±lÄ±yor. Bu aÅŸama, BorderlineSMOTE iÃ§in yeterli Ã¶rnek sayÄ±sÄ±na ulaÅŸmamÄ±zÄ± saÄŸlar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d82e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_dataset(X_train, y_train, label_encoder):\n",
    "    \"\"\"Balance the dataset using two-phase approach\"\"\"\n",
    "    print(\"âš–ï¸ Phase 1: Random Oversampling for minority classes...\")\n",
    "    \n",
    "    # Phase 1: Random oversampling for very small classes\n",
    "    min_threshold = 60\n",
    "    class_counts = Counter(y_train)\n",
    "    small_classes = {cls: min_threshold for cls, count in class_counts.items() if count < min_threshold}\n",
    "    \n",
    "    if small_classes:\n",
    "        ros = RandomOverSampler(sampling_strategy=small_classes, random_state=42)\n",
    "        X_partial, y_partial = ros.fit_resample(X_train, y_train)\n",
    "        print(f\"   âœ… Oversampled {len(small_classes)} minority classes\")\n",
    "    else:\n",
    "        X_partial, y_partial = X_train, y_train\n",
    "        print(f\"   â„¹ï¸ No minority classes found below threshold {min_threshold}\")\n",
    "    \n",
    "    print(\"\\nâš–ï¸ Phase 2: BorderlineSMOTE for remaining imbalance...\")\n",
    "    try:\n",
    "        smote = BorderlineSMOTE(random_state=42)\n",
    "        X_balanced, y_balanced = smote.fit_resample(X_partial, y_partial)\n",
    "        print(f\"   âœ… SMOTE completed successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"   âš ï¸ SMOTE failed: {e}. Using partial balanced data.\")\n",
    "        X_balanced, y_balanced = X_partial, y_partial\n",
    "    \n",
    "    print(f\"\\nğŸ“ˆ Balancing results: {X_train.shape} â†’ {X_balanced.shape}\")\n",
    "    return X_balanced, y_balanced\n",
    "\n",
    "# Apply balancing\n",
    "X_balanced, y_balanced = balance_dataset(X_train, y_train, label_encoder)\n",
    "\n",
    "# Visualize balanced distribution\n",
    "plot_class_distribution(y_balanced, label_encoder.classes_, 'Balanced Training Set')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291b507b",
   "metadata": {},
   "source": [
    "## GeliÅŸmiÅŸ Ã–zellik SeÃ§imi (LSTM Ä°Ã§in Optimize)\n",
    "\n",
    "Model performansÄ±nÄ± artÄ±rmak ve LSTM iÃ§in optimize edilmiÅŸ Ã¶zellik seÃ§imi uygulayacaÄŸÄ±z. Mutual Information ile temporal dependencies yakalarken, correlation filtering ile multicollinearity'yi azaltacaÄŸÄ±z."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c6196b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GeliÅŸmiÅŸ Ã–zellik SeÃ§imi - LSTM iÃ§in Optimize\n",
    "from sklearn.feature_selection import mutual_info_classif, RFE, SelectKBest, f_classif\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def select_features(X, y, n_features=120):\n",
    "    \"\"\"Select best features using SelectKBest with F-test\"\"\"\n",
    "    print(f\"ğŸ” Selecting top {n_features} features from {X.shape[1]} total features...\")\n",
    "    \n",
    "    # Convert to numpy if needed\n",
    "    if hasattr(X, 'values'):\n",
    "        X_array = X.values\n",
    "    else:\n",
    "        X_array = X\n",
    "    \n",
    "    # Apply SelectKBest\n",
    "    selector = SelectKBest(score_func=f_classif, k=min(n_features, X_array.shape[1]))\n",
    "    X_selected = selector.fit_transform(X_array, y)\n",
    "    \n",
    "    # Get feature information\n",
    "    selected_mask = selector.get_support()\n",
    "    selected_indices = np.where(selected_mask)[0]\n",
    "    feature_scores = selector.scores_[selected_mask]\n",
    "    \n",
    "    print(f\"âœ… Selected {X_selected.shape[1]} features\")\n",
    "    print(f\"ğŸ“Š Score range: {feature_scores.min():.2f} - {feature_scores.max():.2f}\")\n",
    "    print(f\"ğŸ“Š Average score: {feature_scores.mean():.2f}\")\n",
    "    \n",
    "    return X_selected, selected_indices\n",
    "\n",
    "# Apply feature selection\n",
    "X_train_selected, selected_indices = select_features(X_balanced, y_balanced)\n",
    "X_test_selected = X_test.iloc[:, selected_indices].values if hasattr(X_test, 'iloc') else X_test[:, selected_indices]\n",
    "\n",
    "print(f\"\\nğŸ¯ Feature selection completed:\")\n",
    "print(f\"   Training features: {X_train_selected.shape}\")\n",
    "print(f\"   Test features: {X_test_selected.shape}\")\n",
    "\n",
    "# Skip the complex feature selection and use the simple approach above\n",
    "# Original complex code follows but is commented out for reference:\n",
    "\"\"\"\n",
    "print('\\nLSTM iÃ§in optimize edilmiÅŸ geliÅŸmiÅŸ Ã¶zellik seÃ§imi uygulanÄ±yor...')\n",
    "\n",
    "# Toplam seÃ§ilecek Ã¶zellik sayÄ±sÄ±\n",
    "total_k = 160  # 4 kategoriye bÃ¶lÃ¼necek (MFCC, Chroma, Spectral, Others)\n",
    "k_per_category = total_k // 4  # Her kategori iÃ§in eÅŸit sayÄ±da Ã¶zellik\n",
    "\n",
    "print(f\"Toplam Ã¶zellik sayÄ±sÄ±: {X_balanced.shape[1]}, SeÃ§ilecek Ã¶zellik sayÄ±sÄ±: {total_k}\")\n",
    "print(f\"Her Ã¶zellik kategorisinden seÃ§ilecek: {k_per_category}\")\n",
    "\n",
    "# Ã–zellik isimlerini oluÅŸtur\n",
    "if hasattr(X_balanced, 'columns'):\n",
    "    if isinstance(X_res.columns, pd.MultiIndex):\n",
    "        feature_names = [f\"{col[0]}_{col[1]}\" if len(col) > 1 else str(col[0]) for col in X_res.columns]\n",
    "    else:\n",
    "        feature_names = [str(col) for col in X_res.columns]\n",
    "else:\n",
    "    feature_names = [f'feature_{i}' for i in range(X_res.shape[1])]\n",
    "\n",
    "print(f\"Ã–zellik isimleri oluÅŸturuldu: {len(feature_names)} adet\")\n",
    "\n",
    "# X_res'i numpy array'e dÃ¶nÃ¼ÅŸtÃ¼r\n",
    "X_res_array = X_res.values\n",
    "X_test_array = X_test.values\n",
    "\n",
    "# HÄ±zlÄ± ve Optimize EdilmiÅŸ Ã–zellik SeÃ§imi (LSTM iÃ§in)\n",
    "print(\"\\nâš¡ HÄ±zlÄ± ve Optimize EdilmiÅŸ Ã¶zellik seÃ§imi uygulanÄ±yor...\")\n",
    "print(\"   âœ… SelectKBest + Basit korelasyon filtreleme\")\n",
    "print(\"   âœ… Ã‡ok daha hÄ±zlÄ± ve etkili\")\n",
    "print(\"   âœ… LSTM iÃ§in yeterince iyi performans\")\n",
    "\n",
    "# Ã–zellik kategorilerini bul\n",
    "mfcc_indices = [i for i, name in enumerate(feature_names) if 'mfcc' in name.lower()]\n",
    "chroma_indices = [i for i, name in enumerate(feature_names) if 'chroma' in name.lower()]\n",
    "spectral_indices = [i for i, name in enumerate(feature_names) if any(spec in name.lower() for spec in ['spectral', 'centroid', 'bandwidth', 'contrast', 'rolloff'])]\n",
    "other_indices = [i for i in range(len(feature_names)) \n",
    "                if i not in mfcc_indices and i not in chroma_indices and i not in spectral_indices]\n",
    "\n",
    "print(f\"\\nÃ–zellik kategori sayÄ±larÄ±:\")\n",
    "print(f\"- MFCC: {len(mfcc_indices)} Ã¶zellik\")\n",
    "print(f\"- Chroma: {len(chroma_indices)} Ã¶zellik\")\n",
    "print(f\"- Spectral: {len(spectral_indices)} Ã¶zellik\")\n",
    "print(f\"- DiÄŸer: {len(other_indices)} Ã¶zellik\")\n",
    "\n",
    "# 1. AdÄ±m: Her kategoriden SelectKBest ile hÄ±zlÄ± seÃ§im\n",
    "selected_indices = []\n",
    "category_info = []\n",
    "\n",
    "for category_name, indices in [('MFCC', mfcc_indices), ('Chroma', chroma_indices), \n",
    "                              ('Spectral', spectral_indices), ('Others', other_indices)]:\n",
    "    if len(indices) > 0:\n",
    "        X_category = X_res_array[:, indices]\n",
    "        \n",
    "        # SelectKBest ile hÄ±zlÄ± seÃ§im (F-test)\n",
    "        k_category = min(k_per_category, len(indices))\n",
    "        selector = SelectKBest(score_func=f_classif, k=k_category)\n",
    "        selector.fit(X_category, y_res)\n",
    "        \n",
    "        # SeÃ§ilen Ã¶zelliklerin orijinal indekslerini al\n",
    "        selected_mask = selector.get_support()\n",
    "        selected_category_indices = [indices[i] for i in range(len(indices)) if selected_mask[i]]\n",
    "        selected_indices.extend(selected_category_indices)\n",
    "        \n",
    "        category_info.append({\n",
    "            'name': category_name,\n",
    "            'total': len(indices),\n",
    "            'selected': len(selected_category_indices),\n",
    "            'avg_score': np.mean(selector.scores_[selected_mask])\n",
    "        })\n",
    "        \n",
    "        print(f\"   {category_name}: {len(selected_category_indices)} Ã¶zellik seÃ§ildi, Ortalama F-score: {np.mean(selector.scores_[selected_mask]):.2f}\")\n",
    "\n",
    "# 2. AdÄ±m: Basit korelasyon filtreleme (Ã§ok daha hÄ±zlÄ±)\n",
    "print(f\"\\nğŸ” Basit korelasyon filtreleme uygulanÄ±yor...\")\n",
    "final_indices = selected_indices.copy()\n",
    "\n",
    "if len(final_indices) > 50:  # Sadece Ã§ok fazla Ã¶zellik varsa korelasyon filtrele\n",
    "    print(\"   Korelasyon matrisi hesaplanÄ±yor...\")\n",
    "    corr_matrix = np.corrcoef(X_res_array[:, final_indices].T)\n",
    "    \n",
    "    # Basit korelasyon filtreleme (ilk bulduÄŸunu Ã§Ä±kar)\n",
    "    to_remove = set()\n",
    "    for i in range(len(corr_matrix)):\n",
    "        if i in to_remove:\n",
    "            continue\n",
    "        for j in range(i+1, len(corr_matrix)):\n",
    "            if j not in to_remove and abs(corr_matrix[i, j]) > 0.95:  # Ã‡ok yÃ¼ksek korelasyon\n",
    "                to_remove.add(j)  # j'yi Ã§Ä±kar (basit kural)\n",
    "                if len(to_remove) > 10:  # Maksimum 10 Ã¶zellik Ã§Ä±kar\n",
    "                    break\n",
    "        if len(to_remove) > 10:\n",
    "            break\n",
    "    \n",
    "    # Ã‡Ä±karÄ±lacak indeksleri kaldÄ±r\n",
    "    final_indices = [final_indices[i] for i in range(len(final_indices)) if i not in to_remove]\n",
    "    \n",
    "    print(f\"   {len(to_remove)} yÃ¼ksek korelasyonlu Ã¶zellik Ã§Ä±karÄ±ldÄ±\")\n",
    "    print(f\"   Final Ã¶zellik sayÄ±sÄ±: {len(final_indices)}\")\n",
    "else:\n",
    "    print(\"   Ã–zellik sayÄ±sÄ± az, korelasyon filtreleme atlanÄ±yor\")\n",
    "\n",
    "hybrid_indices = final_indices\n",
    "\n",
    "# SeÃ§ilen Ã¶zellikleri uygula\n",
    "X_res_selected = X_res_array[:, hybrid_indices]\n",
    "X_test_selected = X_test_array[:, hybrid_indices]\n",
    "selected_feature_names = [feature_names[i] for i in hybrid_indices]\n",
    "\n",
    "print(f\"\\nâœ… HÄ±zlÄ± Ã–zellik SeÃ§imi tamamlandÄ±!\")\n",
    "print(f\"ğŸ“Š Final Ã¶zellik sayÄ±sÄ±: {len(hybrid_indices)}\")\n",
    "print(f\"ğŸ“‹ SeÃ§ilen Ã¶zelliklerin boyutu: {X_res_selected.shape}\")\n",
    "\n",
    "# Final kategori daÄŸÄ±lÄ±mÄ±nÄ± kontrol et\n",
    "final_mfcc = len([name for name in selected_feature_names if 'mfcc' in name.lower()])\n",
    "final_chroma = len([name for name in selected_feature_names if 'chroma' in name.lower()])\n",
    "final_spectral = len([name for name in selected_feature_names if any(spec in name.lower() for spec in ['spectral', 'centroid', 'bandwidth', 'contrast', 'rolloff'])])\n",
    "final_others = len(selected_feature_names) - final_mfcc - final_chroma - final_spectral\n",
    "\n",
    "print(f\"\\nğŸ“ˆ Final Kategori DaÄŸÄ±lÄ±mÄ±:\")\n",
    "print(f\"   - MFCC: {final_mfcc} Ã¶zellik\")\n",
    "print(f\"   - Chroma: {final_chroma} Ã¶zellik\")\n",
    "print(f\"   - Spectral: {final_spectral} Ã¶zellik\")\n",
    "print(f\"   - Others: {final_others} Ã¶zellik\")\n",
    "\n",
    "# Global deÄŸiÅŸkenleri gÃ¼ncelle\n",
    "X_res = X_res_selected\n",
    "X_test = X_test_selected\n",
    "feature_names = selected_feature_names\n",
    "\n",
    "print(f\"\\nğŸ¯ HÄ±zlÄ± Ã¶zellik seÃ§imi tamamlandÄ±!\")\n",
    "print(f\"âš¡ Veri ÅŸekli gÃ¼ncellendi: X_res {X_res.shape}, X_test {X_test.shape}\")\n",
    "print(f\"âœ¨ SelectKBest + Basit korelasyon filtreleme kullanÄ±ldÄ±!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69962f4f",
   "metadata": {},
   "source": [
    "*-----------------------------------------------------------------------------------*\n",
    "# PyTorch LSTM MODEL EÄÄ°TÄ°MÄ°\n",
    "*-----------------------------------------------------------------------------------*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4bc7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_training_data(X_train, X_test, y_train, y_test, test_size=0.1):\n",
    "    \"\"\"Prepare and scale data for training\"\"\"\n",
    "    # Split training data into train/validation\n",
    "    X_train_final, X_val, y_train_final, y_val = train_test_split(\n",
    "        X_train, y_train, test_size=test_size, stratify=y_train, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train_final)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    print(f\"ğŸ“Š Data preparation completed:\")\n",
    "    print(f\"   Training: {X_train_scaled.shape}\")\n",
    "    print(f\"   Validation: {X_val_scaled.shape}\")\n",
    "    print(f\"   Test: {X_test_scaled.shape}\")\n",
    "    \n",
    "    return X_train_scaled, X_val_scaled, X_test_scaled, y_train_final, y_val, scaler\n",
    "\n",
    "# Prepare data\n",
    "X_train_scaled, X_val_scaled, X_test_scaled, y_train_final, y_val, scaler = prepare_training_data(\n",
    "    X_train_selected, X_test_selected, y_balanced, y_test\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa7d165",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(X, y, sequence_length=10):\n",
    "    \"\"\"Create sequence data for LSTM from features\"\"\"\n",
    "    n_samples, n_features = X.shape\n",
    "    features_per_step = max(1, n_features // sequence_length)\n",
    "    \n",
    "    # Adjust sequence length if needed\n",
    "    actual_seq_length = min(sequence_length, n_features)\n",
    "    \n",
    "    # Create sequences\n",
    "    X_seq = np.zeros((n_samples, actual_seq_length, features_per_step))\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        for t in range(actual_seq_length):\n",
    "            start_idx = t * features_per_step\n",
    "            end_idx = min(start_idx + features_per_step, n_features)\n",
    "            if start_idx < n_features:\n",
    "                X_seq[i, t, :end_idx-start_idx] = X[i, start_idx:end_idx]\n",
    "    \n",
    "    return torch.FloatTensor(X_seq), torch.LongTensor(y)\n",
    "\n",
    "def create_data_loaders(X_train, X_val, X_test, y_train, y_val, y_test, batch_size=128):\n",
    "    \"\"\"Create PyTorch data loaders\"\"\"\n",
    "    # Create sequences\n",
    "    X_train_seq, y_train_tensor = create_sequences(X_train, y_train)\n",
    "    X_val_seq, y_val_tensor = create_sequences(X_val, y_val)\n",
    "    X_test_seq, y_test_tensor = create_sequences(X_test, y_test)\n",
    "    \n",
    "    print(f\"ğŸ”„ Sequence shapes:\")\n",
    "    print(f\"   Train: {X_train_seq.shape}\")\n",
    "    print(f\"   Validation: {X_val_seq.shape}\")\n",
    "    print(f\"   Test: {X_test_seq.shape}\")\n",
    "    \n",
    "    # Create datasets and loaders\n",
    "    train_dataset = TensorDataset(X_train_seq, y_train_tensor)\n",
    "    val_dataset = TensorDataset(X_val_seq, y_val_tensor)\n",
    "    test_dataset = TensorDataset(X_test_seq, y_test_tensor)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return train_loader, val_loader, test_loader, X_train_seq.shape\n",
    "\n",
    "# Create data loaders\n",
    "train_loader, val_loader, test_loader, input_shape = create_data_loaders(\n",
    "    X_train_scaled, X_val_scaled, X_test_scaled, y_train_final, y_val, y_test\n",
    ")\n",
    "\n",
    "def train_model(model, train_loader, val_loader, device, num_epochs=20, lr=0.001):\n",
    "    \"\"\"Train the LSTM model with simplified training loop\"\"\"\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3, factor=0.5)\n",
    "    \n",
    "    train_losses, val_losses = [], []\n",
    "    train_accs, val_accs = [], []\n",
    "    best_val_acc = 0\n",
    "    patience_counter = 0\n",
    "    \n",
    "    print(f\"ğŸš€ Starting training for {num_epochs} epochs...\")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss, train_correct, train_total = 0, 0, 0\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            train_total += labels.size(0)\n",
    "            train_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss, val_correct, val_total = 0, 0, 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        # Calculate metrics\n",
    "        train_loss /= len(train_loader)\n",
    "        val_loss /= len(val_loader)\n",
    "        train_acc = train_correct / train_total\n",
    "        val_acc = val_correct / val_total\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        val_accs.append(val_acc)\n",
    "        \n",
    "        # Learning rate scheduling\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # Early stopping check\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        print(f\"Epoch {epoch+1:2d}/{num_epochs} | \"\n",
    "              f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | \"\n",
    "              f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "        \n",
    "        # Early stopping\n",
    "        if patience_counter >= 5:\n",
    "            print(f\"\\nğŸ›‘ Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "    \n",
    "    return train_losses, val_losses, train_accs, val_accs\n",
    "\n",
    "def evaluate_model(model, test_loader, device, label_encoder):\n",
    "    \"\"\"Evaluate model on test set\"\"\"\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            \n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(predicted.cpu().numpy())\n",
    "    \n",
    "    accuracy = np.mean(np.array(y_true) == np.array(y_pred))\n",
    "    \n",
    "    print(f\"\\nğŸ¯ Test Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"\\nğŸ“‹ Classification Report:\")\n",
    "    print(classification_report(y_true, y_pred, target_names=label_encoder.classes_))\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=label_encoder.classes_, \n",
    "                yticklabels=label_encoder.classes_)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return accuracy, y_true, y_pred\n",
    "\n",
    "def plot_training_history(train_losses, val_losses, train_accs, val_accs):\n",
    "    \"\"\"Plot training history\"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    # Loss plot\n",
    "    ax1.plot(train_losses, label='Training', marker='o')\n",
    "    ax1.plot(val_losses, label='Validation', marker='s')\n",
    "    ax1.set_title('Model Loss')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Accuracy plot\n",
    "    ax2.plot(train_accs, label='Training', marker='o')\n",
    "    ax2.plot(val_accs, label='Validation', marker='s')\n",
    "    ax2.set_title('Model Accuracy')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Train the model\n",
    "print(\"\\nğŸµ Training LSTM model...\")\n",
    "train_losses, val_losses, train_accs, val_accs = train_model(\n",
    "    model, train_loader, val_loader, device, num_epochs=30, lr=0.001\n",
    ")\n",
    "\n",
    "# Plot training history\n",
    "plot_training_history(train_losses, val_losses, train_accs, val_accs)\n",
    "\n",
    "# Evaluate on test set\n",
    "test_accuracy, y_true, y_pred = evaluate_model(model, test_loader, device, label_encoder)\n",
    "\n",
    "print(f\"\\nâœ… Training completed! Final test accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d73d18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention mekanizmasÄ± sÄ±nÄ±fÄ±\n",
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Attention aÄŸÄ±rlÄ±klarÄ±nÄ± hesaplamak iÃ§in linear katmanlar\n",
    "        self.attention_linear = nn.Linear(hidden_size, hidden_size)\n",
    "        self.context_vector = nn.Linear(hidden_size, 1, bias=False)\n",
    "        \n",
    "    def forward(self, lstm_outputs):\n",
    "        # lstm_outputs ÅŸekli: (batch_size, sequence_length, hidden_size)\n",
    "        \n",
    "        # Her zaman adÄ±mÄ± iÃ§in attention skorlarÄ± hesapla\n",
    "        attention_weights = self.attention_linear(lstm_outputs)  # (batch_size, seq_len, hidden_size)\n",
    "        attention_weights = torch.tanh(attention_weights)\n",
    "        attention_scores = self.context_vector(attention_weights)  # (batch_size, seq_len, 1)\n",
    "        attention_scores = attention_scores.squeeze(2)  # (batch_size, seq_len)\n",
    "        \n",
    "        # Softmax ile normalize et\n",
    "        attention_weights = torch.softmax(attention_scores, dim=1)  # (batch_size, seq_len)\n",
    "        \n",
    "        # Weighted sum hesapla\n",
    "        # attention_weights: (batch_size, seq_len) -> (batch_size, seq_len, 1)\n",
    "        attention_weights = attention_weights.unsqueeze(2)\n",
    "        \n",
    "        # Weighted combination of LSTM outputs\n",
    "        attended_output = torch.sum(lstm_outputs * attention_weights, dim=1)  # (batch_size, hidden_size)\n",
    "        \n",
    "        return attended_output, attention_weights.squeeze(2)\n",
    "\n",
    "# Bidirectional LSTM model sÄ±nÄ±fÄ±nÄ± tanÄ±mlama (Attention ile)\n",
    "class MusicGenreLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout=0.3, bidirectional=True):\n",
    "        super(MusicGenreLSTM, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional = bidirectional\n",
    "        \n",
    "        # Bidirectional LSTM katmanlarÄ±\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size, \n",
    "            hidden_size=hidden_size, \n",
    "            num_layers=num_layers, \n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=bidirectional\n",
    "        )\n",
    "        \n",
    "        # Bidirectional LSTM Ã§Ä±kÄ±ÅŸ boyutunu hesapla\n",
    "        lstm_output_size = hidden_size * 2 if bidirectional else hidden_size\n",
    "        \n",
    "        # Attention katmanÄ± (bidirectional output iÃ§in)\n",
    "        self.attention = AttentionLayer(lstm_output_size)\n",
    "        \n",
    "        # Batch normalization\n",
    "        self.batch_norm = nn.BatchNorm1d(lstm_output_size)\n",
    "        \n",
    "        # Dropout katmanÄ±\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Tam baÄŸlantÄ±lÄ± katmanlar\n",
    "        self.fc1 = nn.Linear(lstm_output_size, 128)  # Ä°lk tam baÄŸlantÄ±lÄ± katman\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "        \n",
    "        # Aktivasyon fonksiyonlarÄ±\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Bidirectional LSTM katmanÄ±ndan geÃ§irme\n",
    "        # x ÅŸekli: (batch_size, sequence_length, input_size)\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        \n",
    "        # lstm_out ÅŸekli: (batch_size, sequence_length, hidden_size * 2) for bidirectional\n",
    "        # veya (batch_size, sequence_length, hidden_size) for unidirectional\n",
    "        \n",
    "        # Attention mekanizmasÄ± uygula\n",
    "        attended_output, attention_weights = self.attention(lstm_out)\n",
    "        \n",
    "        # Batch normalization\n",
    "        batch_norm_out = self.batch_norm(attended_output)\n",
    "        \n",
    "        # Ä°lk tam baÄŸlantÄ±lÄ± katman\n",
    "        fc1_out = self.fc1(batch_norm_out)\n",
    "        fc1_out = self.relu(fc1_out)\n",
    "        fc1_out = self.dropout(fc1_out)\n",
    "        \n",
    "        # Ä°kinci tam baÄŸlantÄ±lÄ± katman (Ã§Ä±kÄ±ÅŸ katmanÄ±)\n",
    "        out = self.fc2(fc1_out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "# Learning Rate Warmup Scheduler\n",
    "class WarmupScheduler:\n",
    "    def __init__(self, optimizer, warmup_epochs, max_lr, min_lr=1e-7):\n",
    "        self.optimizer = optimizer\n",
    "        self.warmup_epochs = warmup_epochs\n",
    "        self.max_lr = max_lr\n",
    "        self.min_lr = min_lr\n",
    "        self.current_epoch = 0\n",
    "        \n",
    "    def step(self):\n",
    "        if self.current_epoch < self.warmup_epochs:\n",
    "            # Linear warmup\n",
    "            lr = self.min_lr + (self.max_lr - self.min_lr) * (self.current_epoch / self.warmup_epochs)\n",
    "            for param_group in self.optimizer.param_groups:\n",
    "                param_group['lr'] = lr\n",
    "        self.current_epoch += 1\n",
    "        \n",
    "    def get_lr(self):\n",
    "        return self.optimizer.param_groups[0]['lr']\n",
    "\n",
    "# Create simplified LSTM model\n",
    "class SimpleLSTMClassifier(nn.Module):\n",
    "    \"\"\"Simplified LSTM model for music genre classification\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout=0.3):\n",
    "        super(SimpleLSTMClassifier, self).__init__()\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        \n",
    "        # Calculate LSTM output size (bidirectional doubles the hidden size)\n",
    "        lstm_output_size = hidden_size * 2\n",
    "        \n",
    "        # Classification layers\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(lstm_output_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # LSTM forward pass\n",
    "        lstm_out, (hidden, cell) = self.lstm(x)\n",
    "        \n",
    "        # Use the last output for classification\n",
    "        last_output = lstm_out[:, -1, :]\n",
    "        \n",
    "        # Classification\n",
    "        output = self.classifier(last_output)\n",
    "        return output\n",
    "\n",
    "def create_model(input_shape, num_classes, hidden_size=64, num_layers=2, dropout=0.3):\n",
    "    \"\"\"Create and initialize the LSTM model\"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    model = SimpleLSTMClassifier(\n",
    "        input_size=input_shape[2],  # features per timestep\n",
    "        hidden_size=hidden_size,\n",
    "        num_layers=num_layers,\n",
    "        num_classes=num_classes,\n",
    "        dropout=dropout\n",
    "    ).to(device)\n",
    "    \n",
    "    # Count parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    print(f\"ğŸ¤– Model created on {device}\")\n",
    "    print(f\"ğŸ“Š Parameters: {trainable_params:,} trainable / {total_params:,} total\")\n",
    "    \n",
    "    return model, device\n",
    "\n",
    "# Create model\n",
    "model, device = create_model(input_shape, len(label_encoder.classes_))\n",
    "print(f\"\\nğŸ—ï¸ Model architecture:\")\n",
    "print(model)\n",
    "\n",
    "def train_model(model, train_loader, val_loader, device, num_epochs=20, lr=0.001):\n",
    "    \"\"\"Train the LSTM model with simplified training loop\"\"\"\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3, factor=0.5)\n",
    "    \n",
    "    train_losses, val_losses = [], []\n",
    "    train_accs, val_accs = [], []\n",
    "    best_val_acc = 0\n",
    "    patience_counter = 0\n",
    "    \n",
    "    print(f\"ğŸš€ Starting training for {num_epochs} epochs...\")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss, train_correct, train_total = 0, 0, 0\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            train_total += labels.size(0)\n",
    "            train_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss, val_correct, val_total = 0, 0, 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        # Calculate metrics\n",
    "        train_loss /= len(train_loader)\n",
    "        val_loss /= len(val_loader)\n",
    "        train_acc = train_correct / train_total\n",
    "        val_acc = val_correct / val_total\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        val_accs.append(val_acc)\n",
    "        \n",
    "        # Learning rate scheduling\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # Early stopping check\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        print(f\"Epoch {epoch+1:2d}/{num_epochs} | \"\n",
    "              f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | \"\n",
    "              f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "        \n",
    "        # Early stopping\n",
    "        if patience_counter >= 5:\n",
    "            print(f\"\\nğŸ›‘ Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "    \n",
    "    return train_losses, val_losses, train_accs, val_accs\n",
    "\n",
    "def evaluate_model(model, test_loader, device, label_encoder):\n",
    "    \"\"\"Evaluate model on test set\"\"\"\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            \n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(predicted.cpu().numpy())\n",
    "    \n",
    "    accuracy = np.mean(np.array(y_true) == np.array(y_pred))\n",
    "    \n",
    "    print(f\"\\nğŸ¯ Test Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"\\nğŸ“‹ Classification Report:\")\n",
    "    print(classification_report(y_true, y_pred, target_names=label_encoder.classes_))\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=label_encoder.classes_, \n",
    "                yticklabels=label_encoder.classes_)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return accuracy, y_true, y_pred\n",
    "\n",
    "def plot_training_history(train_losses, val_losses, train_accs, val_accs):\n",
    "    \"\"\"Plot training history\"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    # Loss plot\n",
    "    ax1.plot(train_losses, label='Training', marker='o')\n",
    "    ax1.plot(val_losses, label='Validation', marker='s')\n",
    "    ax1.set_title('Model Loss')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Accuracy plot\n",
    "    ax2.plot(train_accs, label='Training', marker='o')\n",
    "    ax2.plot(val_accs, label='Validation', marker='s')\n",
    "    ax2.set_title('Model Accuracy')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Train the model\n",
    "print(\"\\nğŸµ Training LSTM model...\")\n",
    "train_losses, val_losses, train_accs, val_accs = train_model(\n",
    "    model, train_loader, val_loader, device, num_epochs=30, lr=0.001\n",
    ")\n",
    "\n",
    "# Plot training history\n",
    "plot_training_history(train_losses, val_losses, train_accs, val_accs)\n",
    "\n",
    "# Evaluate on test set\n",
    "test_accuracy, y_true, y_pred = evaluate_model(model, test_loader, device, label_encoder)\n",
    "\n",
    "print(f\"\\nâœ… Training completed! Final test accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5361a1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EÄŸitim sonuÃ§larÄ±nÄ± gÃ¶rselleÅŸtirme\n",
    "def plot_training_history(train_losses, val_losses, train_accs, val_accs, learning_rates=None):\n",
    "    if learning_rates is not None:\n",
    "        plt.figure(figsize=(18, 6))\n",
    "        \n",
    "        # KayÄ±p grafiÄŸi\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.plot(train_losses, label='EÄŸitim', marker='o', alpha=0.7)\n",
    "        plt.plot(val_losses, label='DoÄŸrulama', marker='*', alpha=0.7)\n",
    "        plt.title('Model KaybÄ±')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('KayÄ±p (Cross-Entropy)')\n",
    "        plt.legend()\n",
    "        plt.grid(True, linestyle='--', alpha=0.6)\n",
    "        \n",
    "        # DoÄŸruluk grafiÄŸi\n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.plot(train_accs, label='EÄŸitim', marker='o', alpha=0.7)\n",
    "        plt.plot(val_accs, label='DoÄŸrulama', marker='*', alpha=0.7)\n",
    "        plt.title('Model DoÄŸruluÄŸu')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('DoÄŸruluk')\n",
    "        plt.legend()\n",
    "        plt.grid(True, linestyle='--', alpha=0.6)\n",
    "        \n",
    "        # Learning Rate grafiÄŸi\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.plot(learning_rates, marker='s', alpha=0.7, color='red')\n",
    "        plt.title('Learning Rate DeÄŸiÅŸimi')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Learning Rate')\n",
    "        plt.yscale('log')  # Log scale for better visualization\n",
    "        plt.grid(True, linestyle='--', alpha=0.6)\n",
    "    else:\n",
    "        plt.figure(figsize=(14, 5))\n",
    "        \n",
    "        # KayÄ±p grafiÄŸi\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(train_losses, label='EÄŸitim', marker='o')\n",
    "        plt.plot(val_losses, label='DoÄŸrulama', marker='*')\n",
    "        plt.title('Model KaybÄ±')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('KayÄ±p (Cross-Entropy)')\n",
    "        plt.legend()\n",
    "        plt.grid(True, linestyle='--', alpha=0.6)\n",
    "        \n",
    "        # DoÄŸruluk grafiÄŸi\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(train_accs, label='EÄŸitim', marker='o')\n",
    "        plt.plot(val_accs, label='DoÄŸrulama', marker='*')\n",
    "        plt.title('Model DoÄŸruluÄŸu')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('DoÄŸruluk')\n",
    "        plt.legend()\n",
    "        plt.grid(True, linestyle='--', alpha=0.6)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# EÄŸitim sonuÃ§larÄ±nÄ± gÃ¶rselleÅŸtir\n",
    "try:\n",
    "    if 'learning_rates' in locals():\n",
    "        plot_training_history(train_losses, val_losses, train_accs, val_accs, learning_rates)\n",
    "        \n",
    "        # DetaylÄ± learning rate analizi\n",
    "        print(f\"\\nğŸ“Š Learning Rate Ä°statistikleri:\")\n",
    "        print(f\"BaÅŸlangÄ±Ã§ LR: {learning_rates[0]:.2e}\")\n",
    "        print(f\"Maksimum LR: {max(learning_rates):.2e}\")\n",
    "        print(f\"Son LR: {learning_rates[-1]:.2e}\")\n",
    "        print(f\"Ortalama LR: {np.mean(learning_rates):.2e}\")\n",
    "    else:\n",
    "        plot_training_history(train_losses, val_losses, train_accs, val_accs)\n",
    "except NameError:\n",
    "    print(\"EÄŸitim geÃ§miÅŸi bulunamadÄ±. Ã–nce modeli eÄŸitin.\")\n",
    "\n",
    "# Test veri seti Ã¼zerinde deÄŸerlendirme\n",
    "def evaluate_model(model, test_loader, device):\n",
    "    model.eval()\n",
    "    \n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            \n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(predicted.cpu().numpy())\n",
    "    \n",
    "    # DoÄŸruluk hesapla\n",
    "    accuracy = np.mean(np.array(y_true) == np.array(y_pred))\n",
    "    \n",
    "    # SonuÃ§larÄ± yazdÄ±r\n",
    "    print(f\"\\nğŸ¯ Test DoÄŸruluÄŸu: {accuracy:.4f}\")\n",
    "    \n",
    "    # SÄ±nÄ±flandÄ±rma raporu\n",
    "    print(\"\\nğŸ“‹ SÄ±nÄ±flandÄ±rma Raporu:\")\n",
    "    print(classification_report(y_true, y_pred, target_names=label_encoder.classes_))\n",
    "    \n",
    "    # KarmaÅŸÄ±klÄ±k matrisi\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
    "    plt.title('KarmaÅŸÄ±klÄ±k Matrisi')\n",
    "    plt.xlabel('Tahmin Edilen Etiketler')\n",
    "    plt.ylabel('GerÃ§ek Etiketler')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return y_true, y_pred\n",
    "\n",
    "# Test veri seti Ã¼zerinde deÄŸerlendir\n",
    "try:\n",
    "    y_true, y_pred = evaluate_model(model, test_loader, device)\n",
    "except NameError:\n",
    "    print(\"Model bulunamadÄ±. Ã–nce modeli eÄŸitin.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f7e392",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook provides a streamlined approach to music genre classification using the FMA dataset:\n",
    "\n",
    "1. **Data Loading**: Load and preprocess FMA tracks and features\n",
    "2. **Data Balancing**: Handle class imbalance using oversampling techniques\n",
    "3. **Feature Selection**: Select the most informative features for classification\n",
    "4. **Model Training**: Train a bidirectional LSTM with simplified architecture\n",
    "5. **Evaluation**: Assess model performance on test data\n",
    "\n",
    "The refactored code is more modular, maintainable, and easier to understand while maintaining the core functionality."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pydebian (3.11.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
