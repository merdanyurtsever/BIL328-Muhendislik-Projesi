{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bfa6a60",
   "metadata": {},
   "source": [
    "# Müzik Türü Sınıflandırma Projesi\n",
    "\n",
    "Bu notebook, FMA (Free Music Archive) veri setini kullanarak müzik türü sınıflandırma modeli geliştirmek için veri hazırlama ve dengeleme işlemlerini içermektedir.\n",
    "\n",
    "## Gerekli Kütüphanelerin İçe Aktarılması\n",
    "Aşağıdaki hücrede, projede kullanılacak temel Python kütüphaneleri import edilmektedir:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2a41ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from imblearn.over_sampling import RandomOverSampler, BorderlineSMOTE\n",
    "from collections import Counter\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set(style='whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4206449",
   "metadata": {},
   "source": [
    "## Yardımcı Fonksiyonlar\n",
    "\n",
    "### Sınıf Dağılımı Görselleştirme Fonksiyonu\n",
    "Aşağıdaki fonksiyon, veri setindeki sınıf dağılımlarını görselleştirmek için kullanılacaktır. Bu görselleştirme, veri dengesizliğini anlamamıza yardımcı olur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ae2a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_class_distribution(y, labels, title):\n",
    "    counts = pd.Series(y).value_counts().sort_index()\n",
    "    valid_indices = counts.index[counts.index < len(labels)]\n",
    "    counts = counts.loc[valid_indices]\n",
    "    names = labels[counts.index]\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    ax = sns.barplot(x=names, y=counts.values, hue=names, palette='viridis', legend=False)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('Sınıf')\n",
    "    ax.set_ylabel('Sayı')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471baca3",
   "metadata": {},
   "source": [
    "## Veri Yükleme ve Ön İşleme\n",
    "\n",
    "Bu bölümdeki fonksiyon:\n",
    "- FMA metadata dosyalarını yükler\n",
    "- Gerekli sütunları seçer\n",
    "- Eksik verileri temizler\n",
    "- Etiketleri kodlar\n",
    "- Veriyi sayısal formata dönüştürür"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e25b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    tracks_path = 'fma_metadata/tracks.csv'\n",
    "    features_path = 'fma_metadata/features.csv'\n",
    "\n",
    "    if not os.path.exists(tracks_path) or not os.path.exists(features_path):\n",
    "        raise FileNotFoundError(f\"Gerekli veri dosyaları bulunamadı. '{tracks_path}' ve '{features_path}' dosyalarının mevcut olduğundan emin olun.\")\n",
    "\n",
    "    tracks = pd.read_csv(tracks_path, index_col=0, header=[0,1])\n",
    "    features = pd.read_csv(features_path, index_col=0, header=[0,1])  # Çok seviyeli başlıkla oku\n",
    "    \n",
    "    print(\"Kullanılabilir özellik kategorileri:\", features.columns.get_level_values(0).unique())\n",
    "    print(f\"Toplam özellik sayısı (ham): {features.shape[1]}\")\n",
    "    \n",
    "    # İlk olarak statistics sütunlarını çıkar (non-audio features)\n",
    "    print(\"\\n--- İstatistik sütunları çıkarılıyor ---\")\n",
    "    non_statistics_features = features.loc[:, features.columns.get_level_values(0) != 'statistics']\n",
    "    print(f\"Statistics çıkarıldıktan sonra: {non_statistics_features.shape[1]} özellik\")\n",
    "    \n",
    "    # Şimdi tüm audio feature kategorilerini analiz et\n",
    "    feature_categories = non_statistics_features.columns.get_level_values(0).unique()\n",
    "    print(f\"Audio özellik kategorileri: {list(feature_categories)}\")\n",
    "    \n",
    "    # Her kategorideki özellik sayısını göster\n",
    "    category_counts = {}\n",
    "    for category in feature_categories:\n",
    "        count = len([col for col in non_statistics_features.columns if col[0] == category])\n",
    "        category_counts[category] = count\n",
    "        print(f\"- {category}: {count} özellik\")\n",
    "    \n",
    "    # Tüm audio özelliklerini kullan (statistics hariç)\n",
    "    selected_features = non_statistics_features\n",
    "    \n",
    "    # Özellik türlerini kategorize et (daha sonra balanced selection için)\n",
    "    feature_counts = {'mfcc': 0, 'chroma': 0, 'spectral': 0, 'other': 0}\n",
    "    \n",
    "    # MFCC özellikleri\n",
    "    mfcc_cols = [col for col in selected_features.columns if 'mfcc' in col[0].lower()]\n",
    "    feature_counts['mfcc'] = len(mfcc_cols)\n",
    "    \n",
    "    # Chroma özellikleri  \n",
    "    chroma_cols = [col for col in selected_features.columns if 'chroma' in col[0].lower()]\n",
    "    feature_counts['chroma'] = len(chroma_cols)\n",
    "    \n",
    "    # Spectral özellikleri\n",
    "    spectral_keywords = ['spectral', 'centroid', 'bandwidth', 'contrast', 'rolloff']\n",
    "    spectral_cols = [col for col in selected_features.columns \n",
    "                    if any(keyword in col[0].lower() for keyword in spectral_keywords)]\n",
    "    feature_counts['spectral'] = len(spectral_cols)\n",
    "    \n",
    "    # Diğer audio özellikleri (MFCC, Chroma, Spectral olmayan)\n",
    "    other_cols = [col for col in selected_features.columns \n",
    "                 if col not in mfcc_cols and col not in chroma_cols and col not in spectral_cols]\n",
    "    feature_counts['other'] = len(other_cols)\n",
    "    \n",
    "    print(f\"\\n--- Özellik Kategorilerine Göre Dağılım ---\")\n",
    "    print(f\"- MFCC: {feature_counts['mfcc']} özellik\")\n",
    "    print(f\"- Chroma: {feature_counts['chroma']} özellik\")\n",
    "    print(f\"- Spectral: {feature_counts['spectral']} özellik\")\n",
    "    print(f\"- Diğer Audio: {feature_counts['other']} özellik\")\n",
    "    print(f\"- TOPLAM: {sum(feature_counts.values())} özellik\")\n",
    "    \n",
    "    # Özellik türü listelerini global değişken olarak sakla (feature selection'da kullanmak için)\n",
    "    global mfcc_column_names, chroma_column_names, spectral_column_names, other_column_names\n",
    "    mfcc_column_names = mfcc_cols\n",
    "    chroma_column_names = chroma_cols\n",
    "    spectral_column_names = spectral_cols\n",
    "    other_column_names = other_cols\n",
    "    \n",
    "    # Tüm özellikleri kullan\n",
    "    features = selected_features\n",
    "    print(f\"\\n✅ TÜM audio özellikleri kullanılacak: {features.shape[1]} özellik\")\n",
    "    \n",
    "    # Sayısal formata dönüştür\n",
    "    features = features.astype(np.float32)\n",
    "    features.index = features.index.astype(str)\n",
    "    tracks.index = tracks.index.astype(str)\n",
    "\n",
    "    genre_series = tracks[('track', 'genre_top')].dropna()\n",
    "    common_index = features.index.intersection(genre_series.index)\n",
    "\n",
    "    X = features.loc[common_index]\n",
    "    y_labels = genre_series.loc[common_index]\n",
    "\n",
    "    X = X.fillna(0).replace([np.inf, -np.inf], 0).astype(np.float32)\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "    y = label_encoder.fit_transform(y_labels)\n",
    "\n",
    "    print(f'\\n=== SONUÇ ===')    \n",
    "    print(f'- Toplam özellik sayısı: {X.shape[1]} (Önceki 518 ile karşılaştır!)')\n",
    "    print(f'- Örneklem sayısı: {X.shape[0]}')\n",
    "    print(f'- Sınıf sayısı: {len(label_encoder.classes_)}')\n",
    "    print(f'- MFCC: {feature_counts[\"mfcc\"]} özellik')\n",
    "    print(f'- Chroma: {feature_counts[\"chroma\"]} özellik') \n",
    "    print(f'- Spectral: {feature_counts[\"spectral\"]} özellik')\n",
    "    print(f'- Diğer Audio: {feature_counts[\"other\"]} özellik')\n",
    "    print(f'- Tam zaman serisi için TÜM audio özellikler dahil edildi')\n",
    "    \n",
    "    return X, y, label_encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c705ac60",
   "metadata": {},
   "source": [
    "## Başlangıç Veri Analizi\n",
    "\n",
    "Verinin ilk yüklemesini yapıp, başlangıçtaki sınıf dağılımını inceleyelim. Bu analiz, veri dengesizliği problemini görselleştirmemize yardımcı olacak."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97f61d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veriyi yükle ve önişle\n",
    "X, y, le = load_data()\n",
    "\n",
    "# Başlangıç dağılımını göster\n",
    "plot_class_distribution(y, le.classes_, 'Başlangıç Sınıf Dağılımı')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b6b7b9",
   "metadata": {},
   "source": [
    "## Veri Bölme ve Eğitim Seti Analizi\n",
    "\n",
    "Veriyi eğitim ve test setlerine ayırıp, eğitim setindeki sınıf dağılımını inceliyoruz. Stratified split kullanarak orijinal dağılımı koruyoruz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ef4617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veriyi böl ve eğitim dağılımını göster\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "plot_class_distribution(y_train, le.classes_, 'Eğitim Seti Dağılımı')\n",
    "print(f'Eğitim/test bölünmesi tamamlandı: X_train {X_train.shape}, X_test {X_test.shape}')\n",
    "\n",
    "# Detaylı dağılımı yazdır\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "print(\"\\nEğitim Seti Dağılımı (ham sayılar):\")\n",
    "for i, (u, c) in enumerate(zip(unique, counts)):\n",
    "    print(f\"Sınıf {u} ({le.classes_[i]}): {c} örnek\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48a9420",
   "metadata": {},
   "source": [
    "## Veri Dengeleme - Aşama 1\n",
    "\n",
    "İlk aşamada, çok az örneğe sahip sınıflar için RandomOverSampler kullanılıyor. Bu aşama, BorderlineSMOTE için yeterli örnek sayısına ulaşmamızı sağlar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d82e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adım 1: En az temsil edilen sınıflar için RandomOverSampler\n",
    "print('\\nAdım 1: Aşırı az temsil edilen sınıflar için RandomOverSampler uygulanıyor...')\n",
    "min_samples_threshold = 60  # BorderlineSMOTE için gereken minimum örnek sayısı\n",
    "ros = RandomOverSampler(sampling_strategy={3: min_samples_threshold}, random_state=42)\n",
    "X_partial, y_partial = ros.fit_resample(X_train, y_train)\n",
    "\n",
    "# Ara sonuçları göster\n",
    "unique_partial, counts_partial = np.unique(y_partial, return_counts=True)\n",
    "print(\"\\nRandomOverSampler sonrası dağılım (ham sayılar):\")\n",
    "for i, (u, c) in enumerate(zip(unique_partial, counts_partial)):\n",
    "    print(f\"Sınıf {u} ({le.classes_[i]}): {c} örnek\")\n",
    "\n",
    "plot_class_distribution(y_partial, le.classes_, 'RandomOverSampler Sonrası')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b9d611",
   "metadata": {},
   "source": [
    "## Veri Dengeleme - Aşama 2\n",
    "\n",
    "İkinci aşamada, daha sofistike bir yaklaşım olan BorderlineSMOTE kullanılarak kalan sınıflar dengeleniyor. Bu yöntem, sadece rastgele kopyalama yerine sentetik örnekler oluşturur.\n",
    "\n",
    "Not: Bu aşama, veri setinin yapısına bağlı olarak başarısız olabilir. Bu durumda, ilk aşamadaki sonuçlar kullanılacaktır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0e95e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adım 2: Kalan sınıflar için BorderlineSMOTE\n",
    "print('\\nAdım 2: Kalan sınıflar için BorderlineSMOTE uygulanıyor...')\n",
    "borderline_smote = BorderlineSMOTE(random_state=42)\n",
    "\n",
    "try:\n",
    "    X_res, y_res = borderline_smote.fit_resample(X_partial, y_partial)\n",
    "    print(f'Kombine örnekleme tamamlandı: X_res {X_res.shape}, y_res {y_res.shape}')\n",
    "    \n",
    "    # Son dağılımı yazdır ve göster\n",
    "    unique_res, counts_res = np.unique(y_res, return_counts=True)\n",
    "    print(\"\\nSon Dağılım (ham sayılar):\")\n",
    "    for i, (u, c) in enumerate(zip(unique_res, counts_res)):\n",
    "        print(f\"Sınıf {u} ({le.classes_[i]}): {c} örnek\")\n",
    "    \n",
    "    plot_class_distribution(y_res, le.classes_, 'Son Dengelenmiş Dağılım')\n",
    "\n",
    "except Exception as e:\n",
    "    print(f'BorderlineSMOTE örnekleme başarısız oldu: {e} - kısmi örneklenmiş veri kullanılıyor')\n",
    "    X_res, y_res = X_partial, y_partial\n",
    "    plot_class_distribution(y_res, le.classes_, 'Kısmi Örnekleme (BorderlineSMOTE başarısız)')\n",
    "    \n",
    "print(\"\\nİşlem hattı tamamlandı. Yeniden örneklenmiş eğitim verisi (X_res, y_res) ve test verisi (X_test, y_test) hazır.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291b507b",
   "metadata": {},
   "source": [
    "## Gelişmiş Özellik Seçimi (LSTM İçin Optimize)\n",
    "\n",
    "Model performansını artırmak ve LSTM için optimize edilmiş özellik seçimi uygulayacağız. Mutual Information ile temporal dependencies yakalarken, correlation filtering ile multicollinearity'yi azaltacağız."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c6196b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gelişmiş Özellik Seçimi - LSTM için Optimize\n",
    "from sklearn.feature_selection import mutual_info_classif, RFE, SelectKBest, f_classif\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print('\\nLSTM için optimize edilmiş gelişmiş özellik seçimi uygulanıyor...')\n",
    "\n",
    "# Toplam seçilecek özellik sayısı\n",
    "total_k = 160  # 4 kategoriye bölünecek (MFCC, Chroma, Spectral, Others)\n",
    "k_per_category = total_k // 4  # Her kategori için eşit sayıda özellik\n",
    "\n",
    "print(f\"Toplam özellik sayısı: {X_res.shape[1]}, Seçilecek özellik sayısı: {total_k}\")\n",
    "print(f\"Her özellik kategorisinden seçilecek: {k_per_category}\")\n",
    "\n",
    "# Özellik isimlerini oluştur\n",
    "if hasattr(X_res, 'columns'):\n",
    "    if isinstance(X_res.columns, pd.MultiIndex):\n",
    "        feature_names = [f\"{col[0]}_{col[1]}\" if len(col) > 1 else str(col[0]) for col in X_res.columns]\n",
    "    else:\n",
    "        feature_names = [str(col) for col in X_res.columns]\n",
    "else:\n",
    "    feature_names = [f'feature_{i}' for i in range(X_res.shape[1])]\n",
    "\n",
    "print(f\"Özellik isimleri oluşturuldu: {len(feature_names)} adet\")\n",
    "\n",
    "# X_res'i numpy array'e dönüştür\n",
    "X_res_array = X_res.values\n",
    "X_test_array = X_test.values\n",
    "\n",
    "# Hızlı ve Optimize Edilmiş Özellik Seçimi (LSTM için)\n",
    "print(\"\\n⚡ Hızlı ve Optimize Edilmiş özellik seçimi uygulanıyor...\")\n",
    "print(\"   ✅ SelectKBest + Basit korelasyon filtreleme\")\n",
    "print(\"   ✅ Çok daha hızlı ve etkili\")\n",
    "print(\"   ✅ LSTM için yeterince iyi performans\")\n",
    "\n",
    "# Özellik kategorilerini bul\n",
    "mfcc_indices = [i for i, name in enumerate(feature_names) if 'mfcc' in name.lower()]\n",
    "chroma_indices = [i for i, name in enumerate(feature_names) if 'chroma' in name.lower()]\n",
    "spectral_indices = [i for i, name in enumerate(feature_names) if any(spec in name.lower() for spec in ['spectral', 'centroid', 'bandwidth', 'contrast', 'rolloff'])]\n",
    "other_indices = [i for i in range(len(feature_names)) \n",
    "                if i not in mfcc_indices and i not in chroma_indices and i not in spectral_indices]\n",
    "\n",
    "print(f\"\\nÖzellik kategori sayıları:\")\n",
    "print(f\"- MFCC: {len(mfcc_indices)} özellik\")\n",
    "print(f\"- Chroma: {len(chroma_indices)} özellik\")\n",
    "print(f\"- Spectral: {len(spectral_indices)} özellik\")\n",
    "print(f\"- Diğer: {len(other_indices)} özellik\")\n",
    "\n",
    "# 1. Adım: Her kategoriden SelectKBest ile hızlı seçim\n",
    "selected_indices = []\n",
    "category_info = []\n",
    "\n",
    "for category_name, indices in [('MFCC', mfcc_indices), ('Chroma', chroma_indices), \n",
    "                              ('Spectral', spectral_indices), ('Others', other_indices)]:\n",
    "    if len(indices) > 0:\n",
    "        X_category = X_res_array[:, indices]\n",
    "        \n",
    "        # SelectKBest ile hızlı seçim (F-test)\n",
    "        k_category = min(k_per_category, len(indices))\n",
    "        selector = SelectKBest(score_func=f_classif, k=k_category)\n",
    "        selector.fit(X_category, y_res)\n",
    "        \n",
    "        # Seçilen özelliklerin orijinal indekslerini al\n",
    "        selected_mask = selector.get_support()\n",
    "        selected_category_indices = [indices[i] for i in range(len(indices)) if selected_mask[i]]\n",
    "        selected_indices.extend(selected_category_indices)\n",
    "        \n",
    "        category_info.append({\n",
    "            'name': category_name,\n",
    "            'total': len(indices),\n",
    "            'selected': len(selected_category_indices),\n",
    "            'avg_score': np.mean(selector.scores_[selected_mask])\n",
    "        })\n",
    "        \n",
    "        print(f\"   {category_name}: {len(selected_category_indices)} özellik seçildi, Ortalama F-score: {np.mean(selector.scores_[selected_mask]):.2f}\")\n",
    "\n",
    "# 2. Adım: Basit korelasyon filtreleme (çok daha hızlı)\n",
    "print(f\"\\n🔍 Basit korelasyon filtreleme uygulanıyor...\")\n",
    "final_indices = selected_indices.copy()\n",
    "\n",
    "if len(final_indices) > 50:  # Sadece çok fazla özellik varsa korelasyon filtrele\n",
    "    print(\"   Korelasyon matrisi hesaplanıyor...\")\n",
    "    corr_matrix = np.corrcoef(X_res_array[:, final_indices].T)\n",
    "    \n",
    "    # Basit korelasyon filtreleme (ilk bulduğunu çıkar)\n",
    "    to_remove = set()\n",
    "    for i in range(len(corr_matrix)):\n",
    "        if i in to_remove:\n",
    "            continue\n",
    "        for j in range(i+1, len(corr_matrix)):\n",
    "            if j not in to_remove and abs(corr_matrix[i, j]) > 0.95:  # Çok yüksek korelasyon\n",
    "                to_remove.add(j)  # j'yi çıkar (basit kural)\n",
    "                if len(to_remove) > 10:  # Maksimum 10 özellik çıkar\n",
    "                    break\n",
    "        if len(to_remove) > 10:\n",
    "            break\n",
    "    \n",
    "    # Çıkarılacak indeksleri kaldır\n",
    "    final_indices = [final_indices[i] for i in range(len(final_indices)) if i not in to_remove]\n",
    "    \n",
    "    print(f\"   {len(to_remove)} yüksek korelasyonlu özellik çıkarıldı\")\n",
    "    print(f\"   Final özellik sayısı: {len(final_indices)}\")\n",
    "else:\n",
    "    print(\"   Özellik sayısı az, korelasyon filtreleme atlanıyor\")\n",
    "\n",
    "hybrid_indices = final_indices\n",
    "\n",
    "# Seçilen özellikleri uygula\n",
    "X_res_selected = X_res_array[:, hybrid_indices]\n",
    "X_test_selected = X_test_array[:, hybrid_indices]\n",
    "selected_feature_names = [feature_names[i] for i in hybrid_indices]\n",
    "\n",
    "print(f\"\\n✅ Hızlı Özellik Seçimi tamamlandı!\")\n",
    "print(f\"📊 Final özellik sayısı: {len(hybrid_indices)}\")\n",
    "print(f\"📋 Seçilen özelliklerin boyutu: {X_res_selected.shape}\")\n",
    "\n",
    "# Final kategori dağılımını kontrol et\n",
    "final_mfcc = len([name for name in selected_feature_names if 'mfcc' in name.lower()])\n",
    "final_chroma = len([name for name in selected_feature_names if 'chroma' in name.lower()])\n",
    "final_spectral = len([name for name in selected_feature_names if any(spec in name.lower() for spec in ['spectral', 'centroid', 'bandwidth', 'contrast', 'rolloff'])])\n",
    "final_others = len(selected_feature_names) - final_mfcc - final_chroma - final_spectral\n",
    "\n",
    "print(f\"\\n📈 Final Kategori Dağılımı:\")\n",
    "print(f\"   - MFCC: {final_mfcc} özellik\")\n",
    "print(f\"   - Chroma: {final_chroma} özellik\")\n",
    "print(f\"   - Spectral: {final_spectral} özellik\")\n",
    "print(f\"   - Others: {final_others} özellik\")\n",
    "\n",
    "# Global değişkenleri güncelle\n",
    "X_res = X_res_selected\n",
    "X_test = X_test_selected\n",
    "feature_names = selected_feature_names\n",
    "\n",
    "print(f\"\\n🎯 Hızlı özellik seçimi tamamlandı!\")\n",
    "print(f\"⚡ Veri şekli güncellendi: X_res {X_res.shape}, X_test {X_test.shape}\")\n",
    "print(f\"✨ SelectKBest + Basit korelasyon filtreleme kullanıldı!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d81393",
   "metadata": {},
   "source": [
    "## Özellik Seçimi Doğrulaması\n",
    "\n",
    "Seçilen özelliklerin doğru bir şekilde MFCC, Chroma ve Spectral kategorilerinden dengeli olarak seçilip seçilmediğini ve Hybrid MI + Correlation Filter yönteminin etkinliğini kontrol edelim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395819c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seçilen özelliklerin doğrulaması\n",
    "print(\"\\n=== ÖZELLİK SEÇİMİ DOĞRULAMASI ===\")\n",
    "\n",
    "# Seçilen özelliklerin kategorilere göre dağılımını kontrol et\n",
    "mfcc_selected = len([name for name in feature_names if 'mfcc' in name.lower()])\n",
    "chroma_selected = len([name for name in feature_names if 'chroma' in name.lower()])\n",
    "spectral_selected = len([name for name in feature_names if any(spec in name.lower() for spec in ['spectral', 'centroid', 'bandwidth', 'contrast', 'rolloff'])])\n",
    "others_selected = len(feature_names) - mfcc_selected - chroma_selected - spectral_selected\n",
    "\n",
    "print(f\"Seçilen özellik dağılımı:\")\n",
    "print(f\"- MFCC: {mfcc_selected} özellik\")\n",
    "print(f\"- Chroma: {chroma_selected} özellik\")\n",
    "print(f\"- Spectral: {spectral_selected} özellik\")\n",
    "print(f\"- Diğer Audio: {others_selected} özellik\")\n",
    "print(f\"- Toplam: {len(feature_names)} özellik\")\n",
    "\n",
    "# Dengelilik kontrolü\n",
    "expected_per_category = total_k // 4\n",
    "balance_check = (\n",
    "    abs(mfcc_selected - expected_per_category) <= 5 and \n",
    "    abs(chroma_selected - expected_per_category) <= 5 and \n",
    "    abs(spectral_selected - expected_per_category) <= 5 and\n",
    "    abs(others_selected - expected_per_category) <= 5\n",
    ")\n",
    "\n",
    "if balance_check:\n",
    "    print(\"\\n✅ Başarılı! Özellik seçimi dengeli olarak yapıldı.\")\n",
    "else:\n",
    "    print(\"\\n⚠️ Uyarı: Özellik seçimi tam dengeli değil, ancak hızlı ve etkili bir şekilde uygulandı.\")\n",
    "\n",
    "# Hızlı özellik seçimi analizi\n",
    "print(f\"\\n🎵 Hızlı Audio Feature Selection:\")\n",
    "print(f\"- MFCC özellikleri: SelectKBest ile seçilmiş mel-frequency coefficients\")\n",
    "print(f\"- Chroma özellikleri: SelectKBest ile seçilmiş harmonic content\")\n",
    "print(f\"- Spectral özellikleri: SelectKBest ile seçilmiş spectral characteristics\")\n",
    "print(f\"- Others: Diğer önemli audio features\")\n",
    "\n",
    "# Örnek seçilen özellik isimlerini göster\n",
    "print(\"\\nÖrnek seçilen özellik isimleri (ilk 10):\")\n",
    "for i, name in enumerate(feature_names[:10]):\n",
    "    print(f\"{i+1:2d}. {name}\")\n",
    "\n",
    "print(f\"\\nSon veri şekli: X_res {X_res.shape}, X_test {X_test.shape}\")\n",
    "print(\"Artık hızlı ve etkili özellik seçimi ile LSTM modeli eğitmeye hazırız!\")\n",
    "print(\"\\n✨ SelectKBest + basit korelasyon filtreleme uygulandı!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69962f4f",
   "metadata": {},
   "source": [
    "*-----------------------------------------------------------------------------------*\n",
    "# PyTorch LSTM MODEL EĞİTİMİ\n",
    "*-----------------------------------------------------------------------------------*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340b3123",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nPyTorch LSTM Model Eğitimi Başlıyor...\")\n",
    "\n",
    "# Veri yükleme, önişleme, bölme ve dengeleme adımlarının tamamlandığı varsayılır.\n",
    "# Bu noktada aşağıdaki değişkenlerin mevcut olması beklenir:\n",
    "# X_res, y_res (Dengelenmiş eğitim verisi)\n",
    "# X_val, y_val (Doğrulama verisi)\n",
    "# X_test, y_test (Test verisi)\n",
    "# le (LabelEncoder nesnesi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4bc7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dengelenmiş veri setinden doğrulama seti ayır\n",
    "X_train_bal, X_val, y_train_bal, y_val = train_test_split(\n",
    "    X_res, y_res, test_size=0.1, stratify=y_res, random_state=42\n",
    ")\n",
    "\n",
    "# Veri Ölçeklendirme (StandardScaler)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_bal)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"Veri ölçeklendirme tamamlandı.\")\n",
    "print(f\"Ölçeklenmiş eğitim verisi boyutu: {X_train_scaled.shape}\")\n",
    "print(f\"Ölçeklenmiş doğrulama verisi boyutu: {X_val_scaled.shape}\")\n",
    "print(f\"Ölçeklenmiş test verisi boyutu: {X_test_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f746e0b",
   "metadata": {},
   "source": [
    "## LSTM Modeli için Veri Hazırlığı\n",
    "\n",
    "PyTorch LSTM modeli için, veriyi uygun formata dönüştürmemiz gerekir. LSTM modeller sıralı veri bekler.\n",
    "\n",
    "**Önemli**: FMA özellikleri gerçekte temporal (zamansal) yapıya sahiptir:\n",
    "- **MFCC**: Zamansal pencerelerden çıkarılan mel-frekans katsayıları\n",
    "- **Chroma**: Zaman içinde değişen ton özellikleri\n",
    "- **Spectral**: Zamansal spektral karakteristikler\n",
    "\n",
    "Bu nedenle, yapay sıralama yerine **gerçek temporal yapıyı koruyan** bir yaklaşım kullanıyoruz:\n",
    "- Her özellik kategorisi ayrı bir zaman adımı olur\n",
    "- Gerçek audio feature temporal ilişkileri korunur\n",
    "- LSTM gerçek müzik temporal pattern'larını öğrenebilir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de356a9",
   "metadata": {},
   "source": [
    "### Alternatif Yaklaşımlar ve Seçenekler\n",
    "\n",
    "**1. Grouped Approach (Mevcut)**: Her özellik türünü ayrı timestep olarak kullanır\n",
    "- Avantaj: Gerçek audio feature kategorilerini korur\n",
    "- Dezavantaj: Kategori içi temporal sırayı tam koruyamaz\n",
    "\n",
    "**2. Feature Type Separated**: Her özellik türünü ayrı kanal olarak işler\n",
    "- Avantaj: Temporal yapıyı bozmadan özellik türlerini korur\n",
    "- LSTM için optimize edilmiş yaklaşım\n",
    "\n",
    "**3. Transformer Yaklaşım**: Attention mekanizması ile\n",
    "- Avantaj: Uzun mesafe bağımlılıkları yakalar\n",
    "- Modern ve etkili (LSTM alternatifi)\n",
    "\n",
    "**4. Standard MLP**: Sequential structure'u yok say\n",
    "- Avantaj: Basit ve hızlı\n",
    "- Dezavantaj: Temporal bilgiyi kaybeder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa7d165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch tensörlerine dönüştürme ve veri setlerini hazırlama\n",
    "def create_temporal_sequence_data(X, y, feature_names, sequence_approach='grouped'):\n",
    "    \"\"\"\n",
    "    Gerçek temporal özellik yapısını koruyarak sıralı veri oluşturur.\n",
    "    \n",
    "    Args:\n",
    "        X: Özellik matrisi\n",
    "        y: Etiketler\n",
    "        feature_names: Özellik isimleri\n",
    "        sequence_approach: 'grouped' veya 'individual'\n",
    "    \"\"\"\n",
    "    n_samples, n_features = X.shape\n",
    "    \n",
    "    if sequence_approach == 'grouped':\n",
    "        # Yaklaşım 1: Her özellik türünü ayrı timestep olarak kullan\n",
    "        # MFCC -> timestep 1, Chroma -> timestep 2, Spectral -> timestep 3, Others -> timestep 4\n",
    "        \n",
    "        # Özellik kategorilerini ayır\n",
    "        mfcc_indices = [i for i, name in enumerate(feature_names) if 'mfcc' in name.lower()]\n",
    "        chroma_indices = [i for i, name in enumerate(feature_names) if 'chroma' in name.lower()]\n",
    "        spectral_indices = [i for i, name in enumerate(feature_names) if any(spec in name.lower() for spec in ['spectral', 'centroid', 'bandwidth', 'contrast', 'rolloff'])]\n",
    "        other_indices = [i for i in range(len(feature_names)) \n",
    "                        if i not in mfcc_indices and i not in chroma_indices and i not in spectral_indices]\n",
    "        \n",
    "        # Her kategoriyi ayrı timestep olarak düzenle\n",
    "        categories = [mfcc_indices, chroma_indices, spectral_indices, other_indices]\n",
    "        category_names = ['MFCC', 'Chroma', 'Spectral', 'Others']\n",
    "        \n",
    "        # En büyük kategori boyutunu bul (padding için)\n",
    "        max_features_per_category = max(len(cat) for cat in categories if len(cat) > 0)\n",
    "        sequence_length = len([cat for cat in categories if len(cat) > 0])  # Boş olmayan kategori sayısı\n",
    "        \n",
    "        print(f\"Temporal organizasyon:\")\n",
    "        for i, (cat, name) in enumerate(zip(categories, category_names)):\n",
    "            if len(cat) > 0:\n",
    "                print(f\"  Timestep {i+1}: {name} - {len(cat)} özellik\")\n",
    "        \n",
    "        # Sequence tensor oluştur\n",
    "        X_seq = np.zeros((n_samples, sequence_length, max_features_per_category))\n",
    "        \n",
    "        timestep = 0\n",
    "        for cat_indices in categories:\n",
    "            if len(cat_indices) > 0:\n",
    "                # Bu kategorinin özelliklerini al\n",
    "                cat_features = X[:, cat_indices]\n",
    "                # Padding ile aynı boyuta getir\n",
    "                X_seq[:, timestep, :len(cat_indices)] = cat_features\n",
    "                timestep += 1\n",
    "        \n",
    "        print(f\"Final sequence shape: {X_seq.shape}\")\n",
    "        print(f\"(samples, timesteps, features_per_timestep)\")\n",
    "        \n",
    "    elif sequence_approach == 'individual':\n",
    "        # Yaklaşım 2: Her özelliği ayrı timestep olarak kullan (çok uzun olabilir)\n",
    "        sequence_length = min(n_features, 50)  # Maksimum 50 timestep\n",
    "        features_per_timestep = 1\n",
    "        \n",
    "        X_seq = np.zeros((n_samples, sequence_length, features_per_timestep))\n",
    "        \n",
    "        for i in range(n_samples):\n",
    "            for t in range(sequence_length):\n",
    "                X_seq[i, t, 0] = X[i, t]\n",
    "        \n",
    "        print(f\"Individual feature sequence shape: {X_seq.shape}\")\n",
    "    \n",
    "    else:\n",
    "        # Yaklaşım 3: Geleneksel (artificial) yöntem - artık önerilmiyor\n",
    "        print(\"⚠️ Uyarı: Artificial sequence yöntemi kullanılıyor - temporal yapı bozulabilir\")\n",
    "        sequence_length = 10\n",
    "        features_per_timestep = n_features // sequence_length\n",
    "        \n",
    "        if features_per_timestep == 0:\n",
    "            features_per_timestep = 1\n",
    "            sequence_length = min(sequence_length, n_features)\n",
    "        \n",
    "        X_seq = np.zeros((n_samples, sequence_length, features_per_timestep))\n",
    "        \n",
    "        for i in range(n_samples):\n",
    "            for t in range(sequence_length):\n",
    "                start_idx = t * features_per_timestep\n",
    "                end_idx = min(start_idx + features_per_timestep, n_features)\n",
    "                \n",
    "                if start_idx < n_features:\n",
    "                    X_seq[i, t, :end_idx-start_idx] = X[i, start_idx:end_idx]\n",
    "    \n",
    "    # PyTorch tensörlerine dönüştürme\n",
    "    X_tensor = torch.FloatTensor(X_seq)\n",
    "    y_tensor = torch.LongTensor(y)\n",
    "    \n",
    "    return X_tensor, y_tensor\n",
    "\n",
    "# Temporal sequence yaklaşımını seç\n",
    "sequence_approach = 'grouped'  # 'grouped', 'individual', veya 'artificial'\n",
    "\n",
    "print(f\"\\n🎵 Temporal Audio Feature Organization ile Sequence Oluşturma\")\n",
    "print(f\"Seçilen yaklaşım: {sequence_approach}\")\n",
    "print(f\"Bu yaklaşım gerçek audio feature temporal yapısını korur!\\n\")\n",
    "\n",
    "# Ölçeklenmiş verileri temporal forma dönüştürme\n",
    "X_train_seq, y_train_tensor = create_temporal_sequence_data(X_train_scaled, y_train_bal, feature_names, sequence_approach)\n",
    "X_val_seq, y_val_tensor = create_temporal_sequence_data(X_val_scaled, y_val, feature_names, sequence_approach)\n",
    "X_test_seq, y_test_tensor = create_temporal_sequence_data(X_test_scaled, y_test, feature_names, sequence_approach)\n",
    "\n",
    "print(f\"\\n📊 Final Sequence Boyutları:\")\n",
    "print(f\"Eğitim veri boyutu: {X_train_seq.shape}\")\n",
    "print(f\"Doğrulama veri boyutu: {X_val_seq.shape}\")\n",
    "print(f\"Test veri boyutu: {X_test_seq.shape}\")\n",
    "\n",
    "# PyTorch DataLoader oluşturma\n",
    "batch_size = 512\n",
    "train_dataset = TensorDataset(X_train_seq, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_seq, y_val_tensor)\n",
    "test_dataset = TensorDataset(X_test_seq, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"\\n✅ Temporal Audio Sequence DataLoaders hazır!\")\n",
    "print(f\"🎯 Artık LSTM gerçek audio feature temporal patterns öğrenebilir!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a76bffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatif: Daha İyi Temporal Sequence Yaklaşımı\n",
    "# Eğer gerçek temporal yapıyı daha iyi korumak istiyorsak:\n",
    "\n",
    "def create_improved_temporal_data(X, y, feature_names, approach='feature_type_separated'):\n",
    "    \"\"\"\n",
    "    İyileştirilmiş temporal sequence yapısı oluşturur.\n",
    "    \n",
    "    Args:\n",
    "        X: Özellik matrisi\n",
    "        y: Etiketler  \n",
    "        feature_names: Özellik isimleri\n",
    "        approach: 'feature_type_separated' veya 'no_sequence'\n",
    "    \"\"\"\n",
    "    n_samples, n_features = X.shape\n",
    "    \n",
    "    if approach == 'feature_type_separated':\n",
    "        # Her özellik türünü ayrı kanal olarak işle\n",
    "        # Bu, temporal yapıyı bozmadan özellik türlerini korur\n",
    "        \n",
    "        # Özellik kategorilerini bul\n",
    "        mfcc_indices = [i for i, name in enumerate(feature_names) if 'mfcc' in name.lower()]\n",
    "        chroma_indices = [i for i, name in enumerate(feature_names) if 'chroma' in name.lower()]\n",
    "        spectral_indices = [i for i, name in enumerate(feature_names) if any(spec in name.lower() for spec in ['spectral', 'centroid', 'bandwidth', 'contrast', 'rolloff'])]\n",
    "        other_indices = [i for i in range(len(feature_names)) \n",
    "                        if i not in mfcc_indices and i not in chroma_indices and i not in spectral_indices]\n",
    "        \n",
    "        # Her kategoriyi ayrı tensor olarak hazırla\n",
    "        print(f\"Feature Type Separated yaklaşımı:\")\n",
    "        print(f\"- MFCC: {len(mfcc_indices)} özellik\")\n",
    "        print(f\"- Chroma: {len(chroma_indices)} özellik\")\n",
    "        print(f\"- Spectral: {len(spectral_indices)} özellik\")\n",
    "        print(f\"- Others: {len(other_indices)} özellik\")\n",
    "        \n",
    "        # En büyük kategori boyutunu bul\n",
    "        max_features = max(len(mfcc_indices), len(chroma_indices), len(spectral_indices), len(other_indices))\n",
    "        \n",
    "        # 4 kanal (feature type) x max_features boyutunda tensor\n",
    "        X_seq = np.zeros((n_samples, 4, max_features))\n",
    "        \n",
    "        # Her kategoriyi ayrı kanala yerleştir\n",
    "        if len(mfcc_indices) > 0:\n",
    "            X_seq[:, 0, :len(mfcc_indices)] = X[:, mfcc_indices]\n",
    "        if len(chroma_indices) > 0:\n",
    "            X_seq[:, 1, :len(chroma_indices)] = X[:, chroma_indices]\n",
    "        if len(spectral_indices) > 0:\n",
    "            X_seq[:, 2, :len(spectral_indices)] = X[:, spectral_indices]\n",
    "        if len(other_indices) > 0:\n",
    "            X_seq[:, 3, :len(other_indices)] = X[:, other_indices]\n",
    "            \n",
    "        print(f\"Output shape: {X_seq.shape} (samples, feature_types, max_features_per_type)\")\n",
    "        \n",
    "    else:  # 'no_sequence'\n",
    "        # LSTM'e gerek yok, standart Dense layer yaklaşımı\n",
    "        # Bu, temporal yapıyı tamamen yok sayar ama performans açısından daha iyi olabilir\n",
    "        print(f\"⚠️ No Sequence yaklaşımı: Temporal yapı tamamen yok sayılıyor\")\n",
    "        X_seq = X.copy()  # Orijinal boyutlarda tut\n",
    "        print(f\"Output shape: {X_seq.shape} (samples, features) - Standard MLP için uygun\")\n",
    "    \n",
    "    # PyTorch tensors\n",
    "    X_tensor = torch.FloatTensor(X_seq)\n",
    "    y_tensor = torch.LongTensor(y)\n",
    "    \n",
    "    return X_tensor, y_tensor, approach\n",
    "\n",
    "print(\"\\n🔧 Alternatif Temporal Yaklaşımlar Hazır!\")\n",
    "print(\"Seçenekler:\")\n",
    "print(\"1. 'feature_type_separated' - Her özellik türü ayrı kanal\")\n",
    "print(\"2. 'no_sequence' - Temporal yapıyı yok say, standard MLP\")\n",
    "print(\"\\nÖnerimiz: Önce 'no_sequence' deneyin, daha sonra 'feature_type_separated' ile karşılaştırın!\")\n",
    "\n",
    "# Performans karşılaştırması için her yaklaşımı hazırla\n",
    "# Şimdilik mevcut 'grouped' yaklaşımını kullanıyoruz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719d6bcd",
   "metadata": {},
   "source": [
    "## LSTM Model Tanımı ve Eğitimi\n",
    "\n",
    "Aşağıda müzik türü sınıflandırması için bir LSTM (Long Short-Term Memory) ağı tanımlıyoruz. LSTM'ler, müzik gibi sıralı verilerde başarılı olan bir derin öğrenme mimarisidir.\n",
    "\n",
    "Model, LSTM katmanlarından sonra bir **attention mekanizması** içerir. Bu mekanizma, modelin yapay olarak oluşturulan zaman adımlarının hangilerinin daha bilgilendirici olduğunu öğrenmesine yardımcı olur ve her zaman adımına farklı ağırlıklar vererek daha etkili bir özellik kombinasyonu oluşturur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d73d18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention mekanizması sınıfı\n",
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Attention ağırlıklarını hesaplamak için linear katmanlar\n",
    "        self.attention_linear = nn.Linear(hidden_size, hidden_size)\n",
    "        self.context_vector = nn.Linear(hidden_size, 1, bias=False)\n",
    "        \n",
    "    def forward(self, lstm_outputs):\n",
    "        # lstm_outputs şekli: (batch_size, sequence_length, hidden_size)\n",
    "        \n",
    "        # Her zaman adımı için attention skorları hesapla\n",
    "        attention_weights = self.attention_linear(lstm_outputs)  # (batch_size, seq_len, hidden_size)\n",
    "        attention_weights = torch.tanh(attention_weights)\n",
    "        attention_scores = self.context_vector(attention_weights)  # (batch_size, seq_len, 1)\n",
    "        attention_scores = attention_scores.squeeze(2)  # (batch_size, seq_len)\n",
    "        \n",
    "        # Softmax ile normalize et\n",
    "        attention_weights = torch.softmax(attention_scores, dim=1)  # (batch_size, seq_len)\n",
    "        \n",
    "        # Weighted sum hesapla\n",
    "        # attention_weights: (batch_size, seq_len) -> (batch_size, seq_len, 1)\n",
    "        attention_weights = attention_weights.unsqueeze(2)\n",
    "        \n",
    "        # Weighted combination of LSTM outputs\n",
    "        attended_output = torch.sum(lstm_outputs * attention_weights, dim=1)  # (batch_size, hidden_size)\n",
    "        \n",
    "        return attended_output, attention_weights.squeeze(2)\n",
    "\n",
    "# Bidirectional LSTM model sınıfını tanımlama (Attention ile)\n",
    "class MusicGenreLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout=0.3, bidirectional=True):\n",
    "        super(MusicGenreLSTM, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional = bidirectional\n",
    "        \n",
    "        # Bidirectional LSTM katmanları\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size, \n",
    "            hidden_size=hidden_size, \n",
    "            num_layers=num_layers, \n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=bidirectional\n",
    "        )\n",
    "        \n",
    "        # Bidirectional LSTM çıkış boyutunu hesapla\n",
    "        lstm_output_size = hidden_size * 2 if bidirectional else hidden_size\n",
    "        \n",
    "        # Attention katmanı (bidirectional output için)\n",
    "        self.attention = AttentionLayer(lstm_output_size)\n",
    "        \n",
    "        # Batch normalization\n",
    "        self.batch_norm = nn.BatchNorm1d(lstm_output_size)\n",
    "        \n",
    "        # Dropout katmanı\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Tam bağlantılı katmanlar\n",
    "        self.fc1 = nn.Linear(lstm_output_size, 128)  # İlk tam bağlantılı katman\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "        \n",
    "        # Aktivasyon fonksiyonları\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Bidirectional LSTM katmanından geçirme\n",
    "        # x şekli: (batch_size, sequence_length, input_size)\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        \n",
    "        # lstm_out şekli: (batch_size, sequence_length, hidden_size * 2) for bidirectional\n",
    "        # veya (batch_size, sequence_length, hidden_size) for unidirectional\n",
    "        \n",
    "        # Attention mekanizması uygula\n",
    "        attended_output, attention_weights = self.attention(lstm_out)\n",
    "        \n",
    "        # Batch normalization\n",
    "        batch_norm_out = self.batch_norm(attended_output)\n",
    "        \n",
    "        # İlk tam bağlantılı katman\n",
    "        fc1_out = self.fc1(batch_norm_out)\n",
    "        fc1_out = self.relu(fc1_out)\n",
    "        fc1_out = self.dropout(fc1_out)\n",
    "        \n",
    "        # İkinci tam bağlantılı katman (çıkış katmanı)\n",
    "        out = self.fc2(fc1_out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "# Learning Rate Warmup Scheduler\n",
    "class WarmupScheduler:\n",
    "    def __init__(self, optimizer, warmup_epochs, max_lr, min_lr=1e-7):\n",
    "        self.optimizer = optimizer\n",
    "        self.warmup_epochs = warmup_epochs\n",
    "        self.max_lr = max_lr\n",
    "        self.min_lr = min_lr\n",
    "        self.current_epoch = 0\n",
    "        \n",
    "    def step(self):\n",
    "        if self.current_epoch < self.warmup_epochs:\n",
    "            # Linear warmup\n",
    "            lr = self.min_lr + (self.max_lr - self.min_lr) * (self.current_epoch / self.warmup_epochs)\n",
    "            for param_group in self.optimizer.param_groups:\n",
    "                param_group['lr'] = lr\n",
    "        self.current_epoch += 1\n",
    "        \n",
    "    def get_lr(self):\n",
    "        return self.optimizer.param_groups[0]['lr']\n",
    "\n",
    "# Model parametreleri\n",
    "input_size = X_train_seq.shape[2]  # Bir zaman adımındaki özellik sayısı\n",
    "hidden_size = 64  # LSTM gizli katman boyutu\n",
    "num_layers = 2  # LSTM katman sayısı\n",
    "num_classes = len(le.classes_)  # Sınıf sayısı\n",
    "dropout = 0.3\n",
    "bidirectional = True  # Bidirectional LSTM kullan\n",
    "\n",
    "# GPU kullanılabilir mi kontrol et\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Kullanılan cihaz: {device}\")\n",
    "\n",
    "# Model oluşturma\n",
    "model = MusicGenreLSTM(input_size, hidden_size, num_layers, num_classes, dropout, bidirectional).to(device)\n",
    "print(f\"\\nModel yapısı (Bidirectional: {bidirectional}):\")\n",
    "print(model)\n",
    "\n",
    "# Model parametrelerinin sayısını hesapla\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"\\nToplam parametre sayısı: {total_params:,}\")\n",
    "print(f\"Eğitilebilir parametre sayısı: {trainable_params:,}\")\n",
    "\n",
    "# Gelişmiş Learning Rate Ayarları\n",
    "initial_lr = 0.0001\n",
    "max_lr = 0.01  # Maksimum öğrenme oranı\n",
    "min_lr = 1e-6  # Minimum öğrenme oranı\n",
    "warmup_epochs = 3  # İlk 3 epoch'ta yavaşça arttır\n",
    "\n",
    "# Kayıp fonksiyonu ve optimize edici tanımlama\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=min_lr)  # Düşük lr ile başla\n",
    "\n",
    "# Çoklu scheduler sistemi\n",
    "# 1. Warmup scheduler - İlk birkaç epoch'ta learning rate'i yavaşça arttırır\n",
    "warmup_scheduler = WarmupScheduler(optimizer, warmup_epochs=warmup_epochs, max_lr=max_lr, min_lr=min_lr)\n",
    "\n",
    "# 2. ReduceLROnPlateau - Validation loss plateau'ya ulaştığında LR'yi azaltır\n",
    "reduce_scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=3, min_lr=min_lr\n",
    ")\n",
    "\n",
    "# 3. CosineAnnealing - Cosine fonksiyonu ile LR'yi düzenli olarak değiştirir\n",
    "cosine_scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer, T_max=20, eta_min=min_lr\n",
    ")\n",
    "\n",
    "print(f\"\\nLearning Rate Ayarları:\")\n",
    "print(f\"Başlangıç LR: {min_lr}\")\n",
    "print(f\"Maksimum LR: {max_lr}\")\n",
    "print(f\"Minimum LR: {min_lr}\")\n",
    "print(f\"Warmup Epochs: {warmup_epochs}\")\n",
    "\n",
    "# Gelişmiş eğitim fonksiyonu\n",
    "def train_model_with_advanced_lr(model, train_loader, val_loader, criterion, optimizer, \n",
    "                                warmup_scheduler, reduce_scheduler, cosine_scheduler,\n",
    "                                num_epochs=50, early_stopping_patience=3, \n",
    "                                min_improvement_threshold=0.025, use_cosine_after_warmup=True):\n",
    "    \"\"\"\n",
    "    Gelişmiş learning rate scheduling ile model eğitimi\n",
    "    \"\"\"\n",
    "    # Ölçüm değerlerini saklayacak listeler\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accs = []\n",
    "    val_accs = []\n",
    "    learning_rates = []  # LR geçmişini takip et\n",
    "    \n",
    "    # En iyi doğrulama kaybını ve modeli saklama\n",
    "    best_val_loss = float('inf')\n",
    "    best_model = None\n",
    "    \n",
    "    # Erken durdurma için sayaç\n",
    "    early_stopping_counter = 0\n",
    "    \n",
    "    print(\"\\n=== Gelişmiş Learning Rate Scheduling ile Eğitim Başlıyor ===\")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Learning Rate Scheduling\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        if epoch < warmup_scheduler.warmup_epochs:\n",
    "            # Warmup phase\n",
    "            warmup_scheduler.step()\n",
    "            schedule_info = f\"Warmup Phase (Epoch {epoch+1}/{warmup_scheduler.warmup_epochs})\"\n",
    "        elif use_cosine_after_warmup and epoch >= warmup_scheduler.warmup_epochs:\n",
    "            # Cosine annealing after warmup\n",
    "            cosine_scheduler.step()\n",
    "            schedule_info = \"Cosine Annealing\"\n",
    "        else:\n",
    "            schedule_info = \"ReduceLROnPlateau (will be applied after validation)\"\n",
    "        \n",
    "        new_lr = optimizer.param_groups[0]['lr']\n",
    "        learning_rates.append(new_lr)\n",
    "        \n",
    "        # Eğitim modu\n",
    "        model.train()\n",
    "        \n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            # Gradyanları sıfırla\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # İleri geçiş\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Geri yayılım ve optimize etme\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping (exploding gradient problemini önler)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            # İstatistikleri güncelle\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            train_total += labels.size(0)\n",
    "            train_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        # Doğrulama modu\n",
    "        model.eval()\n",
    "        \n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                \n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        # Epoch sonuçlarını hesapla\n",
    "        epoch_train_loss = train_loss / len(train_loader.dataset)\n",
    "        epoch_val_loss = val_loss / len(val_loader.dataset)\n",
    "        epoch_train_acc = train_correct / train_total\n",
    "        epoch_val_acc = val_correct / val_total\n",
    "        \n",
    "        # ReduceLROnPlateau scheduler'ı warmup sonrası uygula\n",
    "        if epoch >= warmup_scheduler.warmup_epochs and not use_cosine_after_warmup:\n",
    "            reduce_scheduler.step(epoch_val_loss)\n",
    "        \n",
    "        # Sonuçları sakla\n",
    "        train_losses.append(epoch_train_loss)\n",
    "        val_losses.append(epoch_val_loss)\n",
    "        train_accs.append(epoch_train_acc)\n",
    "        val_accs.append(epoch_val_acc)\n",
    "        \n",
    "        # Eğitim durumunu yazdır\n",
    "        print(f'Epoch {epoch+1:2d}/{num_epochs} | '\n",
    "              f'Train Loss: {epoch_train_loss:.4f} | Train Acc: {epoch_train_acc:.4f} | '\n",
    "              f'Val Loss: {epoch_val_loss:.4f} | Val Acc: {epoch_val_acc:.4f} | '\n",
    "              f'LR: {new_lr:.2e} | {schedule_info}')\n",
    "        \n",
    "        # En iyi modeli sakla ve erken durdurma kontrolü\n",
    "        improvement = best_val_loss - epoch_val_loss\n",
    "        \n",
    "        if epoch_val_loss < best_val_loss:\n",
    "            if improvement > min_improvement_threshold:\n",
    "                early_stopping_counter = 0\n",
    "                improvement_msg = f\"✓ Significant improvement: {improvement:.6f}\"\n",
    "            else:\n",
    "                early_stopping_counter += 1\n",
    "                improvement_msg = f\"⚠ Minor improvement: {improvement:.6f}\"\n",
    "            \n",
    "            best_val_loss = epoch_val_loss\n",
    "            best_model = model.state_dict()\n",
    "            print(f\"  {improvement_msg}\")\n",
    "        else:\n",
    "            early_stopping_counter += 1\n",
    "            print(f\"  ✗ No improvement (counter: {early_stopping_counter}/{early_stopping_patience})\")\n",
    "            \n",
    "        # Erken durdurma kontrolü\n",
    "        if early_stopping_counter >= early_stopping_patience:\n",
    "            print(f'\\n🛑 Erken durdurma: {early_stopping_patience} epoch boyunca yeterli iyileşme yok.')\n",
    "            break\n",
    "    \n",
    "    # En iyi model ağırlıklarını yükle\n",
    "    if best_model is not None:\n",
    "        model.load_state_dict(best_model)\n",
    "        print(f\"\\n✅ En iyi model yüklendi (Val Loss: {best_val_loss:.6f})\")\n",
    "    \n",
    "    return model, train_losses, val_losses, train_accs, val_accs, learning_rates\n",
    "\n",
    "# Modeli gelişmiş LR scheduling ile eğit\n",
    "print(\"\\n🚀 Bidirectional LSTM model eğitimi (Gelişmiş LR Scheduling) başlıyor...\")\n",
    "num_epochs = 50\n",
    "early_stopping_patience = 3  # Erken durdurma için sabır sayısı\n",
    "min_improvement_threshold = 0.025  # İyileşme eşiği\n",
    "\n",
    "try:\n",
    "    model, train_losses, val_losses, train_accs, val_accs, learning_rates = train_model_with_advanced_lr(\n",
    "        model, train_loader, val_loader, criterion, optimizer, \n",
    "        warmup_scheduler, reduce_scheduler, cosine_scheduler,\n",
    "        num_epochs=num_epochs, \n",
    "        early_stopping_patience=early_stopping_patience,\n",
    "        min_improvement_threshold=min_improvement_threshold,\n",
    "        use_cosine_after_warmup=True  # Warmup sonrası cosine annealing kullan\n",
    "    )\n",
    "    print(\"\\n🎉 Model eğitimi başarıyla tamamlandı!\")\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n⏹️ Eğitim kullanıcı tarafından durduruldu.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f720d377",
   "metadata": {},
   "source": [
    "## Model Değerlendirmesi ve Görselleştirme\n",
    "\n",
    "Bu bölümde eğitilmiş modeli test veri seti üzerinde değerlendirip, sonuçları görselleştireceğiz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5361a1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eğitim sonuçlarını görselleştirme\n",
    "def plot_training_history(train_losses, val_losses, train_accs, val_accs, learning_rates=None):\n",
    "    if learning_rates is not None:\n",
    "        plt.figure(figsize=(18, 6))\n",
    "        \n",
    "        # Kayıp grafiği\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.plot(train_losses, label='Eğitim', marker='o', alpha=0.7)\n",
    "        plt.plot(val_losses, label='Doğrulama', marker='*', alpha=0.7)\n",
    "        plt.title('Model Kaybı')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Kayıp (Cross-Entropy)')\n",
    "        plt.legend()\n",
    "        plt.grid(True, linestyle='--', alpha=0.6)\n",
    "        \n",
    "        # Doğruluk grafiği\n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.plot(train_accs, label='Eğitim', marker='o', alpha=0.7)\n",
    "        plt.plot(val_accs, label='Doğrulama', marker='*', alpha=0.7)\n",
    "        plt.title('Model Doğruluğu')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Doğruluk')\n",
    "        plt.legend()\n",
    "        plt.grid(True, linestyle='--', alpha=0.6)\n",
    "        \n",
    "        # Learning Rate grafiği\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.plot(learning_rates, marker='s', alpha=0.7, color='red')\n",
    "        plt.title('Learning Rate Değişimi')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Learning Rate')\n",
    "        plt.yscale('log')  # Log scale for better visualization\n",
    "        plt.grid(True, linestyle='--', alpha=0.6)\n",
    "    else:\n",
    "        plt.figure(figsize=(14, 5))\n",
    "        \n",
    "        # Kayıp grafiği\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(train_losses, label='Eğitim', marker='o')\n",
    "        plt.plot(val_losses, label='Doğrulama', marker='*')\n",
    "        plt.title('Model Kaybı')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Kayıp (Cross-Entropy)')\n",
    "        plt.legend()\n",
    "        plt.grid(True, linestyle='--', alpha=0.6)\n",
    "        \n",
    "        # Doğruluk grafiği\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(train_accs, label='Eğitim', marker='o')\n",
    "        plt.plot(val_accs, label='Doğrulama', marker='*')\n",
    "        plt.title('Model Doğruluğu')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Doğruluk')\n",
    "        plt.legend()\n",
    "        plt.grid(True, linestyle='--', alpha=0.6)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Eğitim sonuçlarını görselleştir\n",
    "try:\n",
    "    if 'learning_rates' in locals():\n",
    "        plot_training_history(train_losses, val_losses, train_accs, val_accs, learning_rates)\n",
    "        \n",
    "        # Detaylı learning rate analizi\n",
    "        print(f\"\\n📊 Learning Rate İstatistikleri:\")\n",
    "        print(f\"Başlangıç LR: {learning_rates[0]:.2e}\")\n",
    "        print(f\"Maksimum LR: {max(learning_rates):.2e}\")\n",
    "        print(f\"Son LR: {learning_rates[-1]:.2e}\")\n",
    "        print(f\"Ortalama LR: {np.mean(learning_rates):.2e}\")\n",
    "    else:\n",
    "        plot_training_history(train_losses, val_losses, train_accs, val_accs)\n",
    "except NameError:\n",
    "    print(\"Eğitim geçmişi bulunamadı. Önce modeli eğitin.\")\n",
    "\n",
    "# Test veri seti üzerinde değerlendirme\n",
    "def evaluate_model(model, test_loader, device):\n",
    "    model.eval()\n",
    "    \n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            \n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(predicted.cpu().numpy())\n",
    "    \n",
    "    # Doğruluk hesapla\n",
    "    accuracy = np.mean(np.array(y_true) == np.array(y_pred))\n",
    "    \n",
    "    # Sonuçları yazdır\n",
    "    print(f\"\\n🎯 Test Doğruluğu: {accuracy:.4f}\")\n",
    "    \n",
    "    # Sınıflandırma raporu\n",
    "    print(\"\\n📋 Sınıflandırma Raporu:\")\n",
    "    print(classification_report(y_true, y_pred, target_names=le.classes_))\n",
    "    \n",
    "    # Karmaşıklık matrisi\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=le.classes_, yticklabels=le.classes_)\n",
    "    plt.title('Karmaşıklık Matrisi')\n",
    "    plt.xlabel('Tahmin Edilen Etiketler')\n",
    "    plt.ylabel('Gerçek Etiketler')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return y_true, y_pred\n",
    "\n",
    "# Test veri seti üzerinde değerlendir\n",
    "try:\n",
    "    y_true, y_pred = evaluate_model(model, test_loader, device)\n",
    "except NameError:\n",
    "    print(\"Model bulunamadı. Önce modeli eğitin.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pydebian (3.11.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
